[
  {
    "url": "https://habr.com/ru/companies/bothub/news/974590/",
    "site_type": "habr",
    "title": "Новое приложение от Google — Lyria Camera: музыка, генерируемая через объектив смартфона",
    "date": "08.12.2025",
    "text": "Команда Google DeepMind анонсировала приложение Lyria Camera , которое в реальном времени создаёт музыку на основе того, что видит ваша камера. Проще говоря, оно превращает окружающую действительность в уникальный, постоянно меняющийся саундтрек.\n\nНаводите камеру на что угодно — городскую улицу, страницу скетчбука или тарелку с завтраком, — и приложение начинает генерировать музыку, отражающую настроение сцены. Под капотом две мощные технологии: Gemini, который анализирует изображение и описывает его музыкальными терминами, и Lyria RealTime API, который превращает описания в непрерывный поток звука.\n\nРазработчики предлагают несколько сценариев использования, которые отлично демонстрируют потенциал приложения:\n\nМелодии можно искать в рисунках, интерьере или случайном узоре.\n\nНаправьте камеру в окно поезда или закрепите телефон над приборной панелью автомобиля. Lyria Camera будет реагировать на меняющийся пейзаж.\n\nНа десктопе можно использовать функцию «Поделиться экраном», чтобы вместо камеры использовать вкладку браузера или любое другое окно. Попробуйте запустить приложение во время работы или игровой сессии — и получите уникальный саундтрек к процессу.\n\nВ основе Lyria Camera лежит связка AI‑моделей, создающая бесшовный аудиовизуальный цикл.\n\nGemini анализирует видеопоток с камеры и переводит кадры в текстовые описания, но не просты, а связанные с музыкой. Например: «Задумчивое фортепиано, спокойствие городского пейзажа». Эти описания становятся инструкциями для следующего шага.\n\nAPI Lyria RealTime производит бесконечный поток аудио, который можно «направлять». Когда промпты от Gemini меняются, музыка плавно трансформируется, подстраиваясь под новое описание без пауз и скачков.\n\nLyria Camera — это демонстрация возможностей новой Lyria RealTime API , которая теперь доступна для разработчиков. Google создала этот демо‑проект, чтобы показать потенциал непрерывной и управляемой генерации музыки\n\nПопробовать Lyria Camera можно на смартфоне или компьютере .\n\nРазработчикам , готовым экспериментировать, стоит изучить документацию Lyria RealTime API на платформе Google AI Studio.\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИСТОЧНИК",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/abb/c67/302/abbc67302726d1ca51a16d21662565c7.jpg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/abb/c67/302/abbc67302726d1ca51a16d21662565c7.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Команда Google DeepMind анонсировала приложение Lyria Camera , которое в реальном времени создаёт музыку на основе того, что видит ваша камера. Проще говоря, оно превращает окружающую действительность в уникальный, постоянно меняющийся саундтрек."
      },
      {
        "type": "text",
        "content": "Наводите камеру на что угодно — городскую улицу, страницу скетчбука или тарелку с завтраком, — и приложение начинает генерировать музыку, отражающую настроение сцены. Под капотом две мощные технологии: Gemini, который анализирует изображение и описывает его музыкальными терминами, и Lyria RealTime API, который превращает описания в непрерывный поток звука."
      },
      {
        "type": "text",
        "content": "Разработчики предлагают несколько сценариев использования, которые отлично демонстрируют потенциал приложения:"
      },
      {
        "type": "text",
        "content": "В основе Lyria Camera лежит связка AI‑моделей, создающая бесшовный аудиовизуальный цикл."
      },
      {
        "type": "text",
        "content": "Lyria Camera — это демонстрация возможностей новой Lyria RealTime API , которая теперь доступна для разработчиков. Google создала этот демо‑проект, чтобы показать потенциал непрерывной и управляемой генерации музыки"
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "ИСТОЧНИК"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/click/news/974786/",
    "site_type": "habr",
    "title": "Яндекс представил бесплатного ИИ-ассистента для администраторов Telegram-каналов",
    "date": "09.12.2025",
    "text": "Яндекс запустил бота на базе искусственного интеллекта, который предназначен для авторов и владельцев Telegram-каналов. Он помогает готовить публикации: придумывать и править тексты, создавать изображения и другие элементы для постов. Помимо этого, ассистент упрощает подключение монетизации через Рекламную сеть Яндекса (РСЯ) — прямо из интерфейса мессенджера. Использование сервиса бесплатно.\n\nИИ-помощник работает на основе нейротехнологий Yandex Neuro Ads. Чтобы начать работу, нужно найти бота в Telegram и авторизоваться через Яндекс ID: @yandex_ads_helper_bot.\n\nЧто умеет бот:\n\nгенерировать контент. Писать и редактировать тексты под стилистику канала, создавать изображения, формировать опросы;\n\nпомогать с рекламой. Подключать канал к РСЯ, управлять размещениями, находить рекламодателей и зарабатывать на интеграциях.\n\nВ Яндексе подчеркивают, что бот не получает доступ к конфиденциальной информации Telegram-каналов и выполняет только заявленные функции.\n\nВести рекламу в РСЯ можно через click.ru , чтобы получить доступ к бесплатным инструментам сервиса, включая защиту от скликивания , дашборды и другие, а также вернуть часть рекламных расходов.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/c87/442/b58/c87442b58d3ed89e38cd1221bed18b51.png",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "text",
        "content": "Яндекс запустил бота на базе искусственного интеллекта, который предназначен для авторов и владельцев Telegram-каналов. Он помогает готовить публикации: придумывать и править тексты, создавать изображения и другие элементы для постов. Помимо этого, ассистент упрощает подключение монетизации через Рекламную сеть Яндекса (РСЯ) — прямо из интерфейса мессенджера. Использование сервиса бесплатно."
      },
      {
        "type": "text",
        "content": "ИИ-помощник работает на основе нейротехнологий Yandex Neuro Ads. Чтобы начать работу, нужно найти бота в Telegram и авторизоваться через Яндекс ID: @yandex_ads_helper_bot."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/c87/442/b58/c87442b58d3ed89e38cd1221bed18b51.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Что умеет бот:"
      },
      {
        "type": "text",
        "content": "В Яндексе подчеркивают, что бот не получает доступ к конфиденциальной информации Telegram-каналов и выполняет только заявленные функции."
      },
      {
        "type": "text",
        "content": "Вести рекламу в РСЯ можно через click.ru , чтобы получить доступ к бесплатным инструментам сервиса, включая защиту от скликивания , дашборды и другие, а также вернуть часть рекламных расходов."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/08/hinges-new-ai-feature-helps-daters-start-better-convos-moving-beyond-boring-small-talk/",
    "site_type": "techcrunch",
    "title": "Hinge’s new AI feature helps daters move beyond boring small talk",
    "date": "08.12.2025",
    "text": "Many daters on Hinge are getting annoyed with matches who just like their profiles but never bother to start a conversation. It often leads to this awkward silence, putting all the pressure on one person to make the first move. Instead of coming up with something interesting to say, some just fall back on the same old lines or stick to boring small talk, like asking, “How are you?”\n\nTo address this issue, Hinge unveiled “Convo Starters,” a feature powered by AI that provides personalized tips for initiating conversations.\n\nThe feature aims to inspire daters and boost their confidence when sending initial messages. When users like a profile, they’ll now see three tailored tips beneath each photo and prompt. The AI evaluates a user’s profile and generates recommendations based on the individual photos or prompts. For example, if a potential match is pictured playing chess, Hinge might suggest beginning the conversation around board games.\n\nConvo Starters was developed in response to user feedback, Hinge says. The company’s research indicated that 72% of Hinge daters are more inclined to consider someone when a like is accompanied by a message. Data from Hinge reveals that those who include a comment with their likes are twice as likely to arrange a date.\n\nThis new feature follows the launch of its AI-driven Prompt Feedback feature, which assesses user prompts and offers tailored advice aimed at improving them by urging users to elaborate and share engaging details about their lives.\n\nHowever, as Hinge incorporates AI features into its app, many users, especially Gen Z, are uncomfortable with the thought of using AI in their online dating experiences. A Bloomberg Intelligence survey found that Gen Z feels more uneasy about using AI for tasks such as drafting profile prompts and responding to messages than older generations do.\n\nHinge’s parent company, Match Group, is dedicating around $20 million to $30 million toward AI efforts.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/HingeConvoStarter.png?w=1024",
        "alt": "Image Credits:Hinge"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Hinge-ConvoStarter2.png?w=680",
        "alt": "Image Credits:Hinge"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/HingeConvoStarter.png?w=1024",
        "alt": "Image Credits:Hinge"
      },
      {
        "type": "text",
        "content": "Many daters on Hinge are getting annoyed with matches who just like their profiles but never bother to start a conversation. It often leads to this awkward silence, putting all the pressure on one person to make the first move. Instead of coming up with something interesting to say, some just fall back on the same old lines or stick to boring small talk, like asking, “How are you?”"
      },
      {
        "type": "text",
        "content": "To address this issue, Hinge unveiled “Convo Starters,” a feature powered by AI that provides personalized tips for initiating conversations."
      },
      {
        "type": "text",
        "content": "The feature aims to inspire daters and boost their confidence when sending initial messages. When users like a profile, they’ll now see three tailored tips beneath each photo and prompt. The AI evaluates a user’s profile and generates recommendations based on the individual photos or prompts. For example, if a potential match is pictured playing chess, Hinge might suggest beginning the conversation around board games."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Hinge-ConvoStarter2.png?w=680",
        "alt": "Image Credits:Hinge"
      },
      {
        "type": "text",
        "content": "Convo Starters was developed in response to user feedback, Hinge says. The company’s research indicated that 72% of Hinge daters are more inclined to consider someone when a like is accompanied by a message. Data from Hinge reveals that those who include a comment with their likes are twice as likely to arrange a date."
      },
      {
        "type": "text",
        "content": "This new feature follows the launch of its AI-driven Prompt Feedback feature, which assesses user prompts and offers tailored advice aimed at improving them by urging users to elaborate and share engaging details about their lives."
      },
      {
        "type": "text",
        "content": "However, as Hinge incorporates AI features into its app, many users, especially Gen Z, are uncomfortable with the thought of using AI in their online dating experiences. A Bloomberg Intelligence survey found that Gen Z feels more uneasy about using AI for tasks such as drafting profile prompts and responding to messages than older generations do."
      },
      {
        "type": "text",
        "content": "Hinge’s parent company, Match Group, is dedicating around $20 million to $30 million toward AI efforts."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/08/adobe-launches-content-creation-hub-in-premiere-mobile-for-youtube-shorts-creators/",
    "site_type": "techcrunch",
    "title": "Adobe launches content creation hub in Premiere mobile for YouTube Shorts creators",
    "date": "08.12.2025",
    "text": "Adobe is partnering with YouTube to launch a dedicated content creation space in Premiere for iOS for YouTube Shorts creators, the company announced on Monday. The new space gives creators access to exclusive templates, transitions, and effects, along with the ability to instantly publish Shorts to their YouTube channels directly from their phones.\n\nThe company says the space is designed to give creators everything they need to produce viral videos, grow their audience, and tap into trends, whether it’s creating day-in-the-life vlogs, travel videos, or behind-the-scenes clips.\n\nBy partnering with Adobe, YouTube is giving creators on its platform an exclusive space to create content, encouraging them to use its partners’ space instead of competitors’ tools, like Meta’s Edits or CapCut, which is owned by TikTok parent ByteDance.\n\n“Although content edited in Premiere mobile can be shared to other social platforms, what’s unique about this partnership is that creators getting inspiration from their YouTube Shorts feed, can launch a template that caught their eye, directly into Premiere mobile and start customizing it for their own channel,” said Meagan Keane, director, Product Marketing, Digital Video and Audio at Adobe, in an email to TechCrunch. “This content creation space within the Premiere mobile app is designed and optimized for YouTube Shorts.”\n\nThe Create for YouTube Shorts content creation space in Premiere mobile features Shorts templates from top creators with built-in text, effects, and transition presets. Creators can add their own media and customize the templates to their style. They also have the option to create and submit their own original templates.\n\nTo get started, creators need a free Premiere mobile login to access the space and a YouTube profile to publish directly to their Shorts feed. Creators can download Adobe Premiere from the App Store and tap the “Create for YouTube” option to access the creation space.\n\nFrom there, they can upload clips from their iPhone camera roll, cloud storage, or Adobe Creative Cloud. They can then cut and trim clips, layer in video and audio tracks, adjust color and brightness, and add text overlays and captions.\n\nAfter following export prompts, creators can upload the finished product to YouTube.\n\n“New tools and capabilities in the app, like templates, effects, transitions, and this new content creation space for YouTube Short creation, will be powerful for all creators, from longtime creators to those just getting started,” Keane said. “It brings creators polished video editing with studio-quality audio, AI sound effects, precision multi-track editing, Firefly AI content generation, and more to make producing and sharing their content easier and faster. This ultimately helps us further our goal of empowering creativity for all. Every day, we’re focused on doing our part to make this the best time ever to be a creator.”",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/YouTube-x-Premiere-Mobile.png?w=1024",
        "alt": "Image Credits:Adobe"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-12.32.14-PM.png?w=680",
        "alt": "Image Credits:Adobe"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/YouTube-x-Premiere-Mobile.png?w=1024",
        "alt": "Image Credits:Adobe"
      },
      {
        "type": "text",
        "content": "Adobe is partnering with YouTube to launch a dedicated content creation space in Premiere for iOS for YouTube Shorts creators, the company announced on Monday. The new space gives creators access to exclusive templates, transitions, and effects, along with the ability to instantly publish Shorts to their YouTube channels directly from their phones."
      },
      {
        "type": "text",
        "content": "The company says the space is designed to give creators everything they need to produce viral videos, grow their audience, and tap into trends, whether it’s creating day-in-the-life vlogs, travel videos, or behind-the-scenes clips."
      },
      {
        "type": "text",
        "content": "By partnering with Adobe, YouTube is giving creators on its platform an exclusive space to create content, encouraging them to use its partners’ space instead of competitors’ tools, like Meta’s Edits or CapCut, which is owned by TikTok parent ByteDance."
      },
      {
        "type": "text",
        "content": "“Although content edited in Premiere mobile can be shared to other social platforms, what’s unique about this partnership is that creators getting inspiration from their YouTube Shorts feed, can launch a template that caught their eye, directly into Premiere mobile and start customizing it for their own channel,” said Meagan Keane, director, Product Marketing, Digital Video and Audio at Adobe, in an email to TechCrunch. “This content creation space within the Premiere mobile app is designed and optimized for YouTube Shorts.”"
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-05-at-12.32.14-PM.png?w=680",
        "alt": "Image Credits:Adobe"
      },
      {
        "type": "text",
        "content": "The Create for YouTube Shorts content creation space in Premiere mobile features Shorts templates from top creators with built-in text, effects, and transition presets. Creators can add their own media and customize the templates to their style. They also have the option to create and submit their own original templates."
      },
      {
        "type": "text",
        "content": "To get started, creators need a free Premiere mobile login to access the space and a YouTube profile to publish directly to their Shorts feed. Creators can download Adobe Premiere from the App Store and tap the “Create for YouTube” option to access the creation space."
      },
      {
        "type": "text",
        "content": "From there, they can upload clips from their iPhone camera roll, cloud storage, or Adobe Creative Cloud. They can then cut and trim clips, layer in video and audio tracks, adjust color and brightness, and add text overlays and captions."
      },
      {
        "type": "text",
        "content": "After following export prompts, creators can upload the finished product to YouTube."
      },
      {
        "type": "text",
        "content": "“New tools and capabilities in the app, like templates, effects, transitions, and this new content creation space for YouTube Short creation, will be powerful for all creators, from longtime creators to those just getting started,” Keane said. “It brings creators polished video editing with studio-quality audio, AI sound effects, precision multi-track editing, Firefly AI content generation, and more to make producing and sharing their content easier and faster. This ultimately helps us further our goal of empowering creativity for all. Every day, we’re focused on doing our part to make this the best time ever to be a creator.”"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/08/google-details-security-measures-for-chromes-agentic-features/",
    "site_type": "techcrunch",
    "title": "Google details security measures for Chrome’s agentic features",
    "date": "08.12.2025",
    "text": "An increasing number of browsers are experimenting with agentic features that will take actions on your behalf, such as booking tickets or shopping for different items. However, these agentic capabilities also come with security risks that could lead to loss of data or money.\n\nGoogle detailed its approach to handling user security on Chrome using observer models and consent for user action. The company previewed agentic capabilities on Chrome in September and said these features will roll out in the coming months.\n\nThe company said it is using the help of a few models to keep agentic actions in check. Google said it built a User Alignment Critic using Gemini to scrutinize the action items built by the planner model for a particular task. If the critic model thinks that the planned tasks don’t serve the user’s goal, it asks the planner model to rethink the strategy. Google noted that the critic model only sees the metadata of the proposed action and not the actual web content.\n\nWhat’s more, to prevent agents from accessing disallowed or untrustworthy sites, Google is using Agent Origin Sets, which restrict the model to access read-only origins and read-writeable origins. Read-only origin is data that Gemini is permitted to consume content from. For instance, on a shopping site, the listings are relevant to the task, but banner ads aren’t. Similarly, Google said the agent is only allowed to click or type on certain iframes of a page.\n\n“This delineation enforces that only data from a limited set of origins is available to the agent, and this data can only be passed on to the writable origins. This bounds the threat vector of cross-origin data leaks. This also gives the browser the ability to enforce some of that separation, such as by not even sending to the model data that is outside the readable set,” the company said in a blog post.\n\nGoogle is also keeping a check on page navigation by investigating URLs through another observer model. This can prevent navigation to harmful model-generated URLs, the company said.\n\nThe search giant said that it is also handing over the reins to users for sensitive tasks. For instance, when an agent tries to navigate to a sensitive site with information like banking or your medical data, it first asks the user. For sites that require sign-in, it’ll ask the user for permission to let Chrome use the password manager. Google said that the agent’s model doesn’t have exposure to password data. The company added that it will ask users before taking actions like making a purchase or sending a message.\n\nGoogle said that, in addition to this, it also has a prompt-injection classifier to prevent unwanted actions and is also testing agentic capabilities against attacks created by researchers.\n\nAI browser makers are also paying attention to security. Earlier this month, Perplexity released a new open source content detection model to prevent prompt injection attacks against agents.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/07/chrome-with-ai-GettyImages-2184968624.jpg?w=1024",
        "alt": "Image Credits:Gabby Jones/Bloomberg / Getty Images"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/image-1.png?w=680",
        "alt": "Image Credits:Google"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/image-2.jpeg?w=657",
        "alt": "Image Credits:google"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/07/chrome-with-ai-GettyImages-2184968624.jpg?w=1024",
        "alt": "Image Credits:Gabby Jones/Bloomberg / Getty Images"
      },
      {
        "type": "text",
        "content": "An increasing number of browsers are experimenting with agentic features that will take actions on your behalf, such as booking tickets or shopping for different items. However, these agentic capabilities also come with security risks that could lead to loss of data or money."
      },
      {
        "type": "text",
        "content": "Google detailed its approach to handling user security on Chrome using observer models and consent for user action. The company previewed agentic capabilities on Chrome in September and said these features will roll out in the coming months."
      },
      {
        "type": "text",
        "content": "The company said it is using the help of a few models to keep agentic actions in check. Google said it built a User Alignment Critic using Gemini to scrutinize the action items built by the planner model for a particular task. If the critic model thinks that the planned tasks don’t serve the user’s goal, it asks the planner model to rethink the strategy. Google noted that the critic model only sees the metadata of the proposed action and not the actual web content."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/image-1.png?w=680",
        "alt": "Image Credits:Google"
      },
      {
        "type": "text",
        "content": "What’s more, to prevent agents from accessing disallowed or untrustworthy sites, Google is using Agent Origin Sets, which restrict the model to access read-only origins and read-writeable origins. Read-only origin is data that Gemini is permitted to consume content from. For instance, on a shopping site, the listings are relevant to the task, but banner ads aren’t. Similarly, Google said the agent is only allowed to click or type on certain iframes of a page."
      },
      {
        "type": "text",
        "content": "“This delineation enforces that only data from a limited set of origins is available to the agent, and this data can only be passed on to the writable origins. This bounds the threat vector of cross-origin data leaks. This also gives the browser the ability to enforce some of that separation, such as by not even sending to the model data that is outside the readable set,” the company said in a blog post."
      },
      {
        "type": "text",
        "content": "Google is also keeping a check on page navigation by investigating URLs through another observer model. This can prevent navigation to harmful model-generated URLs, the company said."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/image-2.jpeg?w=657",
        "alt": "Image Credits:google"
      },
      {
        "type": "text",
        "content": "The search giant said that it is also handing over the reins to users for sensitive tasks. For instance, when an agent tries to navigate to a sensitive site with information like banking or your medical data, it first asks the user. For sites that require sign-in, it’ll ask the user for permission to let Chrome use the password manager. Google said that the agent’s model doesn’t have exposure to password data. The company added that it will ask users before taking actions like making a purchase or sending a message."
      },
      {
        "type": "text",
        "content": "Google said that, in addition to this, it also has a prompt-injection classifier to prevent unwanted actions and is also testing agentic capabilities against attacks created by researchers."
      },
      {
        "type": "text",
        "content": "AI browser makers are also paying attention to security. Earlier this month, Perplexity released a new open source content detection model to prevent prompt injection attacks against agents."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/08/claude-code-is-coming-to-slack-and-thats-a-bigger-deal-than-it-sounds/",
    "site_type": "techcrunch",
    "title": "Claude Code is coming to Slack, and that’s a bigger deal than it sounds",
    "date": "08.12.2025",
    "text": "Anthropic is launching Claude Code in Slack, allowing developers to delegate coding tasks directly from chat threads. The beta feature, available Monday as a research preview, builds on Anthropic’s existing Slack integration by adding full workflow automation. The rollout signals that the next frontier in coding assistants isn’t the model; it’s the workflow.\n\nPreviously, developers could only get lightweight coding help via Claude in Slack — like writing snippets, debugging, and explanations. Now they can tag @Claude to spin up a complete coding session using Slack context like bug reports or feature requests. Claude analyzes recent messages to determine the right repository, posts progress updates in threads, and shares links to review work and open pull requests.\n\nThe move reflects a broader industry shift: AI coding assistants are migrating from IDEs (integrated development environment, where software development happens) into collaboration tools where teams already work.\n\nCursor offers Slack integration for drafting and debugging code in threads, while GitHub Copilot recently added features to generate pull requests from chat. OpenAI’s Codex is accessible via custom Slack bots.\n\nFor Slack, positioning itself as an “ agentic hub ” where AI meets workplace context creates a strategic advantage: Whichever AI tool dominates Slack — the center of engineering communication — could shape how software teams work.\n\nBy letting developers move seamlessly from conversation to code without switching apps, Claude Code and similar tools represent a shift toward AI-embedded collaboration that could fundamentally change developer workflows.\n\nWhile Anthropic has not yet confirmed when it would make a broader rollout available, the timing is strategic. The AI coding market is getting more competitive, and differentiation is starting to depend more on integration depth and distribution than model capability alone.\n\nThat said, the integration raises questions about code security and IP protection, as it adds another platform through which sensitive repository access must be managed and audited — while also introducing new dependencies where outages or rate limits in either Slack or Claude’s API could disrupt development workflows that teams previously controlled locally.\n\nTechCrunch has reached out to Anthropic and Slack for more information.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/claude-code-slack.png?w=1024",
        "alt": "Image Credits:Anthropic"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/claude-code-slack.png?w=1024",
        "alt": "Image Credits:Anthropic"
      },
      {
        "type": "text",
        "content": "Anthropic is launching Claude Code in Slack, allowing developers to delegate coding tasks directly from chat threads. The beta feature, available Monday as a research preview, builds on Anthropic’s existing Slack integration by adding full workflow automation. The rollout signals that the next frontier in coding assistants isn’t the model; it’s the workflow."
      },
      {
        "type": "text",
        "content": "Previously, developers could only get lightweight coding help via Claude in Slack — like writing snippets, debugging, and explanations. Now they can tag @Claude to spin up a complete coding session using Slack context like bug reports or feature requests. Claude analyzes recent messages to determine the right repository, posts progress updates in threads, and shares links to review work and open pull requests."
      },
      {
        "type": "text",
        "content": "The move reflects a broader industry shift: AI coding assistants are migrating from IDEs (integrated development environment, where software development happens) into collaboration tools where teams already work."
      },
      {
        "type": "text",
        "content": "Cursor offers Slack integration for drafting and debugging code in threads, while GitHub Copilot recently added features to generate pull requests from chat. OpenAI’s Codex is accessible via custom Slack bots."
      },
      {
        "type": "text",
        "content": "For Slack, positioning itself as an “ agentic hub ” where AI meets workplace context creates a strategic advantage: Whichever AI tool dominates Slack — the center of engineering communication — could shape how software teams work."
      },
      {
        "type": "text",
        "content": "By letting developers move seamlessly from conversation to code without switching apps, Claude Code and similar tools represent a shift toward AI-embedded collaboration that could fundamentally change developer workflows."
      },
      {
        "type": "text",
        "content": "While Anthropic has not yet confirmed when it would make a broader rollout available, the timing is strategic. The AI coding market is getting more competitive, and differentiation is starting to depend more on integration depth and distribution than model capability alone."
      },
      {
        "type": "text",
        "content": "That said, the integration raises questions about code security and IP protection, as it adds another platform through which sensitive repository access must be managed and audited — while also introducing new dependencies where outages or rate limits in either Slack or Claude’s API could disrupt development workflows that teams previously controlled locally."
      },
      {
        "type": "text",
        "content": "TechCrunch has reached out to Anthropic and Slack for more information."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/08/googles-ai-try-on-app-doppl-adds-a-shoppable-discovery-feed/",
    "site_type": "techcrunch",
    "title": "Google’s AI try-on app Doppl adds a shoppable discovery feed",
    "date": "08.12.2025",
    "text": "Google announced on Monday that it’s introducing a shoppable discovery feed in Doppl , its experimental app that uses AI to visualize how different outfits might look on you.\n\nThe tech giant says the idea behind the new feed is to display recommendations so users can discover and virtually try on new items. Nearly everything in the feed is shoppable, with direct links to merchants.\n\nThe discovery feed features AI-generated videos of real products and suggests outfits based on your personalized style. Google determines your style by analyzing the preferences you share with Doppl and the items you interact with.\n\nThe move comes as short-form video feeds, particularly on TikTok and Instagram, have conditioned users to scroll visual feeds and buy what they see. However, unlike on TikTok and Instagram, where real influencers showcase products, Google’s new feed only consists of AI-generated content.\n\nWhile some may not be fond of an AI-generated feed, Google likely sees it as a way to surface products in a format that people are already used to. Plus, it makes sense for the company to try a new e-commerce strategy, especially as it continues to lose ground to companies like Amazon and social media platforms.\n\nIt’s worth noting that AI-generated videos aren’t new to Doppl. While the app creates images of a virtual version of yourself wearing different outfits, it can turn these static images and convert them into AI-generated videos. The purpose of this is to give you a better sense of how the outfit would look on you in real life.\n\nThe new discovery feed is rolling out to Doppl on iOS and Android in the U.S. for users 18 and above.\n\nAlthough a feed consisting solely of AI-generated content would have seemed strange a year ago, the idea is now gaining traction. For example, OpenAI in September launched Sora , a social media platform of just AI videos. Meta also has a short-form video feed of AI-generated videos called “ Vibes ” in the Meta AI app.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/doppl.png?w=1024",
        "alt": "Image Credits:Doppl"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/doppl.png?w=1024",
        "alt": "Image Credits:Doppl"
      },
      {
        "type": "text",
        "content": "Google announced on Monday that it’s introducing a shoppable discovery feed in Doppl , its experimental app that uses AI to visualize how different outfits might look on you."
      },
      {
        "type": "text",
        "content": "The tech giant says the idea behind the new feed is to display recommendations so users can discover and virtually try on new items. Nearly everything in the feed is shoppable, with direct links to merchants."
      },
      {
        "type": "text",
        "content": "The discovery feed features AI-generated videos of real products and suggests outfits based on your personalized style. Google determines your style by analyzing the preferences you share with Doppl and the items you interact with."
      },
      {
        "type": "text",
        "content": "The move comes as short-form video feeds, particularly on TikTok and Instagram, have conditioned users to scroll visual feeds and buy what they see. However, unlike on TikTok and Instagram, where real influencers showcase products, Google’s new feed only consists of AI-generated content."
      },
      {
        "type": "text",
        "content": "While some may not be fond of an AI-generated feed, Google likely sees it as a way to surface products in a format that people are already used to. Plus, it makes sense for the company to try a new e-commerce strategy, especially as it continues to lose ground to companies like Amazon and social media platforms."
      },
      {
        "type": "text",
        "content": "It’s worth noting that AI-generated videos aren’t new to Doppl. While the app creates images of a virtual version of yourself wearing different outfits, it can turn these static images and convert them into AI-generated videos. The purpose of this is to give you a better sense of how the outfit would look on you in real life."
      },
      {
        "type": "text",
        "content": "The new discovery feed is rolling out to Doppl on iOS and Android in the U.S. for users 18 and above."
      },
      {
        "type": "text",
        "content": "Although a feed consisting solely of AI-generated content would have seemed strange a year ago, the idea is now gaining traction. For example, OpenAI in September launched Sora , a social media platform of just AI videos. Meta also has a short-form video feed of AI-generated videos called “ Vibes ” in the Meta AI app."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/08/department-of-commerce-may-approve-nvidia-h200-chip-exports-to-china/",
    "site_type": "techcrunch",
    "title": "Department of Commerce approves Nvidia H200 chip exports to China",
    "date": "08.12.2025",
    "text": "Advanced Nvidia AI chips can head back to China after all.\n\nThe Department of Commerce will allow Nvidia to ship H200 chips to China, as originally reported by Semafor , to approved customers in the country. The U.S. will take a 25% cut of these sales , CNBC reported.\n\nH200 chips are much more advanced than the H20 chips Nvidia developed specifically for the Chinese market, but the company would only be able to send H200s that are roughly 18 months old, Semafor reported.\n\nAn Nvidia spokesperson told TechCrunch of the development: “We applaud President Trump’s decision to allow America’s chip industry to compete to support high paying jobs and manufacturing in America. Offering H200 to approved commercial customers, vetted by the Department of Commerce, strikes a thoughtful balance that is great for America.”\n\nThe news report comes a week after U.S. Commerce Secretary Howard Lutnick said the decision on exporting these H200 chips to China was in President Donald Trump’s hands .\n\nThe decision to send the chips to China conflicts with Congressional concerns about national security.\n\nPete Ricketts, a Republican senator from Nebraska, and Chris Coons, a Democratic senator from Delaware, introduced a bill on December 4 that would block the export of advanced AI chips to China for more than two years.\n\nThe Secure and Feasible Exports Act (SAFE) Chips Act would require the Department of Commerce to deny any export license on advanced AI chips to China for 30 months. It’s unclear when legislators will vote on the proposed bill especially now that the Trump administration has given the green light to sell the H200 chips.\n\nWhile Congress has long been clear about sending advanced AI chips to China — on both sides of the aisle — President Trump has waffled on whether or not to allow the exports.\n\nThe Trump administration hit chip companies like Nvidia with licensing requirements to send their chips to China in April before it formally rescinded a Biden administration diffusion rule that would have regulated AI chip exports in May. Over the summer, the U.S. government signaled that companies would be able to start sending chips to China as long as the government got a 15% cut of all revenue , as chips became a bargaining tool in trade talks with China.\n\nHowever, by that point, the market for U.S.-developed chips in China was strained.\n\nIn September, China’s internet regulator, the Cyberspace Administration of China, banned domestic companies from buying Nvidia’s chips, leaving companies in the country to rely on less advanced domestic chips from Alibaba and Huawei.\n\nOn Monday, Trump said that Chinese president Xi Jinping “responded positively” to the latest H200 news in a Truth Social post .\n\nThis story was updated on December 8 when the proposed decision was confirmed.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2219673294.jpg?w=1024",
        "alt": "Image Credits:Chesnot / Getty Images"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2219673294.jpg?w=1024",
        "alt": "Image Credits:Chesnot / Getty Images"
      },
      {
        "type": "text",
        "content": "Advanced Nvidia AI chips can head back to China after all."
      },
      {
        "type": "text",
        "content": "The Department of Commerce will allow Nvidia to ship H200 chips to China, as originally reported by Semafor , to approved customers in the country. The U.S. will take a 25% cut of these sales , CNBC reported."
      },
      {
        "type": "text",
        "content": "H200 chips are much more advanced than the H20 chips Nvidia developed specifically for the Chinese market, but the company would only be able to send H200s that are roughly 18 months old, Semafor reported."
      },
      {
        "type": "text",
        "content": "An Nvidia spokesperson told TechCrunch of the development: “We applaud President Trump’s decision to allow America’s chip industry to compete to support high paying jobs and manufacturing in America. Offering H200 to approved commercial customers, vetted by the Department of Commerce, strikes a thoughtful balance that is great for America.”"
      },
      {
        "type": "text",
        "content": "The news report comes a week after U.S. Commerce Secretary Howard Lutnick said the decision on exporting these H200 chips to China was in President Donald Trump’s hands ."
      },
      {
        "type": "text",
        "content": "The decision to send the chips to China conflicts with Congressional concerns about national security."
      },
      {
        "type": "text",
        "content": "Pete Ricketts, a Republican senator from Nebraska, and Chris Coons, a Democratic senator from Delaware, introduced a bill on December 4 that would block the export of advanced AI chips to China for more than two years."
      },
      {
        "type": "text",
        "content": "The Secure and Feasible Exports Act (SAFE) Chips Act would require the Department of Commerce to deny any export license on advanced AI chips to China for 30 months. It’s unclear when legislators will vote on the proposed bill especially now that the Trump administration has given the green light to sell the H200 chips."
      },
      {
        "type": "text",
        "content": "While Congress has long been clear about sending advanced AI chips to China — on both sides of the aisle — President Trump has waffled on whether or not to allow the exports."
      },
      {
        "type": "text",
        "content": "The Trump administration hit chip companies like Nvidia with licensing requirements to send their chips to China in April before it formally rescinded a Biden administration diffusion rule that would have regulated AI chip exports in May. Over the summer, the U.S. government signaled that companies would be able to start sending chips to China as long as the government got a 15% cut of all revenue , as chips became a bargaining tool in trade talks with China."
      },
      {
        "type": "text",
        "content": "However, by that point, the market for U.S.-developed chips in China was strained."
      },
      {
        "type": "text",
        "content": "In September, China’s internet regulator, the Cyberspace Administration of China, banned domestic companies from buying Nvidia’s chips, leaving companies in the country to rely on less advanced domestic chips from Alibaba and Huawei."
      },
      {
        "type": "text",
        "content": "On Monday, Trump said that Chinese president Xi Jinping “responded positively” to the latest H200 news in a Truth Social post ."
      },
      {
        "type": "text",
        "content": "This story was updated on December 8 when the proposed decision was confirmed."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/09/mistral-ai-surfs-vibe-coding-tailwinds-with-new-coding-models/",
    "site_type": "techcrunch",
    "title": "Mistral AI surfs vibe-coding tailwinds with new coding models",
    "date": "09.12.2025",
    "text": "French AI startup Mistral today launched Devstral 2, a new generation of its AI model designed for coding, as the company seeks to catch up to bigger AI labs like Anthropic and other coding-focused LLMs.\n\nThis announcement follows the recent launch of the Mistral 3 family of open-weight models and confirms Mistral’s intent to close in on its bigger and better-funded AI rivals.\n\nThe unicorn is also jumping into the vibe-coding race, which has fueled the rise of companies like Cursor and Supabase , with Mistral Vibe, a new command-line interface (CLI) aimed at facilitating code automation through natural language, with tools for file manipulation, code searching, version control, and command execution.\n\nMistral AI is betting on the added value of context awareness, which is particularly relevant in business use cases. Similar to its AI assistant, Le Chat, which can remember previous conversations with users and use that context to guide its answers, Vibe CLI features persistent history and can scan file structures and Git statuses to build context to inform its behavior.\n\nThis focus on production-grade workflows also explains why Devstral 2 is relatively demanding, requiring at least four H100 GPUs or equivalent for deployment, and weighing 123 billion parameters. However, the model is also available in a smaller size with Devstral Small, which, at 24 billion parameters, makes it deployable locally on consumer hardware.\n\nThe models differ in their open source licensing — Devstral 2 ships under a modified MIT license, while Devstral Small uses Apache 2.0.\n\nThey also differ in pricing. Devstral 2 is currently free to use via the company’s API. After the free period, the API pricing will cost $0.40/$2.00 per million tokens (input/output) for Devstral 2, and $0.10/$0.30 for Devstral Small.\n\nMistral has partnered with agent tools Kilo Code and Cline to release Devstral 2 to users, while Mistral Vibe CLI is available as an extension in Zed for use inside the IDE.\n\nEurope’s champion AI lab, Mistral is currently valued at €11.7 billion (approximately $13.8 billion) following a Series C funding round led by Dutch semiconductor company ASML , which invested €1.3 billion (approximately $1.5 billion) in September.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-2147859992-e1713960898378.webp?w=1024",
        "alt": "Image Credits:Rafael Henrique/SOPA Images/LightRocket / Getty Images"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-2147859992-e1713960898378.webp?w=1024",
        "alt": "Image Credits:Rafael Henrique/SOPA Images/LightRocket / Getty Images"
      },
      {
        "type": "text",
        "content": "French AI startup Mistral today launched Devstral 2, a new generation of its AI model designed for coding, as the company seeks to catch up to bigger AI labs like Anthropic and other coding-focused LLMs."
      },
      {
        "type": "text",
        "content": "This announcement follows the recent launch of the Mistral 3 family of open-weight models and confirms Mistral’s intent to close in on its bigger and better-funded AI rivals."
      },
      {
        "type": "text",
        "content": "The unicorn is also jumping into the vibe-coding race, which has fueled the rise of companies like Cursor and Supabase , with Mistral Vibe, a new command-line interface (CLI) aimed at facilitating code automation through natural language, with tools for file manipulation, code searching, version control, and command execution."
      },
      {
        "type": "text",
        "content": "Mistral AI is betting on the added value of context awareness, which is particularly relevant in business use cases. Similar to its AI assistant, Le Chat, which can remember previous conversations with users and use that context to guide its answers, Vibe CLI features persistent history and can scan file structures and Git statuses to build context to inform its behavior."
      },
      {
        "type": "text",
        "content": "This focus on production-grade workflows also explains why Devstral 2 is relatively demanding, requiring at least four H100 GPUs or equivalent for deployment, and weighing 123 billion parameters. However, the model is also available in a smaller size with Devstral Small, which, at 24 billion parameters, makes it deployable locally on consumer hardware."
      },
      {
        "type": "text",
        "content": "The models differ in their open source licensing — Devstral 2 ships under a modified MIT license, while Devstral Small uses Apache 2.0."
      },
      {
        "type": "text",
        "content": "They also differ in pricing. Devstral 2 is currently free to use via the company’s API. After the free period, the API pricing will cost $0.40/$2.00 per million tokens (input/output) for Devstral 2, and $0.10/$0.30 for Devstral Small."
      },
      {
        "type": "text",
        "content": "Mistral has partnered with agent tools Kilo Code and Cline to release Devstral 2 to users, while Mistral Vibe CLI is available as an extension in Zed for use inside the IDE."
      },
      {
        "type": "text",
        "content": "Europe’s champion AI lab, Mistral is currently valued at €11.7 billion (approximately $13.8 billion) following a Series C funding round led by Dutch semiconductor company ASML , which invested €1.3 billion (approximately $1.5 billion) in September."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/09/anthropic-and-accenture-sign-multi-year-ai-strategic-partnership/",
    "site_type": "techcrunch",
    "title": "Anthropic and Accenture sign multi-year AI strategic partnership",
    "date": "09.12.2025",
    "text": "AI research lab Anthropic continues to cement its stake as the predominant AI player in the enterprise space.\n\nOn Tuesday, Anthropic announced a multi-year partnership with professional services firm Accenture. Financial terms of the deal were not disclosed; however, The Wall Street Journal reported that the deal is for three years .\n\nAccenture confirmed the deal is for three years but declined to comment on financials. TechCrunch reached out to Anthropic for more information.\n\nThe two companies are forming the Accenture Anthropic Business Group. This will include formal Claude training for Accenture’s 30,000 employees. Anthropic’s Claude Code coding tools will be available for Accenture’s tens of thousands of developers. They are also launching a joint initiative to help chief investment officers track their return on investment for AI.\n\nThis announcement comes as Anthropic’s market share within enterprise continues to grow.\n\nA new report from Menlo Ventures shows that Anthropic holds 40% of the market share within enterprise and 54% of the market share when it comes to coding. This marks a bump from Menlo’s previous survey this summer, where Anthropic held 32% of the enterprise market share.\n\nAnthropic announced a $200 million deal with cloud data company Snowflake last week. The company also announced sizable and similar AI partnerships with both Deloitte and IBM in October.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/06/Anthropic-economic-futures-program.png?w=1024",
        "alt": "Image Credits:Anthropic"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/06/Anthropic-economic-futures-program.png?w=1024",
        "alt": "Image Credits:Anthropic"
      },
      {
        "type": "text",
        "content": "AI research lab Anthropic continues to cement its stake as the predominant AI player in the enterprise space."
      },
      {
        "type": "text",
        "content": "On Tuesday, Anthropic announced a multi-year partnership with professional services firm Accenture. Financial terms of the deal were not disclosed; however, The Wall Street Journal reported that the deal is for three years ."
      },
      {
        "type": "text",
        "content": "Accenture confirmed the deal is for three years but declined to comment on financials. TechCrunch reached out to Anthropic for more information."
      },
      {
        "type": "text",
        "content": "The two companies are forming the Accenture Anthropic Business Group. This will include formal Claude training for Accenture’s 30,000 employees. Anthropic’s Claude Code coding tools will be available for Accenture’s tens of thousands of developers. They are also launching a joint initiative to help chief investment officers track their return on investment for AI."
      },
      {
        "type": "text",
        "content": "This announcement comes as Anthropic’s market share within enterprise continues to grow."
      },
      {
        "type": "text",
        "content": "A new report from Menlo Ventures shows that Anthropic holds 40% of the market share within enterprise and 54% of the market share when it comes to coding. This marks a bump from Menlo’s previous survey this summer, where Anthropic held 32% of the enterprise market share."
      },
      {
        "type": "text",
        "content": "Anthropic announced a $200 million deal with cloud data company Snowflake last week. The company also announced sizable and similar AI partnerships with both Deloitte and IBM in October."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/975074/",
    "site_type": "habr",
    "title": "VK улучшила технологию распознавания речи на 20%",
    "date": "09.12.2025",
    "text": "Компания VK улучшила технологию автоматического распознавания речи в своих продуктах. Инженеры AI VK доработали систему ASR на базе моделей машинного обучения. Новая версия распознаёт речь на 20% точнее по сравнению с предыдущей.\n\nТехнология ASR преобразует голос в текст. Система переводит звук в цифровой формат и очищает запись от шума. После этого она анализирует особенности звучания и определяет произнесённые слова. Нейросетевые модели и LLM помогают понимать контекст и смысловые связи, что делает расшифровку более естественной и точной.\n\nНовую версию технологии дообучили на расширенном наборе данных. Для этого использовали аудиодорожки из публично доступных видеороликов «VK Видео». Благодаря этому система стала лучше понимать темп и манеру речи. По результатам внутренних тестов модель превосходит зарубежные аналоги по качеству распознавания звуковых дорожек видео на русском языке.\n\nТехнологии ASR от VK применяются для создания субтитров в «VK Видео» и «VK Клипах», а также в образовательной платформе «Учи.ру». Система работает с голосовыми сообщениями в мессенджере «ВКонтакте». Технологию используют для внутренних задач компании, включая расшифровку встреч и их суммаризацию. Решение также помогает улучшать мультимодальные модели в рекомендательной системе Discovery.\n\nОбновлённая версия уже запущена в «VK Видео» и «VK Клипах». Её используют во внутренних сервисах команды VK. Постепенно технологию внедрят в другие продукты группы. Команда AI VK планирует повысить точность распознавания голосовых сообщений и расширить поддержку языков. Также разработчики добавят диаризацию для разделения речи по спикерам.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/0a3/b37/646/0a3b376468ed630dc34b16ddefd06a98.jpg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "text",
        "content": "Компания VK улучшила технологию автоматического распознавания речи в своих продуктах. Инженеры AI VK доработали систему ASR на базе моделей машинного обучения. Новая версия распознаёт речь на 20% точнее по сравнению с предыдущей."
      },
      {
        "type": "text",
        "content": "Технология ASR преобразует голос в текст. Система переводит звук в цифровой формат и очищает запись от шума. После этого она анализирует особенности звучания и определяет произнесённые слова. Нейросетевые модели и LLM помогают понимать контекст и смысловые связи, что делает расшифровку более естественной и точной."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/0a3/b37/646/0a3b376468ed630dc34b16ddefd06a98.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Новую версию технологии дообучили на расширенном наборе данных. Для этого использовали аудиодорожки из публично доступных видеороликов «VK Видео». Благодаря этому система стала лучше понимать темп и манеру речи. По результатам внутренних тестов модель превосходит зарубежные аналоги по качеству распознавания звуковых дорожек видео на русском языке."
      },
      {
        "type": "text",
        "content": "Технологии ASR от VK применяются для создания субтитров в «VK Видео» и «VK Клипах», а также в образовательной платформе «Учи.ру». Система работает с голосовыми сообщениями в мессенджере «ВКонтакте». Технологию используют для внутренних задач компании, включая расшифровку встреч и их суммаризацию. Решение также помогает улучшать мультимодальные модели в рекомендательной системе Discovery."
      },
      {
        "type": "text",
        "content": "Обновлённая версия уже запущена в «VK Видео» и «VK Клипах». Её используют во внутренних сервисах команды VK. Постепенно технологию внедрят в другие продукты группы. Команда AI VK планирует повысить точность распознавания голосовых сообщений и расширить поддержку языков. Также разработчики добавят диаризацию для разделения речи по спикерам."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/975128/",
    "site_type": "habr",
    "title": "Илон Маск научил ИИ вставлять рекламу прямо в фильмы без пауз",
    "date": "10.12.2025",
    "text": "xAI, компания Илона Маска, показала экспериментальный инструмент Halftime, который умеет вставлять рекламу прямо в сцены фильмов и сериалов - так, будто это часть сюжета. Вместо обычных рекламных пауз Halftime переписывает кадры: герой неожиданно поднимает банку - и вот перед вами уже брендовый напиток, который вы можете тут же купить.\n\nВ демонстрации показали, как сцена из сериала вроде «Suits» или «Друзей» превращается в скрытую рекламу. Персонажи держат в руках продукты, которых не было в оригинальном сюжете, а при нажатии появляется кнопка «Узнать больше» и ссылка на товар. После закрытия реклама исчезает, сюжет возвращается к исходной версии.\n\nАвторы Halftime подчёркивают, что цель - сделать рекламу частью фильма, а не раздражающим прерыванием. Но многие зрители восприняли это как вторжение: изменение сюжета, использование лиц актёров в коммерческих целях, внедрение брендов без согласия автора.\n\nЭтот подход ставит под угрозу само восприятие кино как искусства. Фильм больше не рассказывает историю - он становится рекламной платформой, подстроенной под рекламодателей. Некоторые уже сравнивают Halftime с антиутопией, где каждую секунду взвешивают не за сюжет, а за количество рекламных импульсов.\n\nДля киноиндустрии это может перерасти в юридический хаос: кто будет отвечать за авторские права, за согласование лиц актёров, за честность перед зрителем? Пока Halftime - эксперимент, но направление тревожное. Многие переживают, что подобный подход загрязнит не только экраны, но и саму доверие к контенту. Будем следить за новостями!\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/42a/5ba/3fe/42a5ba3fe8e435e429f9d33af4948ede.png",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/42a/5ba/3fe/42a5ba3fe8e435e429f9d33af4948ede.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "xAI, компания Илона Маска, показала экспериментальный инструмент Halftime, который умеет вставлять рекламу прямо в сцены фильмов и сериалов - так, будто это часть сюжета. Вместо обычных рекламных пауз Halftime переписывает кадры: герой неожиданно поднимает банку - и вот перед вами уже брендовый напиток, который вы можете тут же купить."
      },
      {
        "type": "text",
        "content": "В демонстрации показали, как сцена из сериала вроде «Suits» или «Друзей» превращается в скрытую рекламу. Персонажи держат в руках продукты, которых не было в оригинальном сюжете, а при нажатии появляется кнопка «Узнать больше» и ссылка на товар. После закрытия реклама исчезает, сюжет возвращается к исходной версии."
      },
      {
        "type": "text",
        "content": "Авторы Halftime подчёркивают, что цель - сделать рекламу частью фильма, а не раздражающим прерыванием. Но многие зрители восприняли это как вторжение: изменение сюжета, использование лиц актёров в коммерческих целях, внедрение брендов без согласия автора."
      },
      {
        "type": "text",
        "content": "Этот подход ставит под угрозу само восприятие кино как искусства. Фильм больше не рассказывает историю - он становится рекламной платформой, подстроенной под рекламодателей. Некоторые уже сравнивают Halftime с антиутопией, где каждую секунду взвешивают не за сюжет, а за количество рекламных импульсов."
      },
      {
        "type": "text",
        "content": "Для киноиндустрии это может перерасти в юридический хаос: кто будет отвечать за авторские права, за согласование лиц актёров, за честность перед зрителем? Пока Halftime - эксперимент, но направление тревожное. Многие переживают, что подобный подход загрязнит не только экраны, но и саму доверие к контенту. Будем следить за новостями!"
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/975190/",
    "site_type": "habr",
    "title": "FDA впервые разрешила ИИ создавать лекарства",
    "date": "10.12.2025",
    "text": "FDA впервые официально одобрила использование искусственного интеллекта для разработки лекарств. Модель AIM-NASH прошла сертификацию американских регуляторов и теперь может применяться фармацевтическими компаниями для ускорения исследований. Этот инструмент анализирует изображения биопсии у пациентов с тяжелой формой жировой болезни печени, оценивает степень воспаления, рубцевания и накопления жира, предоставляя точные количественные показатели, которые ранее могли давать только опытные специалисты.\n\nРегуляторы отметили, что результаты AIM-NASH сопоставимы с оценками экспертов-людей. Это означает, что фармацевтические компании могут использовать модель для принятия решений на разных этапах разработки новых препаратов, включая подбор пациентов для клинических исследований и оценку эффективности потенциальных лекарств. Ожидается, что применение модели позволит сократить время разработки лекарств примерно в два раза в течение ближайших трех-пяти лет, ускоряя выход новых препаратов на рынок и снижая расходы на исследования.\n\nОдобрение AIM-NASH также показывает растущее доверие регулирующих органов к ИИ в медицине и открывает новые возможности для интеграции нейросетей в процесс создания лекарств. Эксперты считают, что использование подобных инструментов может улучшить точность диагностики, повысить скорость клинических исследований и в конечном итоге изменить подход к лечению хронических заболеваний печени. В долгосрочной перспективе FDA рассматривает такие инструменты как часть будущей экосистемы цифровых медицинских технологий, которые смогут работать совместно с врачами, улучшая качество и скорость принятия решений.\n\nСпасибо за прочтение. А что думаете вы?\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/86e/a25/9e6/86ea259e63d150fb3b57716f64ef11f6.jpeg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/86e/a25/9e6/86ea259e63d150fb3b57716f64ef11f6.jpeg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "FDA впервые официально одобрила использование искусственного интеллекта для разработки лекарств. Модель AIM-NASH прошла сертификацию американских регуляторов и теперь может применяться фармацевтическими компаниями для ускорения исследований. Этот инструмент анализирует изображения биопсии у пациентов с тяжелой формой жировой болезни печени, оценивает степень воспаления, рубцевания и накопления жира, предоставляя точные количественные показатели, которые ранее могли давать только опытные специалисты."
      },
      {
        "type": "text",
        "content": "Регуляторы отметили, что результаты AIM-NASH сопоставимы с оценками экспертов-людей. Это означает, что фармацевтические компании могут использовать модель для принятия решений на разных этапах разработки новых препаратов, включая подбор пациентов для клинических исследований и оценку эффективности потенциальных лекарств. Ожидается, что применение модели позволит сократить время разработки лекарств примерно в два раза в течение ближайших трех-пяти лет, ускоряя выход новых препаратов на рынок и снижая расходы на исследования."
      },
      {
        "type": "text",
        "content": "Одобрение AIM-NASH также показывает растущее доверие регулирующих органов к ИИ в медицине и открывает новые возможности для интеграции нейросетей в процесс создания лекарств. Эксперты считают, что использование подобных инструментов может улучшить точность диагностики, повысить скорость клинических исследований и в конечном итоге изменить подход к лечению хронических заболеваний печени. В долгосрочной перспективе FDA рассматривает такие инструменты как часть будущей экосистемы цифровых медицинских технологий, которые смогут работать совместно с врачами, улучшая качество и скорость принятия решений."
      },
      {
        "type": "text",
        "content": "Спасибо за прочтение. А что думаете вы?"
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/975186/",
    "site_type": "habr",
    "title": "Nvidia будет программно отслеживать местоположение ИИ-ускорителей для борьбы с контрабандой",
    "date": "10.12.2025",
    "text": "Источники сообщили Reuters, что Nvidia разработала технологию определения местоположения своих ИИ-ускорителей. Это поможет компании предотвратить контрабанду чипов искусственного интеллекта в страны, куда их экспорт запрещён.\n\nNvidia демонстрировала функцию в частном порядке в последние месяцы, но пока не выпустила её. Это программная опция, которую клиенты смогут установить на ускорители. По словам источников, функция будет использовать так называемые конфиденциальные вычислительные возможности графических процессоров. Программное обеспечение было разработано для того, чтобы клиенты могли отслеживать общую вычислительную производительность чипа — распространённая практика среди компаний, которые приобретают парки процессоров для крупных центров обработки данных, — и будет использовать задержку во времени при обмене данными с серверами, управляемыми Nvidia, чтобы дать представление о местоположении чипа на уровне, сопоставимом с тем, что могут предоставить другие интернет-сервисы.\n\n«Мы находимся в процессе внедрения новой программной услуги, которая позволит операторам центров обработки данных отслеживать состояние и проводить инвентаризацию всего своего парка графических процессоров для ИИ. Этот программный агент, устанавливаемый клиентом, использует телеметрию графического процессора для мониторинга состояния, целостности и отслеживания устройств», — говорится в заявлении Nvidia.\n\nФункция сначала станет доступна на новейших чипах Blackwell, которые обладают наибольшим количеством функций безопасности «аттестации» по сравнению с предыдущими поколениями Hopper и Ampere, но Nvidia изучает варианты внедрения технологии и для этих более старых ускорителей.\n\nТаким образом компания стремится на призывы Белого дома и законодателей от обеих основных политических партий в Конгрессе США и принять меры по предотвращению контрабанды чипов ИИ в Китай и другие страны, где их продажа ограничена. В мае сенатор Том Коттон из Арканзаса представил законопроект, который требует встроить геолокационные технологии в мощные графические и ИИ‑процессоры. Он касается ИИ‑процессоров, серверов для высокопроизводительных вычислений, а также графических карт высокого класса.\n\nРанее министерство юстиции США возбудило уголовные дела против связанных с Китаем контрабандных группировок, которые, предположительно, пытались ввезти в КНР чипы Nvidia на сумму более $160 млн.\n\nМежду тем главный регулятор кибербезопасности Китая вызвал представителей Nvidia на допрос по поводу наличия в продукции бэкдоров, которые позволили бы США обойти функции безопасности чипов.\n\nА президент США Дональд Трамп заявил , что разрешит экспорт Nvidia H200 в Китай. Эксперты выразили скептицизм по поводу того, позволит ли КНР своим компаниям приобретать эти ускорители. Власти уже запретили использовать продукцию Nvidia в ЦОДах с госучастием, а также в работе компании ByteDance . В целом, Китай усилил таможенный контроль за импортом продвинутых микросхем для задач искусственного интеллекта.\n\nСама Nvidia категорически отрицает наличие бэкдоров в своих чипах. Эксперты по программному обеспечению заявили, что компания действительно могла разработать систему проверки местоположения ускорителей без ущерба для безопасности своей продукции.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/9cf/dfb/56b/9cfdfb56b0b387f3e01e50bb1c9d1ccb.JPG",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "text",
        "content": "Источники сообщили Reuters, что Nvidia разработала технологию определения местоположения своих ИИ-ускорителей. Это поможет компании предотвратить контрабанду чипов искусственного интеллекта в страны, куда их экспорт запрещён."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/9cf/dfb/56b/9cfdfb56b0b387f3e01e50bb1c9d1ccb.JPG",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Nvidia демонстрировала функцию в частном порядке в последние месяцы, но пока не выпустила её. Это программная опция, которую клиенты смогут установить на ускорители. По словам источников, функция будет использовать так называемые конфиденциальные вычислительные возможности графических процессоров. Программное обеспечение было разработано для того, чтобы клиенты могли отслеживать общую вычислительную производительность чипа — распространённая практика среди компаний, которые приобретают парки процессоров для крупных центров обработки данных, — и будет использовать задержку во времени при обмене данными с серверами, управляемыми Nvidia, чтобы дать представление о местоположении чипа на уровне, сопоставимом с тем, что могут предоставить другие интернет-сервисы."
      },
      {
        "type": "text",
        "content": "«Мы находимся в процессе внедрения новой программной услуги, которая позволит операторам центров обработки данных отслеживать состояние и проводить инвентаризацию всего своего парка графических процессоров для ИИ. Этот программный агент, устанавливаемый клиентом, использует телеметрию графического процессора для мониторинга состояния, целостности и отслеживания устройств», — говорится в заявлении Nvidia."
      },
      {
        "type": "text",
        "content": "Функция сначала станет доступна на новейших чипах Blackwell, которые обладают наибольшим количеством функций безопасности «аттестации» по сравнению с предыдущими поколениями Hopper и Ampere, но Nvidia изучает варианты внедрения технологии и для этих более старых ускорителей."
      },
      {
        "type": "text",
        "content": "Таким образом компания стремится на призывы Белого дома и законодателей от обеих основных политических партий в Конгрессе США и принять меры по предотвращению контрабанды чипов ИИ в Китай и другие страны, где их продажа ограничена. В мае сенатор Том Коттон из Арканзаса представил законопроект, который требует встроить геолокационные технологии в мощные графические и ИИ‑процессоры. Он касается ИИ‑процессоров, серверов для высокопроизводительных вычислений, а также графических карт высокого класса."
      },
      {
        "type": "text",
        "content": "Ранее министерство юстиции США возбудило уголовные дела против связанных с Китаем контрабандных группировок, которые, предположительно, пытались ввезти в КНР чипы Nvidia на сумму более $160 млн."
      },
      {
        "type": "text",
        "content": "Между тем главный регулятор кибербезопасности Китая вызвал представителей Nvidia на допрос по поводу наличия в продукции бэкдоров, которые позволили бы США обойти функции безопасности чипов."
      },
      {
        "type": "text",
        "content": "А президент США Дональд Трамп заявил , что разрешит экспорт Nvidia H200 в Китай. Эксперты выразили скептицизм по поводу того, позволит ли КНР своим компаниям приобретать эти ускорители. Власти уже запретили использовать продукцию Nvidia в ЦОДах с госучастием, а также в работе компании ByteDance . В целом, Китай усилил таможенный контроль за импортом продвинутых микросхем для задач искусственного интеллекта."
      },
      {
        "type": "text",
        "content": "Сама Nvidia категорически отрицает наличие бэкдоров в своих чипах. Эксперты по программному обеспечению заявили, что компания действительно могла разработать систему проверки местоположения ускорителей без ущерба для безопасности своей продукции."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/975198/",
    "site_type": "habr",
    "title": "OpenAI запустила сертификационные курсы ИИ для работников",
    "date": "10.12.2025",
    "text": "OpenAI запустила первые официальные сертификационные курсы, чтобы предоставить работникам и преподавателям инструменты искусственного интеллекта для соответствия меняющемуся рынку труда.\n\nКомпания заявляет, что миллионы людей уже еженедельно используют ChatGPT для изучения новых навыков или поиска работы, но многие до сих пор не знают, с чего начать. Курсы призваны восполнить этот пробел и подготовить людей к работе, требующей владения инструментами ИИ.\n\nВ центре обучения — новая программа под названием AI Foundations, которая запускается в рамках пилотных проектов с крупными работодателями и государственными партнёрами. Вместо того чтобы переключаться между платформами или смотреть бесконечные обучающие видеоролики, учащиеся получат полный опыт внутри ChatGPT. Чат-бот выступит одновременно и репетитором, и средой для практики, и механизмом обратной связи. Те, кто завершит курс, получат значок, подтверждающий наличие реальных и востребованных на рынке труда навыков.\n\nOpenAI планирует создать более широкую программу, которая в конечном итоге будет предлагать сертификаты, подтверждающие умение применять инструменты ИИ в реальных бизнес-ситуациях.\n\nВ пилотном проекте участвуют такие крупные компании, как Walmart, John Deere, Lowe’s, Elevance Health, BCG и Accenture. Также к ней подключились офис губернатора штата Делавэр и Choose New Jersey.\n\nСтуденты колледжей, участвующие в ChatGPT Lab, также получат ранний доступ к программе, а Университет штата Аризона и система Калифорнийских государственных университетов тестируют новые направления, помогающие студентам получить сертификаты до выхода на рынок труда.\n\nПреподаватели также получат свою собственную программу. Курс Foundations for Teachers уже доступен на Coursera, а в начале 2026 года планируется его интеграция непосредственно в ChatGPT. Он ориентирован на учителей начальной и средней школы, которые уже используют ИИ для планирования уроков, персонализации материалов или просто для сокращения ежедневной рутинной работы. Также им предложат бесплатную программу ChatGPT for Teachers.\n\nOpenAI расширяет и сотрудничество с Indeed и Upwork, чтобы создать более понятный путь от обучения к получению работы. Компания надеется сертифицировать 10 млн американцев к 2030 году.\n\nВ сентябре OpenAI заявила , что разрабатывает платформу найма на базе искусственного интеллекта OpenAI Jobs Platform. Продукт планируется запустить к середине 2026 года. Генеральный директор OpenAI Applications Фиджи Симо объявила, что компания будет «использовать ИИ, чтобы находить идеальное соответствие между потребностями компаний и возможностями сотрудников». Сервис предоставит малому бизнесу и местным органам власти специальную возможность доступа к лучшим специалистам в области ИИ.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/8a2/e6e/0d4/8a2e6e0d4270bdc674227ed92576f0fa.JPG",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "text",
        "content": "OpenAI запустила первые официальные сертификационные курсы, чтобы предоставить работникам и преподавателям инструменты искусственного интеллекта для соответствия меняющемуся рынку труда."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/8a2/e6e/0d4/8a2e6e0d4270bdc674227ed92576f0fa.JPG",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Компания заявляет, что миллионы людей уже еженедельно используют ChatGPT для изучения новых навыков или поиска работы, но многие до сих пор не знают, с чего начать. Курсы призваны восполнить этот пробел и подготовить людей к работе, требующей владения инструментами ИИ."
      },
      {
        "type": "text",
        "content": "В центре обучения — новая программа под названием AI Foundations, которая запускается в рамках пилотных проектов с крупными работодателями и государственными партнёрами. Вместо того чтобы переключаться между платформами или смотреть бесконечные обучающие видеоролики, учащиеся получат полный опыт внутри ChatGPT. Чат-бот выступит одновременно и репетитором, и средой для практики, и механизмом обратной связи. Те, кто завершит курс, получат значок, подтверждающий наличие реальных и востребованных на рынке труда навыков."
      },
      {
        "type": "text",
        "content": "OpenAI планирует создать более широкую программу, которая в конечном итоге будет предлагать сертификаты, подтверждающие умение применять инструменты ИИ в реальных бизнес-ситуациях."
      },
      {
        "type": "text",
        "content": "В пилотном проекте участвуют такие крупные компании, как Walmart, John Deere, Lowe’s, Elevance Health, BCG и Accenture. Также к ней подключились офис губернатора штата Делавэр и Choose New Jersey."
      },
      {
        "type": "text",
        "content": "Студенты колледжей, участвующие в ChatGPT Lab, также получат ранний доступ к программе, а Университет штата Аризона и система Калифорнийских государственных университетов тестируют новые направления, помогающие студентам получить сертификаты до выхода на рынок труда."
      },
      {
        "type": "text",
        "content": "Преподаватели также получат свою собственную программу. Курс Foundations for Teachers уже доступен на Coursera, а в начале 2026 года планируется его интеграция непосредственно в ChatGPT. Он ориентирован на учителей начальной и средней школы, которые уже используют ИИ для планирования уроков, персонализации материалов или просто для сокращения ежедневной рутинной работы. Также им предложат бесплатную программу ChatGPT for Teachers."
      },
      {
        "type": "text",
        "content": "OpenAI расширяет и сотрудничество с Indeed и Upwork, чтобы создать более понятный путь от обучения к получению работы. Компания надеется сертифицировать 10 млн американцев к 2030 году."
      },
      {
        "type": "text",
        "content": "В сентябре OpenAI заявила , что разрабатывает платформу найма на базе искусственного интеллекта OpenAI Jobs Platform. Продукт планируется запустить к середине 2026 года. Генеральный директор OpenAI Applications Фиджи Симо объявила, что компания будет «использовать ИИ, чтобы находить идеальное соответствие между потребностями компаний и возможностями сотрудников». Сервис предоставит малому бизнесу и местным органам власти специальную возможность доступа к лучшим специалистам в области ИИ."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/rshb/news/975228/",
    "site_type": "habr",
    "title": "РСХБ и РСХБ-Автоматизация дополняют ИИ-платформу ассистентами на базе LLM",
    "date": "10.12.2025",
    "text": "РСХБ масштабирует ИИ-ассистентов на обновленной платформе RAISA с компанией ООО «РСХБ-Автоматизация».\n\nРоссельхозбанк (РСХБ) выводит на новый уровень собственную платформу искусственного интеллекта RAISA, наделив её возможностями для запуска и масштабирования десятков ИИ-ассистентов. К работам по развитию платформы активно привлекается команда ИТ-дочки банка ООО «РСХБ-Автоматизация».\n\nRAISA (RSHB AI SYSTEMS AND APPLICATIONS) — решение полного цикла для работы с искусственным интеллектом, объединяющее все этапы — от подготовки и контроля качества данных до разработки, внедрения и мониторинга моделей.\n\nВ основе RAISA — развитая инфраструктура для подготовки и обработки данных, проведения экспериментов и документирования моделей. Платформа обеспечивает стандартизированные процессы построения и тестирования моделей, а также их развёртывание и контроль качества в промышленной среде. Особое внимание уделено пользовательскому опыту моделистов: рабочее пространство интегрирует инструменты аналитики, версионирования кода, автоматизации процессов и взаимодействия с корпоративными источниками данных.\n\nКак отметил директор Департамента больших данных РСХБ Александр Сабуров, платформенный подход является ключевым для Банка. Он позволяет централизованно контролировать весь жизненный цикл ассистентов — от определения моделей и сбора данных до мониторинга ресурсов и расчета финансовой эффективности, что экономит время и силы по сравнению с точечными решениями.\n\nВ рамках обновленной платформы уже запущены первые помощники. Они решают задачи умного поиска, генерации документов по внутренним нормативам, формализации заявок в ИТ-поддержку и предоставляют контекстные подсказки в интерфейсах систем банка. Это позволяет интеллектуально маршрутизировать задачи и снизить нагрузку на сотрудников.\n\nПерспективы развития RAISA связаны с углублением интеграций через открытые API и созданием продвинутых механизмов контроля ресурсов. «При экспоненциальном росте потребностей генеративного ИИ критически важен учет токенов и прогнозирование нагрузок», — подчеркнул руководитель лаборатории ИИ Департамента больших данных РСХБ Даниил Потапов.",
    "images": [],
    "structured_content": [
      {
        "type": "text",
        "content": "РСХБ масштабирует ИИ-ассистентов на обновленной платформе RAISA с компанией ООО «РСХБ-Автоматизация»."
      },
      {
        "type": "text",
        "content": "Россельхозбанк (РСХБ) выводит на новый уровень собственную платформу искусственного интеллекта RAISA, наделив её возможностями для запуска и масштабирования десятков ИИ-ассистентов. К работам по развитию платформы активно привлекается команда ИТ-дочки банка ООО «РСХБ-Автоматизация»."
      },
      {
        "type": "text",
        "content": "RAISA (RSHB AI SYSTEMS AND APPLICATIONS) — решение полного цикла для работы с искусственным интеллектом, объединяющее все этапы — от подготовки и контроля качества данных до разработки, внедрения и мониторинга моделей."
      },
      {
        "type": "text",
        "content": "В основе RAISA — развитая инфраструктура для подготовки и обработки данных, проведения экспериментов и документирования моделей. Платформа обеспечивает стандартизированные процессы построения и тестирования моделей, а также их развёртывание и контроль качества в промышленной среде. Особое внимание уделено пользовательскому опыту моделистов: рабочее пространство интегрирует инструменты аналитики, версионирования кода, автоматизации процессов и взаимодействия с корпоративными источниками данных."
      },
      {
        "type": "text",
        "content": "Как отметил директор Департамента больших данных РСХБ Александр Сабуров, платформенный подход является ключевым для Банка. Он позволяет централизованно контролировать весь жизненный цикл ассистентов — от определения моделей и сбора данных до мониторинга ресурсов и расчета финансовой эффективности, что экономит время и силы по сравнению с точечными решениями."
      },
      {
        "type": "text",
        "content": "В рамках обновленной платформы уже запущены первые помощники. Они решают задачи умного поиска, генерации документов по внутренним нормативам, формализации заявок в ИТ-поддержку и предоставляют контекстные подсказки в интерфейсах систем банка. Это позволяет интеллектуально маршрутизировать задачи и снизить нагрузку на сотрудников."
      },
      {
        "type": "text",
        "content": "Перспективы развития RAISA связаны с углублением интеграций через открытые API и созданием продвинутых механизмов контроля ресурсов. «При экспоненциальном росте потребностей генеративного ИИ критически важен учет токенов и прогнозирование нагрузок», — подчеркнул руководитель лаборатории ИИ Департамента больших данных РСХБ Даниил Потапов."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/975250/",
    "site_type": "habr",
    "title": "Gartner посоветовала компаниям не использовать ИИ-браузеры из-за рисков утечек",
    "date": "10.12.2025",
    "text": "Аналитики Gartner порекомендовали организациям пресекать использование сотрудниками так называемых «агентских ИИ-браузеров», вроде Comet от Perplexity и ChatGPT Atlas от OpenAI, из-за высоких киберрисков.\n\nКомпания опубликовала совет в документе под названием «Кибербезопасность должна пока блокировать браузеры с ИИ». Вице-президент по исследованиям Деннис Сюй, старший директор-аналитик Евгений Миролюбов и вице-президент-аналитик Джон Уоттс отмечают: «В настройках браузера с ИИ по умолчанию приоритет отдаётся удобству использования, а не безопасности».\n\nПо определению аналитиков, Comet от Perplexity и ChatGPT Atlas от OpenAI содержат два элемента: «боковую панель ИИ», которая предоставляет пользователям возможность обобщать, искать, переводить веб-контент и взаимодействовать с ним с помощью сервисов; а также возможность агентных транзакций, что позволяет браузеру автономно перемещаться по веб-сайтам, взаимодействовать с ними и выполнять задачи, особенно в рамках аутентифицированных веб-сессий.\n\nВ документе Gartner содержится предупреждение о том, что «конфиденциальные данные пользователей — такие как активный веб-контент, история просмотров и открытые вкладки — часто отправляются в облачную среду обработки ИИ, а это увеличивает риск утечки данных, если настройки безопасности и конфиденциальности не будут целенаправленно усилены и централизованно управляться».\n\nВ документе предлагается снизить эти риски, оценив серверные службы ИИ, которые обеспечивают работу браузера. В случае прохождения проверки Gartner советует организациям всё равно «проинформировать пользователей о том, что всё, что они просматривают, потенциально может быть отправлено в серверную часть службы ИИ, чтобы гарантировать, что у них не будет активных конфиденциальных данных на вкладке браузера во время использования боковой панели браузера с ИИ для подведения итогов или выполнения других автономных действий».\n\nОпасения аналитиков по поводу возможностей ИИ-браузеров связаны с их уязвимостью к «непрямым действиям агентов, вызванным внедрением подсказок, неточным, ошибочным действиям агентов, а также к дальнейшей потере и злоупотреблению учётными данными, если ИИ-браузер автоматически перейдет на фишинговый веб-сайт».\n\nАвторы также предполагают, что сотрудники «могут испытывать искушение использовать ИИ-браузеры и автоматизировать определённые задачи, которые являются обязательными, повторяющимися и менее интересными».\n\nДругой сценарий — ознакомление ИИ-браузеров с внутренними инструментами закупок. «Форма может быть заполнена неверной информацией, может быть заказан не тот канцелярский товар… или может быть забронирован не тот рейс», — предполагают аналитики.\n\nВ Gartner порекомендовали установить запрет на использование электронной почты агентами, поскольку это ограничит их возможности по выполнению некоторых действий. Эксперты также предлагают использовать настройки, которые гарантируют, что браузеры с ИИ не смогут сохранять данные.\n\nРанее исследователи Cato Networks обнаружили новую атаку на ИИ-браузеры, в рамках которой символ «#» используется в URL для внедрения скрытых команд. ИИ-ассистенты выполняют их в обход традиционных средств защиты. Атака, получившая название HashJack, использует особенность обработки URL, при которой фрагмент адреса после символа «#» остаётся внутри браузера и не отправляется на сервер. Злоумышленник может дописать решётку в конец корректной ссылки, а после неё добавить скрытые промты для ИИ. Когда пользователь обратится к встроенному помощнику, такому как Copilot, Gemini или Comet от Perplexity AI, языковая модель получит URL с фрагментом после «#» и воспримет эти инструкции как легитимные команды.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/fae/d8e/446/faed8e446bc446f686eab9145c15da36.JPG",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "text",
        "content": "Аналитики Gartner порекомендовали организациям пресекать использование сотрудниками так называемых «агентских ИИ-браузеров», вроде Comet от Perplexity и ChatGPT Atlas от OpenAI, из-за высоких киберрисков."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/fae/d8e/446/faed8e446bc446f686eab9145c15da36.JPG",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Компания опубликовала совет в документе под названием «Кибербезопасность должна пока блокировать браузеры с ИИ». Вице-президент по исследованиям Деннис Сюй, старший директор-аналитик Евгений Миролюбов и вице-президент-аналитик Джон Уоттс отмечают: «В настройках браузера с ИИ по умолчанию приоритет отдаётся удобству использования, а не безопасности»."
      },
      {
        "type": "text",
        "content": "По определению аналитиков, Comet от Perplexity и ChatGPT Atlas от OpenAI содержат два элемента: «боковую панель ИИ», которая предоставляет пользователям возможность обобщать, искать, переводить веб-контент и взаимодействовать с ним с помощью сервисов; а также возможность агентных транзакций, что позволяет браузеру автономно перемещаться по веб-сайтам, взаимодействовать с ними и выполнять задачи, особенно в рамках аутентифицированных веб-сессий."
      },
      {
        "type": "text",
        "content": "В документе Gartner содержится предупреждение о том, что «конфиденциальные данные пользователей — такие как активный веб-контент, история просмотров и открытые вкладки — часто отправляются в облачную среду обработки ИИ, а это увеличивает риск утечки данных, если настройки безопасности и конфиденциальности не будут целенаправленно усилены и централизованно управляться»."
      },
      {
        "type": "text",
        "content": "В документе предлагается снизить эти риски, оценив серверные службы ИИ, которые обеспечивают работу браузера. В случае прохождения проверки Gartner советует организациям всё равно «проинформировать пользователей о том, что всё, что они просматривают, потенциально может быть отправлено в серверную часть службы ИИ, чтобы гарантировать, что у них не будет активных конфиденциальных данных на вкладке браузера во время использования боковой панели браузера с ИИ для подведения итогов или выполнения других автономных действий»."
      },
      {
        "type": "text",
        "content": "Опасения аналитиков по поводу возможностей ИИ-браузеров связаны с их уязвимостью к «непрямым действиям агентов, вызванным внедрением подсказок, неточным, ошибочным действиям агентов, а также к дальнейшей потере и злоупотреблению учётными данными, если ИИ-браузер автоматически перейдет на фишинговый веб-сайт»."
      },
      {
        "type": "text",
        "content": "Авторы также предполагают, что сотрудники «могут испытывать искушение использовать ИИ-браузеры и автоматизировать определённые задачи, которые являются обязательными, повторяющимися и менее интересными»."
      },
      {
        "type": "text",
        "content": "Другой сценарий — ознакомление ИИ-браузеров с внутренними инструментами закупок. «Форма может быть заполнена неверной информацией, может быть заказан не тот канцелярский товар… или может быть забронирован не тот рейс», — предполагают аналитики."
      },
      {
        "type": "text",
        "content": "В Gartner порекомендовали установить запрет на использование электронной почты агентами, поскольку это ограничит их возможности по выполнению некоторых действий. Эксперты также предлагают использовать настройки, которые гарантируют, что браузеры с ИИ не смогут сохранять данные."
      },
      {
        "type": "text",
        "content": "Ранее исследователи Cato Networks обнаружили новую атаку на ИИ-браузеры, в рамках которой символ «#» используется в URL для внедрения скрытых команд. ИИ-ассистенты выполняют их в обход традиционных средств защиты. Атака, получившая название HashJack, использует особенность обработки URL, при которой фрагмент адреса после символа «#» остаётся внутри браузера и не отправляется на сервер. Злоумышленник может дописать решётку в конец корректной ссылки, а после неё добавить скрытые промты для ИИ. Когда пользователь обратится к встроенному помощнику, такому как Copilot, Gemini или Comet от Perplexity AI, языковая модель получит URL с фрагментом после «#» и воспримет эти инструкции как легитимные команды."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/975294/",
    "site_type": "habr",
    "title": "Google опередил OpenAI в гонке за Пентагон: Gemini развернули для 3 млн военных",
    "date": "10.12.2025",
    "text": "Министерство обороны США запустило платформу GenAI.mil — впервые в истории ИИ появился в наборе стандартных инструментов каждого сотрудника Пентагона. Первой моделью стал Google Gemini for Government. \"Будущее американской войны уже здесь, и оно пишется как AI\", — заявил министр обороны Пит Хегсет.\n\nДоступ получают военнослужащие, гражданские сотрудники и контрактники — около 3 млн человек. Для входа нужна CAC-карта, платформа работает с чувствительными, но неклассифицированными данными. По словам Хегсета, система позволяет \"проводить глубокие исследования, форматировать документы и анализировать видео или изображения с беспрецедентной скоростью\". Gemini привязан к поиску Google для снижения риска галлюцинаций.\n\nПри этом Google — не единственный поставщик ИИ для Пентагона. В июле CDAO (Chief Digital & AI Office) раздал контракты по $200 млн четырем компаниям: Google, OpenAI, Anthropic и xAI. Google просто оказался первым, кто развернул массовый доступ. Заместитель министра обороны Эмиль Майкл пообещал, что другие модели появятся на GenAI.mil \"в ближайшие недели и месяцы\" — но очевидно, что первенство Google дает компании конкурентное преимущество в борьбе за будущие контракты богатого оборонного ведомства.\n\nВ 2018 году тысячи сотрудников Google подписали письмо протеста против Project Maven — контракта с Пентагоном на ИИ для анализа видео с дронов. Компания тогда свернула проект. Семь лет спустя Gemini появился на каждом рабочем столе американских военных.\n\nP.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/ad4/0e4/e36/ad40e4e36191fa79a9009a10219d1d61.jpg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/ad4/0e4/e36/ad40e4e36191fa79a9009a10219d1d61.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Министерство обороны США запустило платформу GenAI.mil — впервые в истории ИИ появился в наборе стандартных инструментов каждого сотрудника Пентагона. Первой моделью стал Google Gemini for Government. \"Будущее американской войны уже здесь, и оно пишется как AI\", — заявил министр обороны Пит Хегсет."
      },
      {
        "type": "text",
        "content": "Доступ получают военнослужащие, гражданские сотрудники и контрактники — около 3 млн человек. Для входа нужна CAC-карта, платформа работает с чувствительными, но неклассифицированными данными. По словам Хегсета, система позволяет \"проводить глубокие исследования, форматировать документы и анализировать видео или изображения с беспрецедентной скоростью\". Gemini привязан к поиску Google для снижения риска галлюцинаций."
      },
      {
        "type": "text",
        "content": "При этом Google — не единственный поставщик ИИ для Пентагона. В июле CDAO (Chief Digital & AI Office) раздал контракты по $200 млн четырем компаниям: Google, OpenAI, Anthropic и xAI. Google просто оказался первым, кто развернул массовый доступ. Заместитель министра обороны Эмиль Майкл пообещал, что другие модели появятся на GenAI.mil \"в ближайшие недели и месяцы\" — но очевидно, что первенство Google дает компании конкурентное преимущество в борьбе за будущие контракты богатого оборонного ведомства."
      },
      {
        "type": "text",
        "content": "В 2018 году тысячи сотрудников Google подписали письмо протеста против Project Maven — контракта с Пентагоном на ИИ для анализа видео с дронов. Компания тогда свернула проект. Семь лет спустя Gemini появился на каждом рабочем столе американских военных."
      },
      {
        "type": "text",
        "content": "P.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/975320/",
    "site_type": "habr",
    "title": "Сэм Альтман отложил гонку за AGI. ChatGPT важнее",
    "date": "10.12.2025",
    "text": "The Wall Street Journal рассказывает подробности того, как генеральный директор OpenAI Сэм Альтман объявил в компании режим \"красный код\" и потребовал на восемь недель заморозить побочные проекты — включая видеогенератор Sora, рекламную платформу и даже наработки по AGI. Все ресурсы брошены на одну задачу: срочно улучшить ChatGPT. Издание описывает разворот так: \"убавить обороты в грандиозных исследовательских планах и просто дать людям чат-бот, которого они хотят\".\n\nПо данным WSJ, внутри OpenAI давно борются две логики. Продуктовая требует суперпопулярного ChatGPT для всех — он приносит деньги и оправдывает вложения в дата-центры. Исследовательская настаивает на приоритете прорывов и движения к AGI. \"Красный код\" явно усиливает первую линию: линейка передовых рассуждающих моделей, вроде той, которая завоевала золотые медали на международных олимпиадах, формально не закрывается, но временно отходит на второй план. По данным WSJ, это уже вызывает напряженность между продуктовиками и исследователями.\n\nГлавная причина разворота — успехи Google. Аудитория ИИ-продуктов компании выросла с 450 до 650 млн пользователей за несколько месяцев, а Gemini 3 Pro обошла разработки OpenAI в ключевых бенчмарках. При этом OpenAI остается убыточной, несет инфраструктурные обязательства на $1,4 трлн и не может финансировать гонку так же, как Google. Альтман называет Apple следующим серьезным соперником — решающей будет интеграция ИИ в устройства.\n\nWSJ разбирает и обратную сторону погони за популярностью. Чтобы сделать ChatGPT «человечнее», команда активно дообучает модели на пользовательских сигналах — оценках, кликах, паттернах использования. Но тот же механизм стимулирует льстивое, поддакивающее поведение. По данным издания, у части уязвимых пользователей это совпало с ухудшением психического состояния — образуются группы поддержки, против OpenAI подаются иски.\n\nФормально OpenAI не объявляет отказа от AGI. Но динамика показательна: ещё летом 2025 года часть руководства считала, что компания уже близка к этому уровню. Осенью Альтман сдвинул публичный горизонт к 2030 году, сославшись на финансовые и технические ограничения. Теперь исследовательская повестка временно уступает место тактике выживания на рынке с растущей конкуренцией.\n\nP.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/410/f30/bf6/410f30bf61712a8d33198b123a88893f.jpg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/410/f30/bf6/410f30bf61712a8d33198b123a88893f.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "The Wall Street Journal рассказывает подробности того, как генеральный директор OpenAI Сэм Альтман объявил в компании режим \"красный код\" и потребовал на восемь недель заморозить побочные проекты — включая видеогенератор Sora, рекламную платформу и даже наработки по AGI. Все ресурсы брошены на одну задачу: срочно улучшить ChatGPT. Издание описывает разворот так: \"убавить обороты в грандиозных исследовательских планах и просто дать людям чат-бот, которого они хотят\"."
      },
      {
        "type": "text",
        "content": "По данным WSJ, внутри OpenAI давно борются две логики. Продуктовая требует суперпопулярного ChatGPT для всех — он приносит деньги и оправдывает вложения в дата-центры. Исследовательская настаивает на приоритете прорывов и движения к AGI. \"Красный код\" явно усиливает первую линию: линейка передовых рассуждающих моделей, вроде той, которая завоевала золотые медали на международных олимпиадах, формально не закрывается, но временно отходит на второй план. По данным WSJ, это уже вызывает напряженность между продуктовиками и исследователями."
      },
      {
        "type": "text",
        "content": "Главная причина разворота — успехи Google. Аудитория ИИ-продуктов компании выросла с 450 до 650 млн пользователей за несколько месяцев, а Gemini 3 Pro обошла разработки OpenAI в ключевых бенчмарках. При этом OpenAI остается убыточной, несет инфраструктурные обязательства на $1,4 трлн и не может финансировать гонку так же, как Google. Альтман называет Apple следующим серьезным соперником — решающей будет интеграция ИИ в устройства."
      },
      {
        "type": "text",
        "content": "WSJ разбирает и обратную сторону погони за популярностью. Чтобы сделать ChatGPT «человечнее», команда активно дообучает модели на пользовательских сигналах — оценках, кликах, паттернах использования. Но тот же механизм стимулирует льстивое, поддакивающее поведение. По данным издания, у части уязвимых пользователей это совпало с ухудшением психического состояния — образуются группы поддержки, против OpenAI подаются иски."
      },
      {
        "type": "text",
        "content": "Формально OpenAI не объявляет отказа от AGI. Но динамика показательна: ещё летом 2025 года часть руководства считала, что компания уже близка к этому уровню. Осенью Альтман сдвинул публичный горизонт к 2030 году, сославшись на финансовые и технические ограничения. Теперь исследовательская повестка временно уступает место тактике выживания на рынке с растущей конкуренцией."
      },
      {
        "type": "text",
        "content": "P.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/975316/",
    "site_type": "habr",
    "title": "Конгресс впервые приказал Пентагону готовиться к эпохе AGI",
    "date": "10.12.2025",
    "text": "Конгресс США включил в новый оборонный бюджет пункт, который уже называют историческим. В документе стоимостью около 900 млрд долларов появляется обязательство для Минобороны создать к апрелю 2026 года специальный Комитет по будущему ИИ. Это первый официальный мандат, где военным поручено не просто следить за развитием технологий, а изучать потенциальное появление систем, способных работать на уровне человека или выше. Комитету предстоит анализировать ключевые направления, которые могут приблизить AGI, включая большие модели, агентные системы и нейроморфные вычисления. Он также займется оценкой прогресса в Китае, России, Иране и Северной Корее, чтобы понять, где конкуренты могут догнать или перегнать США.\n\nОтдельное внимание уделяется принципу человеческого контроля. Законопроект требует, чтобы Пентагон разработал технические и процедурные механизмы, позволяющие человеку отменить любое решение ИИ. Политики подчеркивают, что будущие автономные системы должны оставаться инструментом, а не самостоятельным субъектом принятия решений.\n\nКомитет возглавят заместитель министра обороны и зампред Объединенного комитета начальников штабов. К работе подключаются руководители видов войск и главный AI-офицер Пентагона, а итоговый доклад в Конгресс должен быть представлен до января 2027 года. Фактически США объявили, что начинают стратегическую подготовку к миру, где AGI может появиться через пять лет или через десятилетия.\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/a4b/4b5/142/a4b4b514251dcd7d1219eac616edf220.jpeg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/a4b/4b5/142/a4b4b514251dcd7d1219eac616edf220.jpeg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Конгресс США включил в новый оборонный бюджет пункт, который уже называют историческим. В документе стоимостью около 900 млрд долларов появляется обязательство для Минобороны создать к апрелю 2026 года специальный Комитет по будущему ИИ. Это первый официальный мандат, где военным поручено не просто следить за развитием технологий, а изучать потенциальное появление систем, способных работать на уровне человека или выше. Комитету предстоит анализировать ключевые направления, которые могут приблизить AGI, включая большие модели, агентные системы и нейроморфные вычисления. Он также займется оценкой прогресса в Китае, России, Иране и Северной Корее, чтобы понять, где конкуренты могут догнать или перегнать США."
      },
      {
        "type": "text",
        "content": "Отдельное внимание уделяется принципу человеческого контроля. Законопроект требует, чтобы Пентагон разработал технические и процедурные механизмы, позволяющие человеку отменить любое решение ИИ. Политики подчеркивают, что будущие автономные системы должны оставаться инструментом, а не самостоятельным субъектом принятия решений."
      },
      {
        "type": "text",
        "content": "Комитет возглавят заместитель министра обороны и зампред Объединенного комитета начальников штабов. К работе подключаются руководители видов войск и главный AI-офицер Пентагона, а итоговый доклад в Конгресс должен быть представлен до января 2027 года. Фактически США объявили, что начинают стратегическую подготовку к миру, где AGI может появиться через пять лет или через десятилетия."
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/975386/",
    "site_type": "habr",
    "title": "DeepSeek V4 хотят выпустить к концу января. Модель тренируют на контрабандных Nvidia Blackwell",
    "date": "10.12.2025",
    "text": "По данным The Information, DeepSeek использует несколько тысяч чипов Nvidia Blackwell для разработки следующей модели — условной DeepSeek V4 (или R2, если придерживаться старого варианта именования). Эти новейшие чипы запрещено экспортировать в Китай — но, как утверждают шесть источников издания, их ввозят контрабандой: серверы с Blackwell сначала отправляют в дата-центры стран, где экспорт разрешен, там разбирают и переправляют компоненты в Китай по частям.\n\nЧасть сотрудников DeepSeek рассчитывает выпустить новую модель к китайскому Новому году — концу января. Впрочем, основатель компании Лян Вэньфэн жесткий дедлайн не ставит и, по словам источников, ставит качество выше сроков. В сентябре DeepSeek выпустила V3.2-Exp — экспериментальную модель, которую сама компания называет \"промежуточным шагом к следующему поколению\". Но перенос нового подхода на большие модели идёт медленно.\n\nКлючевая ставка DeepSeek — архитектура DeepSeek Sparse Attention (DSA), которая должна резко удешевить запуск модели для конечных пользователей за счет разреженных вычислений. Blackwell для этого подходит идеально: чипы B200 ускоряют такие операции примерно вдвое по сравнению с предыдущим поколением и дают в 2,5 раза больше производительности на инференсе LLM, чем H200.\n\nБуквально только что Трамп разрешил Nvidia продавать H200 в Китай с 25-процентным сбором в пользу США. Но даже эти чипы не станут для китайских компаний \"серебряной пулей\". По данным Financial Times, Пекин сам ограничит к ним доступ: компаниям придется доказывать, что отечественных чипов недостаточно, а госзаказчиков будут подталкивать к решениям Huawei и других локальных производителей. В итоге китайские AI-компании продолжают жить на \"лоскутном\" железе: старые запасы Nvidia, локальные чипы, тренировка моделей за рубежом, нелегально ввезенные чипы. Запуск DeepSeek V4 покажет, смогла ли ИИ-индустрия страны адаптироваться к такой ситуации.\n\nP.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/065/953/138/065953138a20ff2dc4d98d3bf8743c46.jpg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/065/953/138/065953138a20ff2dc4d98d3bf8743c46.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "По данным The Information, DeepSeek использует несколько тысяч чипов Nvidia Blackwell для разработки следующей модели — условной DeepSeek V4 (или R2, если придерживаться старого варианта именования). Эти новейшие чипы запрещено экспортировать в Китай — но, как утверждают шесть источников издания, их ввозят контрабандой: серверы с Blackwell сначала отправляют в дата-центры стран, где экспорт разрешен, там разбирают и переправляют компоненты в Китай по частям."
      },
      {
        "type": "text",
        "content": "Часть сотрудников DeepSeek рассчитывает выпустить новую модель к китайскому Новому году — концу января. Впрочем, основатель компании Лян Вэньфэн жесткий дедлайн не ставит и, по словам источников, ставит качество выше сроков. В сентябре DeepSeek выпустила V3.2-Exp — экспериментальную модель, которую сама компания называет \"промежуточным шагом к следующему поколению\". Но перенос нового подхода на большие модели идёт медленно."
      },
      {
        "type": "text",
        "content": "Ключевая ставка DeepSeek — архитектура DeepSeek Sparse Attention (DSA), которая должна резко удешевить запуск модели для конечных пользователей за счет разреженных вычислений. Blackwell для этого подходит идеально: чипы B200 ускоряют такие операции примерно вдвое по сравнению с предыдущим поколением и дают в 2,5 раза больше производительности на инференсе LLM, чем H200."
      },
      {
        "type": "text",
        "content": "Буквально только что Трамп разрешил Nvidia продавать H200 в Китай с 25-процентным сбором в пользу США. Но даже эти чипы не станут для китайских компаний \"серебряной пулей\". По данным Financial Times, Пекин сам ограничит к ним доступ: компаниям придется доказывать, что отечественных чипов недостаточно, а госзаказчиков будут подталкивать к решениям Huawei и других локальных производителей. В итоге китайские AI-компании продолжают жить на \"лоскутном\" железе: старые запасы Nvidia, локальные чипы, тренировка моделей за рубежом, нелегально ввезенные чипы. Запуск DeepSeek V4 покажет, смогла ли ИИ-индустрия страны адаптироваться к такой ситуации."
      },
      {
        "type": "text",
        "content": "P.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/975590/",
    "site_type": "habr",
    "title": "Google делает ставку на ИИ: Амину Вахдату доверяют всю инфраструктуру вычислений",
    "date": "11.12.2025",
    "text": "Google назначила Амина Вахдата главным технологом по инфраструктуре ИИ. Он теперь напрямую подчиняется генеральному директору Alphabet и отвечает за все ключевые вычислительные мощности компании. Назначение подчёркивает, что Google рассматривает вычислительную инфраструктуру как стратегический актив в глобальной гонке ИИ. В 2025 году компания вложила около 93 миллиардов долларов в капитальные расходы, и эта сумма, по прогнозам, будет только расти.\n\nАмин Вахдат работает в Google более 15 лет. Он руководил разработкой TPU-чипов, проекта дата-центров Jupiter с пропускной способностью до 13 петабит в секунду и системой управления вычислительными кластерами Borg. В 2025 году под его руководством представили новые TPU с кодовым названием Ironwood, которые обеспечивают производительность в десятки раз выше старых суперкомпьютеров.\n\nТеперь Вахдат отвечает за всю вычислительную и сетевую инфраструктуру, которая поддерживает бизнес ИИ Google. Это означает, что скорость, масштаб и эффективность вычислений становятся ключевыми факторами для компании в гонке за лидерство в области искусственного интеллекта. Будем следить за новостями!\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/869/dce/c1f/869dcec1fdd7cf087dcb2708086a6e3d.png",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/869/dce/c1f/869dcec1fdd7cf087dcb2708086a6e3d.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Google назначила Амина Вахдата главным технологом по инфраструктуре ИИ. Он теперь напрямую подчиняется генеральному директору Alphabet и отвечает за все ключевые вычислительные мощности компании. Назначение подчёркивает, что Google рассматривает вычислительную инфраструктуру как стратегический актив в глобальной гонке ИИ. В 2025 году компания вложила около 93 миллиардов долларов в капитальные расходы, и эта сумма, по прогнозам, будет только расти."
      },
      {
        "type": "text",
        "content": "Амин Вахдат работает в Google более 15 лет. Он руководил разработкой TPU-чипов, проекта дата-центров Jupiter с пропускной способностью до 13 петабит в секунду и системой управления вычислительными кластерами Borg. В 2025 году под его руководством представили новые TPU с кодовым названием Ironwood, которые обеспечивают производительность в десятки раз выше старых суперкомпьютеров."
      },
      {
        "type": "text",
        "content": "Теперь Вахдат отвечает за всю вычислительную и сетевую инфраструктуру, которая поддерживает бизнес ИИ Google. Это означает, что скорость, масштаб и эффективность вычислений становятся ключевыми факторами для компании в гонке за лидерство в области искусственного интеллекта. Будем следить за новостями!"
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/975630/",
    "site_type": "habr",
    "title": "Qwen3-Omni-Flash: китайская модель догнала Gemini 2.5 Pro по аудио — и стоит в 3 раза дешевле",
    "date": "11.12.2025",
    "text": "Alibaba выпустила обновленную версию Qwen3-Omni-Flash — мультимодальную модель, которая понимает текст, изображения, аудио и видео, а также отвечает голосом в реальном времени. По ключевым аудио-бенчмаркам она догоняет Gemini 2.5 Pro и обгоняет GPT-4o (до сих пор используется как основной голосовой ассистент в ChatGPT).\n\nНа распознавании речи (ASR) Qwen3-Omni-Flash показывает ошибку 2,74% на английском и 2,19% на китайском — это лучше, чем у Gemini 2.5 Pro (2,94% и 2,71%) и GPT-4o (3,32% и 2,44%). На сложных задачах вроде распознавания вокала в песнях разрыв ещё заметнее: 5,85% у Qwen против 9,85% у Gemini и 11,87% у GPT-4o. По мультиязычному бенчмарку Fleurs (19 языков) GPT-4o пока впереди (4,48%), но Qwen (5,31%) обходит Gemini (5,55%). На видео-задачах картина похожая: по длинным видео (MLVU) Qwen набирает 75,7 против 64,6 у GPT-4o, а на аудиовизуальных бенчмарках вроде WorldSense и VideoHolmes обгоняет Gemini 2.5 Flash.\n\nЧисло поддерживаемых голосов в обновлении выросло с 17 до 49 — это не просто разные тембры, а полноценные \"персонажи\" с разным характером, возрастом и манерой речи. Языков озвучки теперь 10: английский, китайский, французский, немецкий, русский, итальянский, испанский, португальский, японский и корейский. Для распознавания речи поддерживаются 19 языков, включая арабский, турецкий, вьетнамский и кантонский. Также Alibaba отмечает \"значительное улучшение способности следовать инструкциям\" — модель лучше понимает промпты и системные настройки.\n\nПо цене Qwen3-Omni-Flash заметно выгоднее конкурентов: $0,43 за миллион входных токенов и $2 за миллион выходных. Для сравнения: GPT-4o стоит $2,50 и $10, Gemini 2.5 Pro — от $1,25 до $2,50 на входе и $10–15 на выходе. Получается разница в 3–5 раз. При этом контекстное окно — около 65 тысяч токенов в режиме рассуждений и 49 тысяч в обычном. В отличие от многих других моделей, эта версия доступна только через Qwen API и сервисы Alibaba Cloud, а вот открытые веса не выпущены.\n\nP.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/6c6/4d8/434/6c64d84341bd68091f95390c9d919eaa.jpg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/6c6/4d8/434/6c64d84341bd68091f95390c9d919eaa.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Alibaba выпустила обновленную версию Qwen3-Omni-Flash — мультимодальную модель, которая понимает текст, изображения, аудио и видео, а также отвечает голосом в реальном времени. По ключевым аудио-бенчмаркам она догоняет Gemini 2.5 Pro и обгоняет GPT-4o (до сих пор используется как основной голосовой ассистент в ChatGPT)."
      },
      {
        "type": "text",
        "content": "На распознавании речи (ASR) Qwen3-Omni-Flash показывает ошибку 2,74% на английском и 2,19% на китайском — это лучше, чем у Gemini 2.5 Pro (2,94% и 2,71%) и GPT-4o (3,32% и 2,44%). На сложных задачах вроде распознавания вокала в песнях разрыв ещё заметнее: 5,85% у Qwen против 9,85% у Gemini и 11,87% у GPT-4o. По мультиязычному бенчмарку Fleurs (19 языков) GPT-4o пока впереди (4,48%), но Qwen (5,31%) обходит Gemini (5,55%). На видео-задачах картина похожая: по длинным видео (MLVU) Qwen набирает 75,7 против 64,6 у GPT-4o, а на аудиовизуальных бенчмарках вроде WorldSense и VideoHolmes обгоняет Gemini 2.5 Flash."
      },
      {
        "type": "text",
        "content": "Число поддерживаемых голосов в обновлении выросло с 17 до 49 — это не просто разные тембры, а полноценные \"персонажи\" с разным характером, возрастом и манерой речи. Языков озвучки теперь 10: английский, китайский, французский, немецкий, русский, итальянский, испанский, португальский, японский и корейский. Для распознавания речи поддерживаются 19 языков, включая арабский, турецкий, вьетнамский и кантонский. Также Alibaba отмечает \"значительное улучшение способности следовать инструкциям\" — модель лучше понимает промпты и системные настройки."
      },
      {
        "type": "text",
        "content": "По цене Qwen3-Omni-Flash заметно выгоднее конкурентов: $0,43 за миллион входных токенов и $2 за миллион выходных. Для сравнения: GPT-4o стоит $2,50 и $10, Gemini 2.5 Pro — от $1,25 до $2,50 на входе и $10–15 на выходе. Получается разница в 3–5 раз. При этом контекстное окно — около 65 тысяч токенов в режиме рассуждений и 49 тысяч в обычном. В отличие от многих других моделей, эта версия доступна только через Qwen API и сервисы Alibaba Cloud, а вот открытые веса не выпущены."
      },
      {
        "type": "text",
        "content": "P.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/975658/",
    "site_type": "habr",
    "title": "Cursor представили Debug Mode",
    "date": "11.12.2025",
    "text": "Cursor внедрили Debug Mode, отдельный рабочий цикл для кодовых агентов, который впервые сочетает автоматическую диагностику, инструментирование рантайма и обязательную верификацию со стороны разработчика. Цель нововведения не ускорить генерацию кода, а научить модель системно устранять баги, которые прежде оказывались для нее слишком «глубокими».\n\nПроцесс начинается не с попытки предложить фиксы, а с аналитики. Агент читает проект и формирует набор гипотез о возможных причинах бага. Это важное отличие от классической работы LLM: вместо проб и ошибок предлагается почти инженерный подход к отладке. После этого агент сам вставляет в код диагностические логи, нацеленные на проверку сформулированных гипотез.\n\nДалее пользователю нужно воспроизвести баг уже в реальном приложении. Пока ошибка проявляется, Debug Mode собирает срез состояния программы: значения переменных, путь исполнения, тайминги. По сути формируется минимальный набор рантайм-фактов, достаточный для точечного исправления.\n\nНа основе полученных логов агент генерирует лаконичный фикс. Разработчики подчёркивают, что речь почти всегда идет о нескольких строках, а не о сотнях строк переписанного кода, как это нередко бывает при обычном взаимодействии с моделями. Это выводит инструмент в отдельную категорию: не «кодогенератор», а ассистент-отладчик.\n\nПрежде чем считать работу завершённой, система требует повторного прогона. Пользователь воспроизводит баг с включенным фиксом и оценивает результат.\n\nЕсли дефект исчез, агент удаляет диагностические вставки и оставляет проект в чистом состоянии. Если нет, цикл повторяется: больше логов, новое наблюдение, уточнённая гипотеза, обновлённый фикс.\n\nDebug Mode можно рассматривать как попытку формализовать лучшие практики живых инженеров внутри агентной архитектуры. Модель берёт на себя рутину — инструментирование, перебор гипотез, сбор телеметрии — а человек делает точечные решения.\n\nДрузья! Эту новость подготовила команда ТГК « AI for Devs » — канала, где мы рассказываем про AI-ассистентов, плагины для IDE, делимся практическими кейсами и свежими новостями из мира ИИ. Подписывайтесь , чтобы быть в курсе и ничего не упустить!",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/d06/6a8/148/d066a81482f4fe77b245ab293d3beffc.png",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "text",
        "content": "Cursor внедрили Debug Mode, отдельный рабочий цикл для кодовых агентов, который впервые сочетает автоматическую диагностику, инструментирование рантайма и обязательную верификацию со стороны разработчика. Цель нововведения не ускорить генерацию кода, а научить модель системно устранять баги, которые прежде оказывались для нее слишком «глубокими»."
      },
      {
        "type": "text",
        "content": "Процесс начинается не с попытки предложить фиксы, а с аналитики. Агент читает проект и формирует набор гипотез о возможных причинах бага. Это важное отличие от классической работы LLM: вместо проб и ошибок предлагается почти инженерный подход к отладке. После этого агент сам вставляет в код диагностические логи, нацеленные на проверку сформулированных гипотез."
      },
      {
        "type": "text",
        "content": "Далее пользователю нужно воспроизвести баг уже в реальном приложении. Пока ошибка проявляется, Debug Mode собирает срез состояния программы: значения переменных, путь исполнения, тайминги. По сути формируется минимальный набор рантайм-фактов, достаточный для точечного исправления."
      },
      {
        "type": "text",
        "content": "На основе полученных логов агент генерирует лаконичный фикс. Разработчики подчёркивают, что речь почти всегда идет о нескольких строках, а не о сотнях строк переписанного кода, как это нередко бывает при обычном взаимодействии с моделями. Это выводит инструмент в отдельную категорию: не «кодогенератор», а ассистент-отладчик."
      },
      {
        "type": "text",
        "content": "Прежде чем считать работу завершённой, система требует повторного прогона. Пользователь воспроизводит баг с включенным фиксом и оценивает результат."
      },
      {
        "type": "text",
        "content": "Если дефект исчез, агент удаляет диагностические вставки и оставляет проект в чистом состоянии. Если нет, цикл повторяется: больше логов, новое наблюдение, уточнённая гипотеза, обновлённый фикс."
      },
      {
        "type": "text",
        "content": "Debug Mode можно рассматривать как попытку формализовать лучшие практики живых инженеров внутри агентной архитектуры. Модель берёт на себя рутину — инструментирование, перебор гипотез, сбор телеметрии — а человек делает точечные решения."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/d06/6a8/148/d066a81482f4fe77b245ab293d3beffc.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Друзья! Эту новость подготовила команда ТГК « AI for Devs » — канала, где мы рассказываем про AI-ассистентов, плагины для IDE, делимся практическими кейсами и свежими новостями из мира ИИ. Подписывайтесь , чтобы быть в курсе и ничего не упустить!"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/975682/",
    "site_type": "habr",
    "title": "Бум майнинга в США закончился — дата-центры уходят в ИИ",
    "date": "11.12.2025",
    "text": "Крупные биткоин-майнеры невольно создали идеальную инфраструктуру для ИИ-бума: гигантские дата-центры с подведенным электричеством, охлаждением и договорами на дешевую энергию. Теперь ИИ-компании, которым строить с нуля слишком долго, забирают все это себе — если просто выкинуть майнинговое оборудование и поставить GPU, то все равно получается выгодно.\n\nМасштаб исхода впечатляет: минимум восемь публичных компаний объявили о переходе в ИИ, за последние месяцы подписаны контракты на $43 млрд с Amazon, Microsoft и Google. Крупнейшая биткоин-ферма мира — объект Riot Platforms в Техасе — на две трети превращается в ИИ-фабрику. Полтора года назад ее строили как храм майнинга.\n\nЭкономика проста: майнинг стал лотереей с падающими шансами. Награда за блок в прошлом году автоматически сократилась вдвое, конкуренция выросла, биткоин просел на 30% от январского пика. \"Покупаю оборудование для майнинга — и не знаю, отобью ли вложения\", — признает один из руководителей отрасли. ИИ-контракты, напротив, гарантируют предсказуемый доход на годы вперед, а акции компаний растут сразу после объявления о пивоте.\n\nСреди крупных игроков продолжает держаться American Bitcoin, компания Эрика Трампа. Ее президент настаивает, что бизнес строился именно под майнинг, а не как \"универсальный инфраструктурный оператор\". Но давление акционеров растет: если конкуренты получают стопроцентный рост акций после сделки с ИИ-гигантом, устоять сложно.\n\nОткрытым остается вопрос: кто вообще будет майнить биткоин через несколько лет? Аналитики предполагают, что добыча уйдет в страны с дешевой энергией — вроде Парагвая — или станет делом государств. Бутан, Сальвадор, США накопили биткоин в национальных резервах и не могут допустить угрозы сети. Возможно, майнинг в убыток станет вопросом национальной безопасности.\n\nP.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/f9d/eb8/696/f9deb8696026589a3226484f6ebd5f28.jpg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/f9d/eb8/696/f9deb8696026589a3226484f6ebd5f28.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Крупные биткоин-майнеры невольно создали идеальную инфраструктуру для ИИ-бума: гигантские дата-центры с подведенным электричеством, охлаждением и договорами на дешевую энергию. Теперь ИИ-компании, которым строить с нуля слишком долго, забирают все это себе — если просто выкинуть майнинговое оборудование и поставить GPU, то все равно получается выгодно."
      },
      {
        "type": "text",
        "content": "Масштаб исхода впечатляет: минимум восемь публичных компаний объявили о переходе в ИИ, за последние месяцы подписаны контракты на $43 млрд с Amazon, Microsoft и Google. Крупнейшая биткоин-ферма мира — объект Riot Platforms в Техасе — на две трети превращается в ИИ-фабрику. Полтора года назад ее строили как храм майнинга."
      },
      {
        "type": "text",
        "content": "Экономика проста: майнинг стал лотереей с падающими шансами. Награда за блок в прошлом году автоматически сократилась вдвое, конкуренция выросла, биткоин просел на 30% от январского пика. \"Покупаю оборудование для майнинга — и не знаю, отобью ли вложения\", — признает один из руководителей отрасли. ИИ-контракты, напротив, гарантируют предсказуемый доход на годы вперед, а акции компаний растут сразу после объявления о пивоте."
      },
      {
        "type": "text",
        "content": "Среди крупных игроков продолжает держаться American Bitcoin, компания Эрика Трампа. Ее президент настаивает, что бизнес строился именно под майнинг, а не как \"универсальный инфраструктурный оператор\". Но давление акционеров растет: если конкуренты получают стопроцентный рост акций после сделки с ИИ-гигантом, устоять сложно."
      },
      {
        "type": "text",
        "content": "Открытым остается вопрос: кто вообще будет майнить биткоин через несколько лет? Аналитики предполагают, что добыча уйдет в страны с дешевой энергией — вроде Парагвая — или станет делом государств. Бутан, Сальвадор, США накопили биткоин в национальных резервах и не могут допустить угрозы сети. Возможно, майнинг в убыток станет вопросом национальной безопасности."
      },
      {
        "type": "text",
        "content": "P.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/975798/",
    "site_type": "habr",
    "title": "Starcloud впервые обучил ИИ в космосе и готовит орбитальные дата центры",
    "date": "11.12.2025",
    "text": "Стартап Starcloud провел уникальный эксперимент: он запустил на орбиту спутник с графическим процессором Nvidia H100 и начал выполнять на нем полноценные операции с нейросетями. Сейчас на борту спутника работает модель Gemma, и это первый случай, когда LLM не просто выполняет вычисления в космосе, а функционирует как полноценная система.\n\nКомпания параллельно протестировала обучение упрощенной версии NanoGPT. В качестве датасета использовали полный набор произведений Уильяма Шекспира. Эксперимент подтвердил, что орбитальные платформы способны выдерживать стабильные вычисления даже при обучении модели.\n\nStarcloud считает, что космические дата центры могут быть значительно дешевле наземных. Причина в том, что в космосе нет необходимости строить мощные системы охлаждения. Тепло можно без дополнительных затрат выводить в открытое пространство, а энергия поступает постоянно благодаря солнечным панелям. Такой подход снижает расходы и потенциально дает более высокую вычислительную эффективность.\n\nИдея космических дата центров получила поддержку и у других гигантов. Концепцию активно обсуждают Google и xAI. Эксперты считают, что подобная инфраструктура может стать новым форматом высокопроизводительных вычислений, особенно если стоимость вывода грузов на орбиту продолжит снижаться.\n\nТекущая миссия Starcloud пока остается экспериментальной. Однако первые результаты уже показывают, что орбита может стать подходящим местом для размещения ИИ систем, которые требуют большой мощности и стабильного энергопитания. Будем следить за новостями!\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/5a4/263/284/5a4263284281e7addd71f0f6b681bd17.webp",
        "alt": ""
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/e70/ace/b64/e70aceb6435643e32a318ac8d2f19d95.webp",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/5a4/263/284/5a4263284281e7addd71f0f6b681bd17.webp",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Стартап Starcloud провел уникальный эксперимент: он запустил на орбиту спутник с графическим процессором Nvidia H100 и начал выполнять на нем полноценные операции с нейросетями. Сейчас на борту спутника работает модель Gemma, и это первый случай, когда LLM не просто выполняет вычисления в космосе, а функционирует как полноценная система."
      },
      {
        "type": "text",
        "content": "Компания параллельно протестировала обучение упрощенной версии NanoGPT. В качестве датасета использовали полный набор произведений Уильяма Шекспира. Эксперимент подтвердил, что орбитальные платформы способны выдерживать стабильные вычисления даже при обучении модели."
      },
      {
        "type": "text",
        "content": "Starcloud считает, что космические дата центры могут быть значительно дешевле наземных. Причина в том, что в космосе нет необходимости строить мощные системы охлаждения. Тепло можно без дополнительных затрат выводить в открытое пространство, а энергия поступает постоянно благодаря солнечным панелям. Такой подход снижает расходы и потенциально дает более высокую вычислительную эффективность."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/e70/ace/b64/e70aceb6435643e32a318ac8d2f19d95.webp",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Идея космических дата центров получила поддержку и у других гигантов. Концепцию активно обсуждают Google и xAI. Эксперты считают, что подобная инфраструктура может стать новым форматом высокопроизводительных вычислений, особенно если стоимость вывода грузов на орбиту продолжит снижаться."
      },
      {
        "type": "text",
        "content": "Текущая миссия Starcloud пока остается экспериментальной. Однако первые результаты уже показывают, что орбита может стать подходящим местом для размещения ИИ систем, которые требуют большой мощности и стабильного энергопитания. Будем следить за новостями!"
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/975854/",
    "site_type": "habr",
    "title": "ИИ создал полноценный Linux-компьютер из 843 компонентов и он загрузился с первого раза",
    "date": "11.12.2025",
    "text": "Калифорнийский стартап Quilter продемонстрировал прорыв в проектировании аппаратного обеспечения с помощью искусственного интеллекта. Их ИИ смог за одну неделю проектировать двухплатный компьютер на базе Linux, причём система загружается и работает без единой ошибки с первой попытки. Обычно на такую задачу уходят месяцы работы опытных инженеров, но Quilter сумел сделать это намного быстрее.\n\nПроект под названием Project Speedrun занял всего 38,5 часов фактической человеческой работы по сравнению с примерно 428 часами, которые профессиональные инженеры по дизайну печатных плат обычно оценивают для аналогичных задач. Система спроектирована вокруг архитектуры i.MX 8M Mini с четырёхъядерным ARM-процессором, двумя гигабайтами памяти и 32 гигабайтами встроенного хранилища. Итог состоит из 843 компонентов и более 5 000 соединений на платах, которые прошли все тесты и успешно загрузили операционную систему Debian Linux с самого начала.\n\nОтличие подхода Quilter заключается в том, что их ИИ не обучался на примерах готовых плат. Инженеры компании создали систему, в которой ИИ учится проектировать через взаимодействие с физическими ограничениями и законами. Он получает обратную связь, когда проект удовлетворяет требованиям по электромагнитным, тепловым и производственным параметрам. Такой подход позволяет получать рабочий результат, а не просто имитировать человеческие действия.\n\nТони Фаделл, инженер, который руководил разработкой iPod и iPhone в Apple и позже основал Nest, выступает инвестором и советником Quilter. Он говорит, что раньше разработка печатных плат была узким местом в аппаратных проектах, и автоматизация этого этапа может радикально ускорить весь цикл создания новых устройств. Если технологии Quilter продолжат развиваться, это может привести к тому, что компании смогут проектировать и тестировать продукты с невероятной скоростью, значительно снижая затраты и сроки вывода на рынок.\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/8a1/519/8ea/8a15198eacc2bdd76e071a78c50c7eee.webp",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/8a1/519/8ea/8a15198eacc2bdd76e071a78c50c7eee.webp",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Калифорнийский стартап Quilter продемонстрировал прорыв в проектировании аппаратного обеспечения с помощью искусственного интеллекта. Их ИИ смог за одну неделю проектировать двухплатный компьютер на базе Linux, причём система загружается и работает без единой ошибки с первой попытки. Обычно на такую задачу уходят месяцы работы опытных инженеров, но Quilter сумел сделать это намного быстрее."
      },
      {
        "type": "text",
        "content": "Проект под названием Project Speedrun занял всего 38,5 часов фактической человеческой работы по сравнению с примерно 428 часами, которые профессиональные инженеры по дизайну печатных плат обычно оценивают для аналогичных задач. Система спроектирована вокруг архитектуры i.MX 8M Mini с четырёхъядерным ARM-процессором, двумя гигабайтами памяти и 32 гигабайтами встроенного хранилища. Итог состоит из 843 компонентов и более 5 000 соединений на платах, которые прошли все тесты и успешно загрузили операционную систему Debian Linux с самого начала."
      },
      {
        "type": "text",
        "content": "Отличие подхода Quilter заключается в том, что их ИИ не обучался на примерах готовых плат. Инженеры компании создали систему, в которой ИИ учится проектировать через взаимодействие с физическими ограничениями и законами. Он получает обратную связь, когда проект удовлетворяет требованиям по электромагнитным, тепловым и производственным параметрам. Такой подход позволяет получать рабочий результат, а не просто имитировать человеческие действия."
      },
      {
        "type": "text",
        "content": "Тони Фаделл, инженер, который руководил разработкой iPod и iPhone в Apple и позже основал Nest, выступает инвестором и советником Quilter. Он говорит, что раньше разработка печатных плат была узким местом в аппаратных проектах, и автоматизация этого этапа может радикально ускорить весь цикл создания новых устройств. Если технологии Quilter продолжат развиваться, это может привести к тому, что компании смогут проектировать и тестировать продукты с невероятной скоростью, значительно снижая затраты и сроки вывода на рынок."
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/975892/",
    "site_type": "habr",
    "title": "OpenAI представила GPT‑5.2 — новую версию ведущей языковой модели с улучшенной логикой и скоростью",
    "date": "11.12.2025",
    "text": "OpenAI официально анонсировала GPT‑5.2, обновлённую версию своей флагманской языковой модели. Компания описывает этот апдейт как шаг к ещё более надёжной, точной и быстрой генерации текста, где усилия сосредоточены на улучшении логического рассуждения, точности ответов и общей надежности работы. По словам OpenAI, GPT‑5.2 не просто увеличивает размер модели, а перераспределяет вычислительные ресурсы и улучшает способы обработки данных, чтобы снижать количество ошибок и неверных выводов в ответах.\n\nОдна из ключевых особенностей GPT‑5.2 - повышенная способность к последовательным рассуждениям. Это значит, что модель стала лучше удерживать логику через длинные цепочки аргументации и сложные запросы. Раньше при сложных вычислениях или при решении задач с большим количеством условий модели могли «запутаться» или выдавать противоречивые результаты. Новая версия значительно реже допускает такие погрешности благодаря усовершенствованной внутренней архитектуре и оптимизации вычислений.\n\nКроме того, GPT‑5.2 демонстрирует более высокий уровень устойчивости к вводящим в заблуждение примерам. В новых возможностях модели предусмотрены улучшенные механизмы редуцирования некорректных ответов и уменьшения уверенных, но неверных утверждений. Это часть общей стратегии OpenAI по повышению точности и ответственности работы моделей - важная дорожная карта в сторону более безопасных и надёжных ИИ‑инструментов.\n\nOpenAI также подчеркнула, что GPT‑5.2 работает быстрее и эффективнее в обработке запросов. Это означает, что пользователи увидят ускорение откликов без заметного увеличения затрат на инфраструктуру. Программисты и разработчики, которые интегрируют модель через API, смогут получить более отзывчивые и предсказуемые ответы в своих продуктах, что особенно важно для корпоративных решений и сложных рабочих процессов.\n\nЗапуск GPT‑5.2 - ещё один шаг OpenAI в направлении создания ИИ, который не просто отвечает на вопросы, а понимает их глубже и точнее. Компания продолжает фокусироваться на улучшении качества, точности, безопасности и продуктивности моделей, что отражает стремление сделать ИИ мощным инструментом реального мира, а не только демонстрацией технологических возможностей.\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/1fc/d10/c54/1fcd10c5417feac9c360e4bbb5fff092.png",
        "alt": ""
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/e58/470/4d3/e584704d34372eb3dbe32f6c76e05c91.png",
        "alt": ""
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/5bc/30d/7c9/5bc30d7c9bf5708a8a69239257a8d447.png",
        "alt": ""
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/76c/ffb/281/76cffb281f860f728c52cc2731d2c4a4.png",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/1fc/d10/c54/1fcd10c5417feac9c360e4bbb5fff092.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "OpenAI официально анонсировала GPT‑5.2, обновлённую версию своей флагманской языковой модели. Компания описывает этот апдейт как шаг к ещё более надёжной, точной и быстрой генерации текста, где усилия сосредоточены на улучшении логического рассуждения, точности ответов и общей надежности работы. По словам OpenAI, GPT‑5.2 не просто увеличивает размер модели, а перераспределяет вычислительные ресурсы и улучшает способы обработки данных, чтобы снижать количество ошибок и неверных выводов в ответах."
      },
      {
        "type": "text",
        "content": "Одна из ключевых особенностей GPT‑5.2 - повышенная способность к последовательным рассуждениям. Это значит, что модель стала лучше удерживать логику через длинные цепочки аргументации и сложные запросы. Раньше при сложных вычислениях или при решении задач с большим количеством условий модели могли «запутаться» или выдавать противоречивые результаты. Новая версия значительно реже допускает такие погрешности благодаря усовершенствованной внутренней архитектуре и оптимизации вычислений."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/e58/470/4d3/e584704d34372eb3dbe32f6c76e05c91.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Кроме того, GPT‑5.2 демонстрирует более высокий уровень устойчивости к вводящим в заблуждение примерам. В новых возможностях модели предусмотрены улучшенные механизмы редуцирования некорректных ответов и уменьшения уверенных, но неверных утверждений. Это часть общей стратегии OpenAI по повышению точности и ответственности работы моделей - важная дорожная карта в сторону более безопасных и надёжных ИИ‑инструментов."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/5bc/30d/7c9/5bc30d7c9bf5708a8a69239257a8d447.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "OpenAI также подчеркнула, что GPT‑5.2 работает быстрее и эффективнее в обработке запросов. Это означает, что пользователи увидят ускорение откликов без заметного увеличения затрат на инфраструктуру. Программисты и разработчики, которые интегрируют модель через API, смогут получить более отзывчивые и предсказуемые ответы в своих продуктах, что особенно важно для корпоративных решений и сложных рабочих процессов."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/76c/ffb/281/76cffb281f860f728c52cc2731d2c4a4.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Запуск GPT‑5.2 - ещё один шаг OpenAI в направлении создания ИИ, который не просто отвечает на вопросы, а понимает их глубже и точнее. Компания продолжает фокусироваться на улучшении качества, точности, безопасности и продуктивности моделей, что отражает стремление сделать ИИ мощным инструментом реального мира, а не только демонстрацией технологических возможностей."
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/975998/",
    "site_type": "habr",
    "title": "ИИ-функция Excel позволит не запоминать формулы",
    "date": "12.12.2025",
    "text": "Microsoft внедрила в электронные таблицы Excel функцию COPILOT(), которая позволяет пользователям описывать задачи на естественном языке. Теперь компания дополняет работу этой опции формулами.\n\nВстроенный в сетку инструмент Copilot позволяет пользователям писать запросы на естественном языке, а затем ИИ генерирует формулу вместе с предварительным просмотром результата. Например, можно просто выбрать ячейку, где требуется получить результат, ввести «=» и «Рассчитать общую прибыль» в подсказке Copilot, после чего ИИ-помощник предложит формулу и покажет итоги расчётов. Если результат устраивает, то потребуется выбрать «Сохранить», либо нажать «Отменить» и повторить процесс.\n\nПанель подсказок Copilot также позволяет изменять существующие формулы, объединять и использовать данные из разных листов, уточнять сгенерированную формулу в соответствии с более детальными требованиями через интерфейс на естественном языке.\n\nПока Copilot ограничен генерацией только одной формулы, обрабатывающей один диапазон за раз, и доступен только клиентам Microsoft 365 Copilot, использующим Excel в веб-версии. Microsoft рассматривает возможность добавления поддержки нескольких формул в будущих обновлениях.\n\nТестирование функции COPILOT() в Excel запустили летом. Она интегрирована с вычислительным механизмом сервиса, и пользователю не нужно вручную обновлять книгу, чтобы применять изменения к сгенерированным ИИ результатам каждый раз при апдейте данных. Кроме того, COPILOT() может работать в паре с другими функциями Excel, такими как IF, SWITCH или LAMBDA.\n\nВ сентябре Microsoft обновила опцию. Она начала предлагать автодополнение, что избавляет пользователей от необходимости проверки синтаксиса, функций, ссылок на нужные ячейки т. д.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/290/5ec/1b2/2905ec1b2ca1fec8d222726acb08e290.jpeg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "text",
        "content": "Microsoft внедрила в электронные таблицы Excel функцию COPILOT(), которая позволяет пользователям описывать задачи на естественном языке. Теперь компания дополняет работу этой опции формулами."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/290/5ec/1b2/2905ec1b2ca1fec8d222726acb08e290.jpeg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Встроенный в сетку инструмент Copilot позволяет пользователям писать запросы на естественном языке, а затем ИИ генерирует формулу вместе с предварительным просмотром результата. Например, можно просто выбрать ячейку, где требуется получить результат, ввести «=» и «Рассчитать общую прибыль» в подсказке Copilot, после чего ИИ-помощник предложит формулу и покажет итоги расчётов. Если результат устраивает, то потребуется выбрать «Сохранить», либо нажать «Отменить» и повторить процесс."
      },
      {
        "type": "text",
        "content": "Панель подсказок Copilot также позволяет изменять существующие формулы, объединять и использовать данные из разных листов, уточнять сгенерированную формулу в соответствии с более детальными требованиями через интерфейс на естественном языке."
      },
      {
        "type": "text",
        "content": "Пока Copilot ограничен генерацией только одной формулы, обрабатывающей один диапазон за раз, и доступен только клиентам Microsoft 365 Copilot, использующим Excel в веб-версии. Microsoft рассматривает возможность добавления поддержки нескольких формул в будущих обновлениях."
      },
      {
        "type": "text",
        "content": "Тестирование функции COPILOT() в Excel запустили летом. Она интегрирована с вычислительным механизмом сервиса, и пользователю не нужно вручную обновлять книгу, чтобы применять изменения к сгенерированным ИИ результатам каждый раз при апдейте данных. Кроме того, COPILOT() может работать в паре с другими функциями Excel, такими как IF, SWITCH или LAMBDA."
      },
      {
        "type": "text",
        "content": "В сентябре Microsoft обновила опцию. Она начала предлагать автодополнение, что избавляет пользователей от необходимости проверки синтаксиса, функций, ссылок на нужные ячейки т. д."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/976074/",
    "site_type": "habr",
    "title": "Disney подал письмо с требованием к Google из-за массового нарушения авторских прав в ИИ",
    "date": "12.12.2025",
    "text": "Крупнейшая американская развлекательная компания Disney направила Google официальное письмо с требованием прекратить, как она утверждает, широкомасштабное нарушение авторских прав через искусственный интеллект. Disney обвиняет Google в том, что его ИИ-сервисы создают и распространяют изображения и видеоматериалы с персонажами, защищёнными авторским правом, без разрешения правообладателя. В письме Disney называет это нарушение масштабным и добавляет, что модели вроде Gemini, Veo, Imagen и Nano Banana фактически действуют как автомат, который воспроизводит и распространяет материалы из богатой библиотеки Disney.\n\nСреди перечисленных примеров - популярные персонажи из «Короля льва», «Русалочки», «Моаны», «Холодного сердца», «Дэдпула» и других франшиз. Disney также утверждает, что многие из создаваемых изображений маркированы логотипом Gemini, что может вводить пользователей в заблуждение относительно того, что Google имеет право использовать такой контент. Disney настаивает, что неоднократно поднимал этот вопрос перед Google в течение нескольких месяцев, но не получил адекватной реакции и мер по защите своих прав, поэтому решил перейти к юридическим действиям.\n\nGoogle пока не подтвердил и не опроверг обвинения, но заявил, что намерен «взаимодействовать» с Disney по этим вопросам. Представитель компании отметил, что Google использует публичные данные из открытого интернета для обучения своих ИИ и внедрил механизмы защиты авторских прав вроде расширенной системы контроля контента и инструментов Content ID на YouTube, которые дают правообладателям инструменты для контроля своих материалов. Тем временем это противостояние появляется на фоне крупной сделки Disney с OpenAI, в рамках которой Disney инвестирует $1 млрд и лицензирует более 200 персонажей для генерации видео в приложении Sora.\n\nЮридическая борьба вокруг ИИ и авторских прав развивается в США и мире с ускорением. Disney уже ранее отправлял аналогичные требования другим компаниям, включая Meta и Character.AI, а также участвовал в исках против генеративных платформ за нарушение интеллектуальной собственности. Эта новая претензия к Google может стать одним из ключевых событий, которые определят, как крупные технологические корпорации будут использовать защищённые материалы в эпоху ИИ.\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/623/f7b/137/623f7b137eef3a4ce1411e70a5a54954.webp",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/623/f7b/137/623f7b137eef3a4ce1411e70a5a54954.webp",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Крупнейшая американская развлекательная компания Disney направила Google официальное письмо с требованием прекратить, как она утверждает, широкомасштабное нарушение авторских прав через искусственный интеллект. Disney обвиняет Google в том, что его ИИ-сервисы создают и распространяют изображения и видеоматериалы с персонажами, защищёнными авторским правом, без разрешения правообладателя. В письме Disney называет это нарушение масштабным и добавляет, что модели вроде Gemini, Veo, Imagen и Nano Banana фактически действуют как автомат, который воспроизводит и распространяет материалы из богатой библиотеки Disney."
      },
      {
        "type": "text",
        "content": "Среди перечисленных примеров - популярные персонажи из «Короля льва», «Русалочки», «Моаны», «Холодного сердца», «Дэдпула» и других франшиз. Disney также утверждает, что многие из создаваемых изображений маркированы логотипом Gemini, что может вводить пользователей в заблуждение относительно того, что Google имеет право использовать такой контент. Disney настаивает, что неоднократно поднимал этот вопрос перед Google в течение нескольких месяцев, но не получил адекватной реакции и мер по защите своих прав, поэтому решил перейти к юридическим действиям."
      },
      {
        "type": "text",
        "content": "Google пока не подтвердил и не опроверг обвинения, но заявил, что намерен «взаимодействовать» с Disney по этим вопросам. Представитель компании отметил, что Google использует публичные данные из открытого интернета для обучения своих ИИ и внедрил механизмы защиты авторских прав вроде расширенной системы контроля контента и инструментов Content ID на YouTube, которые дают правообладателям инструменты для контроля своих материалов. Тем временем это противостояние появляется на фоне крупной сделки Disney с OpenAI, в рамках которой Disney инвестирует $1 млрд и лицензирует более 200 персонажей для генерации видео в приложении Sora."
      },
      {
        "type": "text",
        "content": "Юридическая борьба вокруг ИИ и авторских прав развивается в США и мире с ускорением. Disney уже ранее отправлял аналогичные требования другим компаниям, включая Meta и Character.AI, а также участвовал в исках против генеративных платформ за нарушение интеллектуальной собственности. Эта новая претензия к Google может стать одним из ключевых событий, которые определят, как крупные технологические корпорации будут использовать защищённые материалы в эпоху ИИ."
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/976098/",
    "site_type": "habr",
    "title": "Браузер Brave начал тестирование режима агентного ИИ для автоматизированных задач",
    "date": "12.12.2025",
    "text": "Brave представила в своём браузере функцию просмотра веб-страниц с использованием голосового помощника на базе искусственного интеллекта Leo для выполнения автоматизированных задач за пользователя. Опция предназначена для таких задач, как автономный поиск информации в интернете, сравнение товаров, поиск промокодов и составление кратких обзоров новостей. Сейчас эта функция доступна только в версии Brave Nightly.\n\nРежим просмотра с использованием ИИ отключён по умолчанию. Функция представляет собой первый шаг Brave к более тесной интеграции ИИ с пользователем в браузере, ориентированном на конфиденциальность.\n\nКомпания подчеркнула, что использование ИИ-агентов для просмотра веб-страниц «опасно по своей сути» и не должно применяться для критически важных операций из-за рисков prompt-injection атак и потенциальной возможности неправильной интерпретации намерений пользователей.\n\nЧтобы сократить эти риски, новый режим работает в отдельном изолированном профиле, который не имеет доступ к файлам cookie пользователя, учётным данным и другой конфиденциальной информации.\n\nВ этом режиме также ограничен доступ к странице настроек браузера, интернет-магазину Chrome, помеченным системой безопасного просмотра Brave ресурсам и сайтам, не использующим HTTPS.\n\nВсе действия будут отображаться во вкладках, а любая рискованная операция выведет предупреждение пользователю с просьбой дать явное согласие.\n\nЗа работой этого режима будет следить механизм «проверки соответствия». Система аналогична недавно анонсированному решения для агентного режима Gemini в Chrome, где изолированная вторая модель оценивает соответствие действий ИИ-агента намерениям пользователя. Из-за изолированности вторая модель не подвержена prompt-injection атакам, направленным на основной агент.\n\nНовый режим можно протестировать в Brave, включив Brave's AI browsing на странице brave://flags.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/20b/929/9f5/20b9299f563f4a3e9b99f416a8d4eba3.jpg",
        "alt": ""
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/76f/9c2/2b1/76f9c22b1559f673fd0508b3a5c1d5b7.jpg",
        "alt": ""
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/ef6/03c/3b2/ef603c3b282db70212223e2a7451ede6.jpg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "text",
        "content": "Brave представила в своём браузере функцию просмотра веб-страниц с использованием голосового помощника на базе искусственного интеллекта Leo для выполнения автоматизированных задач за пользователя. Опция предназначена для таких задач, как автономный поиск информации в интернете, сравнение товаров, поиск промокодов и составление кратких обзоров новостей. Сейчас эта функция доступна только в версии Brave Nightly."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/20b/929/9f5/20b9299f563f4a3e9b99f416a8d4eba3.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Режим просмотра с использованием ИИ отключён по умолчанию. Функция представляет собой первый шаг Brave к более тесной интеграции ИИ с пользователем в браузере, ориентированном на конфиденциальность."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/76f/9c2/2b1/76f9c22b1559f673fd0508b3a5c1d5b7.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Компания подчеркнула, что использование ИИ-агентов для просмотра веб-страниц «опасно по своей сути» и не должно применяться для критически важных операций из-за рисков prompt-injection атак и потенциальной возможности неправильной интерпретации намерений пользователей."
      },
      {
        "type": "text",
        "content": "Чтобы сократить эти риски, новый режим работает в отдельном изолированном профиле, который не имеет доступ к файлам cookie пользователя, учётным данным и другой конфиденциальной информации."
      },
      {
        "type": "text",
        "content": "В этом режиме также ограничен доступ к странице настроек браузера, интернет-магазину Chrome, помеченным системой безопасного просмотра Brave ресурсам и сайтам, не использующим HTTPS."
      },
      {
        "type": "text",
        "content": "Все действия будут отображаться во вкладках, а любая рискованная операция выведет предупреждение пользователю с просьбой дать явное согласие."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/ef6/03c/3b2/ef603c3b282db70212223e2a7451ede6.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "За работой этого режима будет следить механизм «проверки соответствия». Система аналогична недавно анонсированному решения для агентного режима Gemini в Chrome, где изолированная вторая модель оценивает соответствие действий ИИ-агента намерениям пользователя. Из-за изолированности вторая модель не подвержена prompt-injection атакам, направленным на основной агент."
      },
      {
        "type": "text",
        "content": "Новый режим можно протестировать в Brave, включив Brave's AI browsing на странице brave://flags."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/976092/",
    "site_type": "habr",
    "title": "Google добавил генерацию речи в Gemini API",
    "date": "12.12.2025",
    "text": "Google официально расширил функциональность своего Gemini API, добавив полноценную генерацию речи на основе искусственного интеллекта. Теперь разработчики могут превращать текстовые ответы моделей Gemini в реалистичный голос, который звучит естественно и близко к человеческой речи. Это важное обновление, потому что раньше такие возможности требовали подключения отдельных TTS‑сервисов или внешних библиотек. С новым API разработчики получают готовый голосовой интерфейс, который можно использовать прямо в своих приложениях, сервисах и устройствах.\n\nНовая система синтеза речи поддерживает множество языков и акцентов. Это позволяет создавать голосовые интерфейсы не только на английском, но и на других международных языках. В настройках можно выбирать параметры озвучивания, такие как стиль голоса, тембр, скорость и выразительность, чтобы адаптировать звучание под конкретные задачи. Например, голосовой помощник для навигации может звучать энергично и ясно, а образовательное приложение может использовать более спокойный и мягкий голос для объяснений.\n\nРабота с генерацией звука осуществляется через стандартные REST‑вызовы к Gemini API. Разработчик передаёт текст и параметры желаемого голоса, а сервис возвращает готовый аудиофайл. Это значительно упрощает интеграцию, поскольку нет необходимости вручную настраивать аудио‑движки или конвертировать форматы. Полученный звуковой файл можно сразу воспроизводить пользователю в веб‑ или мобильном приложении, умном устройстве или любой платформе с поддержкой аудио.\n\nGoogle в документации отмечает, что синтез речи ориентирован на понятность и естественное звучание, но при работе с очень специфическими терминами или узкоспециальной лексикой возможны ошибки в произношении. Для таких случаев предусмотрены инструменты тонкой настройки, которые позволяют корректировать результат на уровне фонетики. Это важно для медицинских приложений, обучения или других профессиональных сфер, где точность звукового вывода имеет значение.\n\nДобавление речь‑генерации в Gemini API делает платформу ещё более универсальной. Теперь разработчики могут создавать мультимодальные ИИ‑приложения, где ИИ не только отвечает на вопросы текстом, но и произносит ответы вслух. Это особенно полезно для голосовых ассистентов, умных устройств, образовательных продуктов, навигационных систем, игр и приложений для людей с ограниченными возможностями, которым важно слышать, а не только читать ответ.\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/be8/f4f/8d1/be8f4f8d1f9fc45bbf1e492a1b816881.jpeg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/be8/f4f/8d1/be8f4f8d1f9fc45bbf1e492a1b816881.jpeg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Google официально расширил функциональность своего Gemini API, добавив полноценную генерацию речи на основе искусственного интеллекта. Теперь разработчики могут превращать текстовые ответы моделей Gemini в реалистичный голос, который звучит естественно и близко к человеческой речи. Это важное обновление, потому что раньше такие возможности требовали подключения отдельных TTS‑сервисов или внешних библиотек. С новым API разработчики получают готовый голосовой интерфейс, который можно использовать прямо в своих приложениях, сервисах и устройствах."
      },
      {
        "type": "text",
        "content": "Новая система синтеза речи поддерживает множество языков и акцентов. Это позволяет создавать голосовые интерфейсы не только на английском, но и на других международных языках. В настройках можно выбирать параметры озвучивания, такие как стиль голоса, тембр, скорость и выразительность, чтобы адаптировать звучание под конкретные задачи. Например, голосовой помощник для навигации может звучать энергично и ясно, а образовательное приложение может использовать более спокойный и мягкий голос для объяснений."
      },
      {
        "type": "text",
        "content": "Работа с генерацией звука осуществляется через стандартные REST‑вызовы к Gemini API. Разработчик передаёт текст и параметры желаемого голоса, а сервис возвращает готовый аудиофайл. Это значительно упрощает интеграцию, поскольку нет необходимости вручную настраивать аудио‑движки или конвертировать форматы. Полученный звуковой файл можно сразу воспроизводить пользователю в веб‑ или мобильном приложении, умном устройстве или любой платформе с поддержкой аудио."
      },
      {
        "type": "text",
        "content": "Google в документации отмечает, что синтез речи ориентирован на понятность и естественное звучание, но при работе с очень специфическими терминами или узкоспециальной лексикой возможны ошибки в произношении. Для таких случаев предусмотрены инструменты тонкой настройки, которые позволяют корректировать результат на уровне фонетики. Это важно для медицинских приложений, обучения или других профессиональных сфер, где точность звукового вывода имеет значение."
      },
      {
        "type": "text",
        "content": "Добавление речь‑генерации в Gemini API делает платформу ещё более универсальной. Теперь разработчики могут создавать мультимодальные ИИ‑приложения, где ИИ не только отвечает на вопросы текстом, но и произносит ответы вслух. Это особенно полезно для голосовых ассистентов, умных устройств, образовательных продуктов, навигационных систем, игр и приложений для людей с ограниченными возможностями, которым важно слышать, а не только читать ответ."
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/976174/",
    "site_type": "habr",
    "title": "DeepMind строит ИИ-лабораторию, где роботы будут синтезировать сотни новых материалов в день",
    "date": "12.12.2025",
    "text": "Google DeepMind объявил о строительстве первой автоматизированной научной лаборатории в Великобритании. Открытие запланировано на 2026 год. Роботы под управлением Gemini будут синтезировать и тестировать сотни материалов ежедневно — главная цель в том, чтобы найти сверхпроводники, работающие при комнатной температуре.\n\nТакие сверхпроводники — своего рода святой Грааль материаловедения. Материал будет проводить электричество с нулевым сопротивлением, а значит, без потерь энергии. Практические применения — от дешевых МРТ-аппаратов до энергосетей, которые не теряют электричество при передаче. Сегодня известные сверхпроводники требуют охлаждения до экстремально низких температур, что делает их применение дорогим и ограниченным.\n\nAI уже изменил правила поиска. В 2023 году DeepMind представил GNoME — нейросеть для предсказания стабильных кристаллических структур. Она обнаружила 2,2 млн потенциально стабильных материалов, из которых 380 000 — наиболее перспективные кандидаты для синтеза. Среди них — 52 000 слоистых соединений, похожих на графен (раньше было известно около 1000), и 528 потенциальных литий-ионных проводников для батарей. По оценке DeepMind, это эквивалент примерно 800 лет традиционных научных открытий.\n\nНо предсказание — только половина задачи. Материал нужно синтезировать и проверить. В партнерстве с Lawrence Berkeley National Laboratory роботизированная лаборатория A-Lab уже создала 41 новый материал по предсказаниям GNoME. Теперь DeepMind строит собственную версию — масштабнее и полностью интегрированную с Gemini.\n\nЛаборатория будет работать по замкнутому циклу: Gemini анализирует данные и предсказывает перспективные составы, роботы синтезируют образцы и измеряют их свойства (проводимость, структуру, стабильность), результаты возвращаются в модель для следующей итерации. По словам Пушмита Коли , вице-президента DeepMind по науке, \"AI теперь способен соединить цифровой мир с реальным открытием новых материалов\". Традиционный процесс, где ученый вручную тестирует один состав за другим, занимает месяцы или годы. Роботизированная лаборатория сможет проверять сотни кандидатов в день.\n\nP.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/8d2/f9e/a9e/8d2f9ea9e730ee7c85e009a0ddfce8b2.jpg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/8d2/f9e/a9e/8d2f9ea9e730ee7c85e009a0ddfce8b2.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Google DeepMind объявил о строительстве первой автоматизированной научной лаборатории в Великобритании. Открытие запланировано на 2026 год. Роботы под управлением Gemini будут синтезировать и тестировать сотни материалов ежедневно — главная цель в том, чтобы найти сверхпроводники, работающие при комнатной температуре."
      },
      {
        "type": "text",
        "content": "Такие сверхпроводники — своего рода святой Грааль материаловедения. Материал будет проводить электричество с нулевым сопротивлением, а значит, без потерь энергии. Практические применения — от дешевых МРТ-аппаратов до энергосетей, которые не теряют электричество при передаче. Сегодня известные сверхпроводники требуют охлаждения до экстремально низких температур, что делает их применение дорогим и ограниченным."
      },
      {
        "type": "text",
        "content": "AI уже изменил правила поиска. В 2023 году DeepMind представил GNoME — нейросеть для предсказания стабильных кристаллических структур. Она обнаружила 2,2 млн потенциально стабильных материалов, из которых 380 000 — наиболее перспективные кандидаты для синтеза. Среди них — 52 000 слоистых соединений, похожих на графен (раньше было известно около 1000), и 528 потенциальных литий-ионных проводников для батарей. По оценке DeepMind, это эквивалент примерно 800 лет традиционных научных открытий."
      },
      {
        "type": "text",
        "content": "Но предсказание — только половина задачи. Материал нужно синтезировать и проверить. В партнерстве с Lawrence Berkeley National Laboratory роботизированная лаборатория A-Lab уже создала 41 новый материал по предсказаниям GNoME. Теперь DeepMind строит собственную версию — масштабнее и полностью интегрированную с Gemini."
      },
      {
        "type": "text",
        "content": "Лаборатория будет работать по замкнутому циклу: Gemini анализирует данные и предсказывает перспективные составы, роботы синтезируют образцы и измеряют их свойства (проводимость, структуру, стабильность), результаты возвращаются в модель для следующей итерации. По словам Пушмита Коли , вице-президента DeepMind по науке, \"AI теперь способен соединить цифровой мир с реальным открытием новых материалов\". Традиционный процесс, где ученый вручную тестирует один состав за другим, занимает месяцы или годы. Роботизированная лаборатория сможет проверять сотни кандидатов в день."
      },
      {
        "type": "text",
        "content": "P.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/976214/",
    "site_type": "habr",
    "title": "Вайб-кодинг теперь и для дизайнеров: Cursor выпустил визуальный редактор кода",
    "date": "12.12.2025",
    "text": "Компания Cursor представила Visual Editor — инструмент, который позволяет дизайнерам редактировать интерфейсы веб-приложений без написания кода. Достаточно перетаскивать элементы, двигать слайдеры стилей или просто описывать правки словами — ИИ-агент сам внесет изменения в кодовую базу. Релиз вошел в обновление Cursor 2.2.\n\nРедактор объединяет в одном окне работающее приложение, код и визуальные инструменты. Можно перетаскивать кнопки и блоки, менять их порядок, а затем просить агента сохранить изменения. Стили настраиваются через ползунки и палитры: шрифты, отступы, цвета, сетки. Несколько команд можно давать параллельно — \"сделай эту кнопку красной\", \"увеличь заголовок\", \"поменяй блоки местами\" — и агенты выполнят все за секунды.\n\nГлавное отличие от конкурентов: дизайнер работает не с макетом, а с настоящим кодом, который пойдет в продакшен. Все изменения сразу отражаются в проекте, а не в отдельном файле, который потом нужно \"переводить\" для разработчиков.\n\nГлава дизайна Cursor Рио Лу подчеркнул в интервью WIRED: профессиональные разработчики остаются ядром продукта, но они работают не в одиночку. Все, кто участвует в создании софта, должны находить в Cursor что-то полезное. При этом компания не считает конкурентами Replit или Lovable — те ориентированы на быстрые прототипы, тогда как Visual Editor создан для команд с большими проектами.\n\nP.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/d6f/616/28e/d6f61628e6db0cf2285f9610363be023.jpg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/d6f/616/28e/d6f61628e6db0cf2285f9610363be023.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Компания Cursor представила Visual Editor — инструмент, который позволяет дизайнерам редактировать интерфейсы веб-приложений без написания кода. Достаточно перетаскивать элементы, двигать слайдеры стилей или просто описывать правки словами — ИИ-агент сам внесет изменения в кодовую базу. Релиз вошел в обновление Cursor 2.2."
      },
      {
        "type": "text",
        "content": "Редактор объединяет в одном окне работающее приложение, код и визуальные инструменты. Можно перетаскивать кнопки и блоки, менять их порядок, а затем просить агента сохранить изменения. Стили настраиваются через ползунки и палитры: шрифты, отступы, цвета, сетки. Несколько команд можно давать параллельно — \"сделай эту кнопку красной\", \"увеличь заголовок\", \"поменяй блоки местами\" — и агенты выполнят все за секунды."
      },
      {
        "type": "text",
        "content": "Главное отличие от конкурентов: дизайнер работает не с макетом, а с настоящим кодом, который пойдет в продакшен. Все изменения сразу отражаются в проекте, а не в отдельном файле, который потом нужно \"переводить\" для разработчиков."
      },
      {
        "type": "text",
        "content": "Глава дизайна Cursor Рио Лу подчеркнул в интервью WIRED: профессиональные разработчики остаются ядром продукта, но они работают не в одиночку. Все, кто участвует в создании софта, должны находить в Cursor что-то полезное. При этом компания не считает конкурентами Replit или Lovable — те ориентированы на быстрые прототипы, тогда как Visual Editor создан для команд с большими проектами."
      },
      {
        "type": "text",
        "content": "P.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/976144/",
    "site_type": "habr",
    "title": "RSL 1.0 стал официальным стандартом лицензирования контента для ИИ‑эры",
    "date": "12.12.2025",
    "text": "Интернет‑сообщество объявило, что Really Simple Licensing 1.0 (RSL) получил статус официальной спецификации и теперь представляет собой открытый отраслевой стандарт для лицензирования цифрового контента, который используется ИИ‑системами. RSL создан группой RSL Collective совместно с ведущими издателями, платформами и технологическими организациями, чтобы дать авторам и владельцам контента возможность задавать понятные, машиночитаемые правила использования их материалов ИИ‑компаниями. Этот стандарт основан на идеях RSS и расширяет существующие правила для веб‑краулеров, позволяя вводить лицензии и условия, которые определяют, как именно ИИ может использовать контент.\n\nRSL 1.0 позволяет издателям прописывать условия, при которых их контент может быть сканирован, индексирован и использован ИИ‑системами. В отличие от традиционного файла robots.txt, который просто разрешает или запрещает доступ, новая спецификация вводит расширенные категории использования, такие как ai‑all, ai‑input и ai‑index, которые дают точечный контроль над разными сценариями ИИ‑использования. Платформа также включает механизмы для требования финансовой или иной компенсации за использование контента, а также опции, которые поддерживают некоммерческие проекты, чтобы укрепить общий пул знаний - «цифровой общий ресурс».\n\nПоддержка RSL 1.0 уже охватывает более 1500 медиа‑организаций, издателей и технологических компаний по всему миру. Среди них крупные имена, такие как Reddit, Yahoo, Ziff Davis, O’Reilly Media, The Guardian и другие, а также инфраструктурные провайдеры вроде Cloudflare и Akamai, которые могут технически поддерживать и применять правила RSL на уровне сетевого трафика. Это создаёт реальную базу для внедрения лицензий на интернет‑масштабе и усиливает позиции правообладателей, давая им возможность не только контролировать доступ ИИ‑краулеров, но и получать справедливую компенсацию за использование своих материалов в обучении и выдаче ответов.\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/b0f/fec/c5a/b0ffecc5aa328ae78fbc212ccc34e4c3.jpeg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/b0f/fec/c5a/b0ffecc5aa328ae78fbc212ccc34e4c3.jpeg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Интернет‑сообщество объявило, что Really Simple Licensing 1.0 (RSL) получил статус официальной спецификации и теперь представляет собой открытый отраслевой стандарт для лицензирования цифрового контента, который используется ИИ‑системами. RSL создан группой RSL Collective совместно с ведущими издателями, платформами и технологическими организациями, чтобы дать авторам и владельцам контента возможность задавать понятные, машиночитаемые правила использования их материалов ИИ‑компаниями. Этот стандарт основан на идеях RSS и расширяет существующие правила для веб‑краулеров, позволяя вводить лицензии и условия, которые определяют, как именно ИИ может использовать контент."
      },
      {
        "type": "text",
        "content": "RSL 1.0 позволяет издателям прописывать условия, при которых их контент может быть сканирован, индексирован и использован ИИ‑системами. В отличие от традиционного файла robots.txt, который просто разрешает или запрещает доступ, новая спецификация вводит расширенные категории использования, такие как ai‑all, ai‑input и ai‑index, которые дают точечный контроль над разными сценариями ИИ‑использования. Платформа также включает механизмы для требования финансовой или иной компенсации за использование контента, а также опции, которые поддерживают некоммерческие проекты, чтобы укрепить общий пул знаний - «цифровой общий ресурс»."
      },
      {
        "type": "text",
        "content": "Поддержка RSL 1.0 уже охватывает более 1500 медиа‑организаций, издателей и технологических компаний по всему миру. Среди них крупные имена, такие как Reddit, Yahoo, Ziff Davis, O’Reilly Media, The Guardian и другие, а также инфраструктурные провайдеры вроде Cloudflare и Akamai, которые могут технически поддерживать и применять правила RSL на уровне сетевого трафика. Это создаёт реальную базу для внедрения лицензий на интернет‑масштабе и усиливает позиции правообладателей, давая им возможность не только контролировать доступ ИИ‑краулеров, но и получать справедливую компенсацию за использование своих материалов в обучении и выдаче ответов."
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/976288/",
    "site_type": "habr",
    "title": "Читалки Kindle стали применять ИИ для ответов на вопросы о книгах",
    "date": "13.12.2025",
    "text": "Amazon незаметно добавила новую функцию искусственного интеллекта в приложение Kindle для iOS. Она «позволяет задавать вопросы о читаемой книге и получать ответы без спойлеров». Авторы не могут отказаться от этой опции.\n\nКомпания заявляет, что функция под названием «Задайте вопрос об этой книге» (Ask this Book) послужит «экспертным помощником в чтении, мгновенно отвечая на вопросы о деталях сюжета, взаимоотношениях персонажей и тематических элементах, не прерывая процесс чтения».\n\nПредставитель Amazon отметил : «Эта функция использует технологии, включая ИИ, для предоставления мгновенных ответов без спойлеров на вопросы клиентов о том, что они читают. Функция даёт короткие ответы, основанные на фактической информации о книге, которые доступны только читателям, купившим или взявшим книгу напрокат, и не подлежат распространению и копированию».\n\nПри этом в компании не стали уточнять, на какие права компания опиралась при внедрении новой функции, технические детали работы сервиса и какие меры защиты применяла, будь то предотвращение галлюцинаций или защита текста от обучения моделей ИИ.\n\nПри этом у авторов и издателей нет возможности отключить опцию для своих произведений. Они, вероятно, даже не были уведомлены о существовании этой функции. Многие из них могут посчитать такой ИИ-анализ своего произведения как прямое нарушение авторских прав.\n\nВ настоящее время Ask this Book работает только в приложении Kindle для iOS в США. Функция появится на устройствах Kindle и Android OS в следующем году.\n\nРанее компания заявила , что позволит авторам предлагать свои электронные книги без технических средств защиты авторских прав (DRM) в форматах EPUB и PDF через платформу для самостоятельной публикации KIndle Direct Publishing. С 20 января 2026 года авторы, которые укажут книги как не защищённые DRM, смогут видеть свои произведения доступными в этих более открытых форматах.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/b38/250/33d/b3825033d5eb866f9a4d1884074eec3d.JPG",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "text",
        "content": "Amazon незаметно добавила новую функцию искусственного интеллекта в приложение Kindle для iOS. Она «позволяет задавать вопросы о читаемой книге и получать ответы без спойлеров». Авторы не могут отказаться от этой опции."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/b38/250/33d/b3825033d5eb866f9a4d1884074eec3d.JPG",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Компания заявляет, что функция под названием «Задайте вопрос об этой книге» (Ask this Book) послужит «экспертным помощником в чтении, мгновенно отвечая на вопросы о деталях сюжета, взаимоотношениях персонажей и тематических элементах, не прерывая процесс чтения»."
      },
      {
        "type": "text",
        "content": "Представитель Amazon отметил : «Эта функция использует технологии, включая ИИ, для предоставления мгновенных ответов без спойлеров на вопросы клиентов о том, что они читают. Функция даёт короткие ответы, основанные на фактической информации о книге, которые доступны только читателям, купившим или взявшим книгу напрокат, и не подлежат распространению и копированию»."
      },
      {
        "type": "text",
        "content": "При этом в компании не стали уточнять, на какие права компания опиралась при внедрении новой функции, технические детали работы сервиса и какие меры защиты применяла, будь то предотвращение галлюцинаций или защита текста от обучения моделей ИИ."
      },
      {
        "type": "text",
        "content": "При этом у авторов и издателей нет возможности отключить опцию для своих произведений. Они, вероятно, даже не были уведомлены о существовании этой функции. Многие из них могут посчитать такой ИИ-анализ своего произведения как прямое нарушение авторских прав."
      },
      {
        "type": "text",
        "content": "В настоящее время Ask this Book работает только в приложении Kindle для iOS в США. Функция появится на устройствах Kindle и Android OS в следующем году."
      },
      {
        "type": "text",
        "content": "Ранее компания заявила , что позволит авторам предлагать свои электронные книги без технических средств защиты авторских прав (DRM) в форматах EPUB и PDF через платформу для самостоятельной публикации KIndle Direct Publishing. С 20 января 2026 года авторы, которые укажут книги как не защищённые DRM, смогут видеть свои произведения доступными в этих более открытых форматах."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/976306/",
    "site_type": "habr",
    "title": "NVIDIA предложила ИИ-концепцию для инженерного моделирования",
    "date": "13.12.2025",
    "text": "NVIDIA Research предлагает смену парадигмы Computer-Aided Engineering (CAE) на AI-Aided Engineering. Вместо прямых вычислений предлагается использовать ИИ-модели, обученные на физических законах и данных симуляций. Такие модели работают как быстрая замена классическим расчётам. Например, прогноз погоды можно сделать за минуты вместо дней.\n\nНа протяжении десятилетий прогресс в инженерии, от аэрокосмической отрасли до материаловедения, обеспечивался с помощью компьютерного моделирования (CAE). Традиционные численные методы являются основополагающими, но их высокая вычислительная стоимость часто требует дней или недель для одной высокоточной симуляции, создавая существенное узкое место, ограничивающее темпы инноваций. Ускорение на графических процессорах (GPU) обеспечило первую волну значительного ускорения, и теперь искусственный интеллект имеет потенциал для дальнейшего ускорения научных открытий.\n\nДля этого NVIDIA продвигает новую парадигму: проектирование с использованием искусственного интеллекта (AI-Aided Engineering, AIE) - видение того, как исследования в области ИИ ускорят и преобразуют область промышленного инжиниринга, и будут опираться на существующие работы NVIDIA в области ИИ и CAE.\n\nОсновной принцип AIE заключается в разработке и обучении новых базовых моделей на основе наблюдательных или имитационных данных для изучения основных физических законов системы. Эти модели функционируют как масштабируемые и эффективные заменители, способные аппроксимировать сложные модели на порядки быстрее, чем традиционные методы - например, FourCastNet 3 предоставляет 60-дневные прогнозы погоды менее чем за 4 минуты, что в 60 раз быстрее, чем традиционные методы.\n\nОсновная исследовательская задача - разработка алгоритмов следующего поколения, которые будут обеспечивать работу этой экосистемы. Центральным элементом работы NVIDIA является отказ от дискретных сеток и сосредоточение внимания на моделях, которые могут работать непосредственно с исходными CAD-моделями, используемыми инженерами. Для достижения этой цели компания разрабатывает фундаментальные алгоритмы, такие как новые архитектуры для нейронных операторов, которые спроектированы не только для высокой скорости, но и для поддержания высокой степени физической точности, а также эффективного кодирования геометрии, граничных и начальных условий.\n\nРеволюция в области искусственного интеллекта была построена на программном обеспечении с открытым исходным кодом и научных достижениях, и один из ключевых принципов работы NVIDIA заключается в том, что оно должно способствовать этому прогрессу и быть полностью открытым. Делая эти передовые инструменты доступными для всех, компания стремится демократизировать передовое моделирование, расширить возможности научного сообщества и ускорить развитие следующего поколения инженерных разработок с использованием ИИ. Это создает замкнутый цикл: работа NVIDEA основана на реальных потребностях промышленности, а подтвержденные достижения возвращаются во всю экосистему.\n\nNVIDIA считают, что решение этой грандиозной задачи требует комплексного подхода, от кремниевых компонентов до программного обеспечения и систем. Работа основана на всей инфраструктуре ускоренных вычислений компании:\n\nУскоренное оборудование;\n\nОсновное программное обеспечение;\n\nПередовые исследования;\n\nСпециализированные модели ИИ для конкретных областей;\n\nФреймворки для физического ИИ, обеспечивающие качество продукции;\n\nНаборы данных;\n\nВизуализация в реальном времени.\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/912/d65/f1c/912d65f1c03a6584bf582b29303b6925.png",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/912/d65/f1c/912d65f1c03a6584bf582b29303b6925.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "NVIDIA Research предлагает смену парадигмы Computer-Aided Engineering (CAE) на AI-Aided Engineering. Вместо прямых вычислений предлагается использовать ИИ-модели, обученные на физических законах и данных симуляций. Такие модели работают как быстрая замена классическим расчётам. Например, прогноз погоды можно сделать за минуты вместо дней."
      },
      {
        "type": "text",
        "content": "На протяжении десятилетий прогресс в инженерии, от аэрокосмической отрасли до материаловедения, обеспечивался с помощью компьютерного моделирования (CAE). Традиционные численные методы являются основополагающими, но их высокая вычислительная стоимость часто требует дней или недель для одной высокоточной симуляции, создавая существенное узкое место, ограничивающее темпы инноваций. Ускорение на графических процессорах (GPU) обеспечило первую волну значительного ускорения, и теперь искусственный интеллект имеет потенциал для дальнейшего ускорения научных открытий."
      },
      {
        "type": "text",
        "content": "Для этого NVIDIA продвигает новую парадигму: проектирование с использованием искусственного интеллекта (AI-Aided Engineering, AIE) - видение того, как исследования в области ИИ ускорят и преобразуют область промышленного инжиниринга, и будут опираться на существующие работы NVIDIA в области ИИ и CAE."
      },
      {
        "type": "text",
        "content": "Основной принцип AIE заключается в разработке и обучении новых базовых моделей на основе наблюдательных или имитационных данных для изучения основных физических законов системы. Эти модели функционируют как масштабируемые и эффективные заменители, способные аппроксимировать сложные модели на порядки быстрее, чем традиционные методы - например, FourCastNet 3 предоставляет 60-дневные прогнозы погоды менее чем за 4 минуты, что в 60 раз быстрее, чем традиционные методы."
      },
      {
        "type": "text",
        "content": "Основная исследовательская задача - разработка алгоритмов следующего поколения, которые будут обеспечивать работу этой экосистемы. Центральным элементом работы NVIDIA является отказ от дискретных сеток и сосредоточение внимания на моделях, которые могут работать непосредственно с исходными CAD-моделями, используемыми инженерами. Для достижения этой цели компания разрабатывает фундаментальные алгоритмы, такие как новые архитектуры для нейронных операторов, которые спроектированы не только для высокой скорости, но и для поддержания высокой степени физической точности, а также эффективного кодирования геометрии, граничных и начальных условий."
      },
      {
        "type": "text",
        "content": "Революция в области искусственного интеллекта была построена на программном обеспечении с открытым исходным кодом и научных достижениях, и один из ключевых принципов работы NVIDIA заключается в том, что оно должно способствовать этому прогрессу и быть полностью открытым. Делая эти передовые инструменты доступными для всех, компания стремится демократизировать передовое моделирование, расширить возможности научного сообщества и ускорить развитие следующего поколения инженерных разработок с использованием ИИ. Это создает замкнутый цикл: работа NVIDEA основана на реальных потребностях промышленности, а подтвержденные достижения возвращаются во всю экосистему."
      },
      {
        "type": "text",
        "content": "NVIDIA считают, что решение этой грандиозной задачи требует комплексного подхода, от кремниевых компонентов до программного обеспечения и систем. Работа основана на всей инфраструктуре ускоренных вычислений компании:"
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/976314/",
    "site_type": "habr",
    "title": "Google выпустила новый голосовой ИИ на Gemini: в тестах он помог оформить 14 000 ипотечных кредитов",
    "date": "13.12.2025",
    "text": "Google выпустила Gemini 2.5 Flash Native Audio — обновленную модель для голосовых агентов, которая обходит OpenAI gpt-realtime в бенчмарке сложных функциональных вызовов. Модель уже работает в продуктах компании и доступна разработчикам.\n\nНа тесте ComplexFuncBench Audio, который измеряет способность модели выполнять многошаговые вызовы функций с ограничениями, Gemini 2.5 Flash Native Audio набрала 71,5% — против 66,5% у gpt-realtime от OpenAI. Google также заявляет о 90% точности следования инструкциям разработчика (было 84%) и улучшенном удержании контекста в многоходовых диалогах. По словам компании, модель лучше понимает, когда нужно вызвать внешнюю функцию, и встраивает результаты обратно в разговор без потери естественности.\n\nМодель уже развернута в голосовом режиме приложения Gemini и Search Live — поиск Google впервые получил нативное аудио вместо каскадной архитектуры «распознавание → LLM → синтез». Для разработчиков Gemini 2.5 Flash Native Audio доступна в Google AI Studio и Vertex AI (GA), а также в Gemini API в режиме preview.\n\nСреди первых клиентов — United Wholesale Mortgage, крупнейший ипотечный брокер США. По словам технического директора компании Джейсона Бресслера, голосовой ассистент Mia на базе Gemini помог оформить более 14 000 кредитов для брокеров-партнеров с момента запуска в мае 2025 года. Shopify отмечает, что пользователи их голосового помощника Sidekick \"забывают, что говорят с ИИ, в течение минуты\".\n\nВместе с обновлением голосовой модели Google запустила бета-версию синхронного перевода речи в приложении Google Translate. Функция работает с любыми наушниками и поддерживает более 70 языков в 2000 языковых парах. Перевод сохраняет интонацию, темп и тембр говорящего. Доступно два режима: непрерывное прослушивание (для лекций, фильмов, разговоров вокруг) и двусторонний разговор, где система автоматически переключает направление перевода в зависимости от того, кто говорит. Пока бета доступна на Android в США, Мексике и Индии; iOS и другие регионы — в 2026 году.\n\nP.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/19b/667/b82/19b667b828d20a9dda2b09a4d40c74fc.jpg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/19b/667/b82/19b667b828d20a9dda2b09a4d40c74fc.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Google выпустила Gemini 2.5 Flash Native Audio — обновленную модель для голосовых агентов, которая обходит OpenAI gpt-realtime в бенчмарке сложных функциональных вызовов. Модель уже работает в продуктах компании и доступна разработчикам."
      },
      {
        "type": "text",
        "content": "На тесте ComplexFuncBench Audio, который измеряет способность модели выполнять многошаговые вызовы функций с ограничениями, Gemini 2.5 Flash Native Audio набрала 71,5% — против 66,5% у gpt-realtime от OpenAI. Google также заявляет о 90% точности следования инструкциям разработчика (было 84%) и улучшенном удержании контекста в многоходовых диалогах. По словам компании, модель лучше понимает, когда нужно вызвать внешнюю функцию, и встраивает результаты обратно в разговор без потери естественности."
      },
      {
        "type": "text",
        "content": "Модель уже развернута в голосовом режиме приложения Gemini и Search Live — поиск Google впервые получил нативное аудио вместо каскадной архитектуры «распознавание → LLM → синтез». Для разработчиков Gemini 2.5 Flash Native Audio доступна в Google AI Studio и Vertex AI (GA), а также в Gemini API в режиме preview."
      },
      {
        "type": "text",
        "content": "Среди первых клиентов — United Wholesale Mortgage, крупнейший ипотечный брокер США. По словам технического директора компании Джейсона Бресслера, голосовой ассистент Mia на базе Gemini помог оформить более 14 000 кредитов для брокеров-партнеров с момента запуска в мае 2025 года. Shopify отмечает, что пользователи их голосового помощника Sidekick \"забывают, что говорят с ИИ, в течение минуты\"."
      },
      {
        "type": "text",
        "content": "Вместе с обновлением голосовой модели Google запустила бета-версию синхронного перевода речи в приложении Google Translate. Функция работает с любыми наушниками и поддерживает более 70 языков в 2000 языковых парах. Перевод сохраняет интонацию, темп и тембр говорящего. Доступно два режима: непрерывное прослушивание (для лекций, фильмов, разговоров вокруг) и двусторонний разговор, где система автоматически переключает направление перевода в зависимости от того, кто говорит. Пока бета доступна на Android в США, Мексике и Индии; iOS и другие регионы — в 2026 году."
      },
      {
        "type": "text",
        "content": "P.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/976358/",
    "site_type": "habr",
    "title": "OpenAI незаметно внедряет модульную структуру навыков Anthropic для повышения возможностей агентов",
    "date": "13.12.2025",
    "text": "По всей видимости, OpenAI внедряет систему навыков, представленную Anthropic в октябре , как показало открытие пользователя Elias Judin, опубликованное на X. Поддержка этих навыков появилась как в инструменте командной строки Codex, так и в ChatGPT.\n\nДжудин обнаружил папки с названиями \"pdfs\" и \"spreadsheets\", содержащие файлы \" skill.md \". Эти файлы содержат конкретные инструкции по обработке документов и данных. По сути, это как если бы ваша подсказка вызывала более конкретную подсказку для решения сложной подзадачи, необходимой для достижения основной цели - например, извлечения текста из PDF-файла. Поскольку это всего лишь папка, содержащая файл Markdown и, возможно, скрипты, ее легко адаптировать.\n\nСтруктура файлов указывает на то, что OpenAI организует инструменты ИИ в модули, похожие на приложения, предназначенные для решения конкретных задач. Джудин, обнаруживший эту функцию при использовании модели \"5.2 pro\", задокументировал свои находки на GitHub . Компания Anthropic представила эту модульную систему в октябре, чтобы помочь своему ассистенту Claude справляться со специализированными задачами.\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/999/16f/200/99916f200a9a2d5e4e0c61938540e8b2.png",
        "alt": ""
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/474/ad1/b98/474ad1b984dac6eb01d22b5448af4f22.png",
        "alt": "Изучение файла \"skill.md\", содержащего инструкции по обработке PDF-файлов, показывает конкретные указания по чтению и созданию документов  "
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/999/16f/200/99916f200a9a2d5e4e0c61938540e8b2.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "По всей видимости, OpenAI внедряет систему навыков, представленную Anthropic в октябре , как показало открытие пользователя Elias Judin, опубликованное на X. Поддержка этих навыков появилась как в инструменте командной строки Codex, так и в ChatGPT."
      },
      {
        "type": "text",
        "content": "Джудин обнаружил папки с названиями \"pdfs\" и \"spreadsheets\", содержащие файлы \" skill.md \". Эти файлы содержат конкретные инструкции по обработке документов и данных. По сути, это как если бы ваша подсказка вызывала более конкретную подсказку для решения сложной подзадачи, необходимой для достижения основной цели - например, извлечения текста из PDF-файла. Поскольку это всего лишь папка, содержащая файл Markdown и, возможно, скрипты, ее легко адаптировать."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/474/ad1/b98/474ad1b984dac6eb01d22b5448af4f22.png",
        "alt": "Изучение файла \"skill.md\", содержащего инструкции по обработке PDF-файлов, показывает конкретные указания по чтению и созданию документов  "
      },
      {
        "type": "text",
        "content": "Структура файлов указывает на то, что OpenAI организует инструменты ИИ в модули, похожие на приложения, предназначенные для решения конкретных задач. Джудин, обнаруживший эту функцию при использовании модели \"5.2 pro\", задокументировал свои находки на GitHub . Компания Anthropic представила эту модульную систему в октябре, чтобы помочь своему ассистенту Claude справляться со специализированными задачами."
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/976392/",
    "site_type": "habr",
    "title": "Zoom AI ворвалась в гонку ИИ-моделей",
    "date": "13.12.2025",
    "text": "Технический директор Zoom сообщил о важном достижении в области искусственного интеллекта. Он объявил о том, что Zoom достиг нового передового результата (SOTA) в сложном тесте Humanity's Last Exam (HLE) с полным набором тестов, набрав 48,1%, что представляет собой существенное улучшение на 2,3% по сравнению с предыдущим передовым результатом в 45,8%, достигнутым Google Gemini 3 Pro с интеграцией инструментов.\n\nЭтот прорыв - не просто цифра, он воплощает эволюцию от AI Companion 1.0 к грядущему AI Companion 3.0, демонстрируя, как продуманное сотрудничество с лидерами отрасли может стимулировать инновации, приносящие пользу всем.\n\nТест Humanity's Last Exam (HLE) представляет собой одно из самых строгих испытаний в области искусственного интеллекта, предназначенное для оценки моделей в различных областях, требующих экспертных знаний и сложного логического мышления. В отличие от более простых тестов, которые могут основываться на сопоставлении образов, HLE требует подлинного понимания, многоэтапного логического мышления и способности синтезировать информацию в рамках сложных, взаимосвязанных задач.\n\nЭтот критерий был разработан экспертами со всего мира и стал важнейшим показателем прогресса ИИ в достижении уровня производительности человека при решении сложных интеллектуальных задач. Zoom AI имеет результат в 48,1%, ставя федеративный подход ИИ в авангард этой конкурентной среды.\n\nУспех Zoom AI основан на фундаменте, заложенном исследовательским сообществом в области искусственного интеллекта. Компания глубоко восхищаемся новаторской работой OpenAI, чьи модели GPT переосмыслили возможности понимания и генерации естественного языка. Google Gemini 3 Pro расширил границы мультимодального ИИ, а Anthropic Claude Opus 4.5 углубил понимание возможностей агентного управления.\n\nВместо того чтобы рассматривать эти достижения как конкуренцию, Zoom видит в них возможности для сотрудничества и взаимного совершенствования. Будущее ИИ заключается не в изоляции, а в интеллектуальной координации.\n\nУже на ранних этапах разработки AI Companion 1.0 компания поняла, что ни одна модель, какой бы продвинутой она ни была, не может преуспеть во всех задачах. Понимание этого привело Zoom к разработке федеративного подхода к искусственному интеллекту - сложной системы, которая использует уникальные преимущества множества моделей, одновременно внедряя новые архитектурные инновации.\n\nФедеративный подход сочетает в себе собственные небольшие языковые модели Zoom с передовыми моделями с открытым и закрытым исходным кодом, используя запатентованную систему «Z-scorer» для выбора или уточнения результатов с целью достижения оптимальной производительности. Такой подход позволяет компании сосредоточиться на:\n\nСпециализированной производительности;\n\nСкорости и масштабируемости;\n\nЭкономической эффективности.\n\nВыдающиеся результаты в последнем экзамене человечества обусловлены как мощными моделями, так и новым подходом к их применению. Ключевым фактором успеха является эффективно управляемая стратегия «исследование–проверка–объединение», инновационный агентный рабочий процесс, оптимально сочетающий исследовательское мышление с тщательной проверкой. Вместо генерации обширных траекторий рассуждений, метод стратегически определяет и использует наиболее информативные и повышающие точность пути рассуждений.\n\nОсновой подхода является федеративная многоуровневая LLM-структура, которая объединяет различные модели для генерации, проверки и уточнения рассуждений посредством диалектического взаимодействия. Эта структура позволяет каждой модели вносить свой уникальный вклад, а всесторонняя фаза верификации интегрирует весь контекст для определения наиболее точного решения.\n\nТакое сочетание целенаправленного исследования, проверки на основе контекста и федеративной оркестровки позволяет будущим системам искусственного интеллекта Zoom достигать гораздо более глубокого понимания, более высокой точности и более надежной производительности в решении самых сложных задач в области ИИ, эффективно обеспечивая новый лучший результат Zoom в бенчмарке HLE.\n\nХотите быть в курсе важных новостей из мира ИИ? Подписывайтесь на наш Telegram‑канал BotHub AI News .\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/513/bfd/3a8/513bfd3a82006eff13bb8a88838c1181.png",
        "alt": ""
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/9eb/9a5/e74/9eb9a5e747448e7b243c1e3c1e1aa74b.png",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/513/bfd/3a8/513bfd3a82006eff13bb8a88838c1181.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Технический директор Zoom сообщил о важном достижении в области искусственного интеллекта. Он объявил о том, что Zoom достиг нового передового результата (SOTA) в сложном тесте Humanity's Last Exam (HLE) с полным набором тестов, набрав 48,1%, что представляет собой существенное улучшение на 2,3% по сравнению с предыдущим передовым результатом в 45,8%, достигнутым Google Gemini 3 Pro с интеграцией инструментов."
      },
      {
        "type": "text",
        "content": "Этот прорыв - не просто цифра, он воплощает эволюцию от AI Companion 1.0 к грядущему AI Companion 3.0, демонстрируя, как продуманное сотрудничество с лидерами отрасли может стимулировать инновации, приносящие пользу всем."
      },
      {
        "type": "text",
        "content": "Тест Humanity's Last Exam (HLE) представляет собой одно из самых строгих испытаний в области искусственного интеллекта, предназначенное для оценки моделей в различных областях, требующих экспертных знаний и сложного логического мышления. В отличие от более простых тестов, которые могут основываться на сопоставлении образов, HLE требует подлинного понимания, многоэтапного логического мышления и способности синтезировать информацию в рамках сложных, взаимосвязанных задач."
      },
      {
        "type": "text",
        "content": "Этот критерий был разработан экспертами со всего мира и стал важнейшим показателем прогресса ИИ в достижении уровня производительности человека при решении сложных интеллектуальных задач. Zoom AI имеет результат в 48,1%, ставя федеративный подход ИИ в авангард этой конкурентной среды."
      },
      {
        "type": "text",
        "content": "Успех Zoom AI основан на фундаменте, заложенном исследовательским сообществом в области искусственного интеллекта. Компания глубоко восхищаемся новаторской работой OpenAI, чьи модели GPT переосмыслили возможности понимания и генерации естественного языка. Google Gemini 3 Pro расширил границы мультимодального ИИ, а Anthropic Claude Opus 4.5 углубил понимание возможностей агентного управления."
      },
      {
        "type": "text",
        "content": "Вместо того чтобы рассматривать эти достижения как конкуренцию, Zoom видит в них возможности для сотрудничества и взаимного совершенствования. Будущее ИИ заключается не в изоляции, а в интеллектуальной координации."
      },
      {
        "type": "text",
        "content": "Уже на ранних этапах разработки AI Companion 1.0 компания поняла, что ни одна модель, какой бы продвинутой она ни была, не может преуспеть во всех задачах. Понимание этого привело Zoom к разработке федеративного подхода к искусственному интеллекту - сложной системы, которая использует уникальные преимущества множества моделей, одновременно внедряя новые архитектурные инновации."
      },
      {
        "type": "text",
        "content": "Федеративный подход сочетает в себе собственные небольшие языковые модели Zoom с передовыми моделями с открытым и закрытым исходным кодом, используя запатентованную систему «Z-scorer» для выбора или уточнения результатов с целью достижения оптимальной производительности. Такой подход позволяет компании сосредоточиться на:"
      },
      {
        "type": "text",
        "content": "Выдающиеся результаты в последнем экзамене человечества обусловлены как мощными моделями, так и новым подходом к их применению. Ключевым фактором успеха является эффективно управляемая стратегия «исследование–проверка–объединение», инновационный агентный рабочий процесс, оптимально сочетающий исследовательское мышление с тщательной проверкой. Вместо генерации обширных траекторий рассуждений, метод стратегически определяет и использует наиболее информативные и повышающие точность пути рассуждений."
      },
      {
        "type": "text",
        "content": "Основой подхода является федеративная многоуровневая LLM-структура, которая объединяет различные модели для генерации, проверки и уточнения рассуждений посредством диалектического взаимодействия. Эта структура позволяет каждой модели вносить свой уникальный вклад, а всесторонняя фаза верификации интегрирует весь контекст для определения наиболее точного решения."
      },
      {
        "type": "text",
        "content": "Такое сочетание целенаправленного исследования, проверки на основе контекста и федеративной оркестровки позволяет будущим системам искусственного интеллекта Zoom достигать гораздо более глубокого понимания, более высокой точности и более надежной производительности в решении самых сложных задач в области ИИ, эффективно обеспечивая новый лучший результат Zoom в бенчмарке HLE."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/9eb/9a5/e74/9eb9a5e747448e7b243c1e3c1e1aa74b.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Хотите быть в курсе важных новостей из мира ИИ? Подписывайтесь на наш Telegram‑канал BotHub AI News ."
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/news/976402/",
    "site_type": "habr",
    "title": "«ИИ написал 85% кода»: как четыре инженера OpenAI собрали Android-версию Sora 2 за 28 дней",
    "date": "13.12.2025",
    "text": "OpenAI в своем блоге рассказала о создании Android-версии Sora 2: команда из четырех инженеров справилась за 28 дней, а около 85% кода, по их оценке, написал Codex (порядка 45 тысяч долларов, если считать в ценах на API). Для мобильного приложения с высоким трафиком и жесткими требованиями к стабильности это выглядит как отличный результат.\n\nСтавка изначально была на маленькую команду и жесткую дисциплину, а не на наращивание численности. В OpenAI прямо ссылаются на старое правило Брукса: добавление разработчиков в сжатые сроки чаще замедляет проект. Codex рассматривали не как \"волшебную кнопку\", а как усилитель — инструмент, который берет на себя максимум рутинной работы, если ему задать правильное направление.\n\nКлючевым принципом стало правило \"сначала план, потом код\". Перед каждой заметной задачей инженеры формулировали короткий проектный документ: что делаем, какими шагами, где критерии готовности. Этот план либо прямо передавался Codex, либо сохранялся в файле, чтобы модель не теряла контекст при длинной работе. В таком режиме Codex уверенно писал большие куски логики, дополнял их тестами и чинил ошибки по журналам CI.\n\nЕще один важный прием — параллельные сессии. Вместо одной длинной цепочки запросов команда запускала несколько независимых потоков: один отвечал за новую функциональность, другой — за исправление багов, третий — за тесты или интеграцию. Это резко ускоряло работу, но одновременно переносило нагрузку на людей: все результаты сходились в ревью, где нужно было принимать решения и удерживать целостность проекта.\n\nСамым сильным местом Codex оказалась работа с существующим кодом. Он быстро читал кодовую базу, уверенно переносил логику с iOS-версии на Kotlin, исправлял проблемы сборки и аккуратно дописывал недостающие части. По ощущениям команды, это был \"еще один очень старательный инженер\", который не устает и не забывает контекст внутри задачи.\n\nФинальный вывод OpenAI звучит неожиданно трезво. Codex действительно снял ограничение по скорости написания кода, но узким местом стали архитектура, ревью и ответственность за решения. Поэтому выигрыш получают не те, кто \"пишет меньше руками\", а те, кто умеет четко формулировать задачи, держать структуру проекта и вовремя останавливать модель. В этом смысле кейс Sora для Android — не про замену инженеров, а про то, как их роль окончательно смещается от написания кода к управлению сложной системой.\n\nP.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны.",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/d9b/9c2/520/d9b9c252057dda79d0b6ead4775de4bd.jpg",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/d9b/9c2/520/d9b9c252057dda79d0b6ead4775de4bd.jpg",
        "alt": ""
      },
      {
        "type": "text",
        "content": "OpenAI в своем блоге рассказала о создании Android-версии Sora 2: команда из четырех инженеров справилась за 28 дней, а около 85% кода, по их оценке, написал Codex (порядка 45 тысяч долларов, если считать в ценах на API). Для мобильного приложения с высоким трафиком и жесткими требованиями к стабильности это выглядит как отличный результат."
      },
      {
        "type": "text",
        "content": "Ставка изначально была на маленькую команду и жесткую дисциплину, а не на наращивание численности. В OpenAI прямо ссылаются на старое правило Брукса: добавление разработчиков в сжатые сроки чаще замедляет проект. Codex рассматривали не как \"волшебную кнопку\", а как усилитель — инструмент, который берет на себя максимум рутинной работы, если ему задать правильное направление."
      },
      {
        "type": "text",
        "content": "Ключевым принципом стало правило \"сначала план, потом код\". Перед каждой заметной задачей инженеры формулировали короткий проектный документ: что делаем, какими шагами, где критерии готовности. Этот план либо прямо передавался Codex, либо сохранялся в файле, чтобы модель не теряла контекст при длинной работе. В таком режиме Codex уверенно писал большие куски логики, дополнял их тестами и чинил ошибки по журналам CI."
      },
      {
        "type": "text",
        "content": "Еще один важный прием — параллельные сессии. Вместо одной длинной цепочки запросов команда запускала несколько независимых потоков: один отвечал за новую функциональность, другой — за исправление багов, третий — за тесты или интеграцию. Это резко ускоряло работу, но одновременно переносило нагрузку на людей: все результаты сходились в ревью, где нужно было принимать решения и удерживать целостность проекта."
      },
      {
        "type": "text",
        "content": "Самым сильным местом Codex оказалась работа с существующим кодом. Он быстро читал кодовую базу, уверенно переносил логику с iOS-версии на Kotlin, исправлял проблемы сборки и аккуратно дописывал недостающие части. По ощущениям команды, это был \"еще один очень старательный инженер\", который не устает и не забывает контекст внутри задачи."
      },
      {
        "type": "text",
        "content": "Финальный вывод OpenAI звучит неожиданно трезво. Codex действительно снял ограничение по скорости написания кода, но узким местом стали архитектура, ревью и ответственность за решения. Поэтому выигрыш получают не те, кто \"пишет меньше руками\", а те, кто умеет четко формулировать задачи, держать структуру проекта и вовремя останавливать модель. В этом смысле кейс Sora для Android — не про замену инженеров, а про то, как их роль окончательно смещается от написания кода к управлению сложной системой."
      },
      {
        "type": "text",
        "content": "P.S. Поддержать меня можно подпиской на канал \" сбежавшая нейросеть \", где я рассказываю про ИИ с творческой стороны."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/976472/",
    "site_type": "habr",
    "title": "Allen Institute for AI обновил линейку моделей Olmo до версии 3.1",
    "date": "14.12.2025",
    "text": "ИИ-стартап Ai2 объявил о выпуске OLMo 3.1 32B Think - новой флагманской модели в рамках семейства OLMo 3, ориентированной на задачи, требующие сложного пошагового рассуждения. Модель распространяется под лицензией Apache 2.0, что делает ее доступной для исследовательского и образовательного использования с полным раскрытием кода, контрольных точек и данных обучения.\n\nOLMo 3.1 32B Think является трансформер-моделью с 32 миллиардами параметров и обучена исключительно на англоязычном датасете Dolma 3, после чего модель прошла многоэтапное пост-обучение на специализированных наборах Dolci, направленных на развитие математического, алгоритмического и программного мышления. В Ai2 подчеркивают, что ключевая цель серии OLMo - не только достичь высокого качества, но обеспечить максимальную открытость: архитектура, данные и процесс обучения полностью задокументированы и доступны в открытом доступе.\n\nФинальная версия OLMo 3.1 32B Think формировалась в три этапа. Сначала модель прошла SFT-обучение, включающем задачи по математике, программированию, общему чату и работе с инструкциями. Затем применялась техника DPO, где модель обучалась выбирать более качественные способы выстраивания рассуждения. Завершающим этапом стало обучение с подкреплением на основе проверяемых вознаграждений (RLVR), ориентированного на строго проверяемые математические и кодовые ответы. Именно этот этап, по словам разработчиков, дал основной прирост производительности в задачах формального рассуждения.\n\nВ математическом бенчмарке MATH модель набрала 96,2%, превзойдя OLMo 2 32B Instruct с результатом 49,2% и Qwen 2.5 32B с 80,2%. В задачах AIME 2024 результат составил 80,6%, а на AIME 2025 - 78,1%, что значительно выше показателей большинства открытых альтернатив. В более сложном наборе OMEGA модель достигла 53,4%, тогда как OLMo 2 32B показал менее 10 %. В задачах логического мышления OLMo 3.1 32B Think набрала 88,6% на BigBenchHard, 80,1% на ZebraLogic и 89,2% на AGI Eval English. В программировании модель показала 91,5% на HumanEvalPlus и 83,3% на LiveCodeBench v3, что выводит ее в один ряд с лучшими открытыми моделями для агентного кодинга. В тестах на следование инструкциям результаты также выросли: 93,8% на IFEval и 68,1% на IFBench, что заметно выше показателей версий без RLVR-этапа.\n\nТакже модель набрала 86,4% на MMLU и 57,5% на GPQA, уступая некоторым закрытым и мультимодальным моделям, но при этом сохраняя конкурентоспособность в открытом сегменте. В диалоговых оценках AlpacaEval 2 LC результат составил 69,1%, а по метрикам безопасности модель получила 83,6%, что указывает на более устойчивое поведение по сравнению с предыдущими поколениями OLMo Think. OLMo 3.1 32B Think доступна в формате BF16 и может запускаться как через библиотеку Transformers, так и через vLLM или другие совместимые движки инференса.\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/b78/e7f/43c/b78e7f43cabbb11d942f0b0ef82c4e7e.png",
        "alt": ""
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/9a8/04b/e9a/9a804be9ae5e96ba657daf6b9b54f543.png",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/b78/e7f/43c/b78e7f43cabbb11d942f0b0ef82c4e7e.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "ИИ-стартап Ai2 объявил о выпуске OLMo 3.1 32B Think - новой флагманской модели в рамках семейства OLMo 3, ориентированной на задачи, требующие сложного пошагового рассуждения. Модель распространяется под лицензией Apache 2.0, что делает ее доступной для исследовательского и образовательного использования с полным раскрытием кода, контрольных точек и данных обучения."
      },
      {
        "type": "text",
        "content": "OLMo 3.1 32B Think является трансформер-моделью с 32 миллиардами параметров и обучена исключительно на англоязычном датасете Dolma 3, после чего модель прошла многоэтапное пост-обучение на специализированных наборах Dolci, направленных на развитие математического, алгоритмического и программного мышления. В Ai2 подчеркивают, что ключевая цель серии OLMo - не только достичь высокого качества, но обеспечить максимальную открытость: архитектура, данные и процесс обучения полностью задокументированы и доступны в открытом доступе."
      },
      {
        "type": "text",
        "content": "Финальная версия OLMo 3.1 32B Think формировалась в три этапа. Сначала модель прошла SFT-обучение, включающем задачи по математике, программированию, общему чату и работе с инструкциями. Затем применялась техника DPO, где модель обучалась выбирать более качественные способы выстраивания рассуждения. Завершающим этапом стало обучение с подкреплением на основе проверяемых вознаграждений (RLVR), ориентированного на строго проверяемые математические и кодовые ответы. Именно этот этап, по словам разработчиков, дал основной прирост производительности в задачах формального рассуждения."
      },
      {
        "type": "text",
        "content": "В математическом бенчмарке MATH модель набрала 96,2%, превзойдя OLMo 2 32B Instruct с результатом 49,2% и Qwen 2.5 32B с 80,2%. В задачах AIME 2024 результат составил 80,6%, а на AIME 2025 - 78,1%, что значительно выше показателей большинства открытых альтернатив. В более сложном наборе OMEGA модель достигла 53,4%, тогда как OLMo 2 32B показал менее 10 %. В задачах логического мышления OLMo 3.1 32B Think набрала 88,6% на BigBenchHard, 80,1% на ZebraLogic и 89,2% на AGI Eval English. В программировании модель показала 91,5% на HumanEvalPlus и 83,3% на LiveCodeBench v3, что выводит ее в один ряд с лучшими открытыми моделями для агентного кодинга. В тестах на следование инструкциям результаты также выросли: 93,8% на IFEval и 68,1% на IFBench, что заметно выше показателей версий без RLVR-этапа."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/9a8/04b/e9a/9a804be9ae5e96ba657daf6b9b54f543.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Также модель набрала 86,4% на MMLU и 57,5% на GPQA, уступая некоторым закрытым и мультимодальным моделям, но при этом сохраняя конкурентоспособность в открытом сегменте. В диалоговых оценках AlpacaEval 2 LC результат составил 69,1%, а по метрикам безопасности модель получила 83,6%, что указывает на более устойчивое поведение по сравнению с предыдущими поколениями OLMo Think. OLMo 3.1 32B Think доступна в формате BF16 и может запускаться как через библиотеку Transformers, так и через vLLM или другие совместимые движки инференса."
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://habr.com/ru/companies/bothub/news/976494/",
    "site_type": "habr",
    "title": "LongCat-Image доказывает, что 6 миллиардов параметров могут превзойти более крупные модели с лучшей обработкой данных",
    "date": "14.12.2025",
    "text": "Китайская технологическая компания Meituan выпустила LongCat-Image, новую модель обработки изображений с открытым исходным кодом, которая бросает вызов распространенному в отрасли принципу «чем больше, тем лучше». Сообщается, что модель, имеющая всего 6 миллиардов параметров, значительно превосходит более крупных конкурентов как по фотореализму, так и по рендерингу текста, благодаря строгой обработке данных и продуманному подходу к обработке текста.\n\nВ то время как конкуренты, такие как Tencent и Alibaba, продолжают создавать всё более крупные модели - Hunyuan3.0 вмещает до 80 миллиардов параметров - Meituan пошла в противоположном направлении. Команда утверждает, что масштабирование методом грубой силы тратит ресурсы оборудования, не улучшая при этом качество изображений. Вместо этого LongCat-Image использует архитектуру, аналогичную популярной Flux.1-dev, построенную на гибридном многомодальном диффузионном трансформаторе (MM-DiT).\n\nСистема обрабатывает данные изображения и текста через два отдельных пути внимания на начальных уровнях, прежде чем объединить их позже. Это обеспечивает текстовой подсказке более жесткий контроль над генерацией изображения без увеличения вычислительной нагрузки.\n\nПо мнению исследователей, одной из самых больших проблем современных систем искусственного интеллекта для обработки изображений является загрязнение обучающих данных. Когда модели обучаются на изображениях, созданных другими системами ИИ, они перенимают пластиковую или жирную текстуру. Модель учится использовать упрощенные подходы вместо понимания реальной сложности.\n\nРешение команды было простым, но радикальным: они удалили весь контент, сгенерированный ИИ, из своего набора данных на этапах предварительного и промежуточного обучения. Компания Alibaba применила аналогичный подход к Qwen-Image. Только на заключительном этапе тонкой настройки они разрешили вернуть в набор данных тщательно отобранные высококачественные синтетические данные.\n\nРазработчики также придумали новый приём обучения с подкреплением: модель обнаружения, которая наказывает генератор всякий раз, когда он обнаруживает артефакты ИИ. Это заставляет модель создавать текстуры, достаточно реалистичные, чтобы обмануть детектор.\n\nРезультаты говорят сами за себя. В тестах производительности модель 6B регулярно превосходит гораздо более крупные модели, такие как Qwen-Image-20B и HunyuanImage-3.0. А благодаря своей высокой эффективности она использует гораздо меньше видеопамяти - хорошая новость для тех, кто хочет запускать ее локально.\n\nОдна из лучших особенностей модели - это способ обработки текста внутри изображений. Большинство моделей допускают ошибки в правописании, потому что рассматривают слова как абстрактные токены, а не как отдельные буквы. LongCat-Image использует гибридный подход. Она использует Qwen2.5-VL-7B для понимания общего текста запроса, но когда видит текст в кавычках , переключается на токенизатор на уровне символов. Вместо запоминания визуальных шаблонов для каждого возможного слова, модель формирует текст по буквам.\n\nВместо того чтобы объединять все данные в одной модели, команда разработала автономный инструмент под названием LongCat-Image-Edit. Они обнаружили, что синтетические данные, необходимые для обучения редактированию, фактически ухудшают фотореалистичность основной модели.\n\nМодель редактирования начинает работу с контрольной точки промежуточного обучения - момента, когда система еще достаточно гибкая, чтобы осваивать новые навыки. Обучая ее задачам редактирования наряду с генерацией, модель учится следовать инструкциям, не забывая, как выглядят реальные изображения.\n\nКомпания Meituan опубликовала веса для обеих моделей на GitHub и Hugging Face , а также контрольные точки в процессе обучения и полный код конвейера обучения.\n\nДелегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!\n\nИсточник",
    "images": [
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/dbc/4fb/26a/dbc4fb26a1e2a84d54aa9f63a2f912e9.png",
        "alt": ""
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/390/52c/fb8/39052cfb8d530aa98890f29f45909b88.png",
        "alt": "LongCat-Image одинаково эффективно справляется как с фотореалистичными портретами и сложным освещением, так и с отображением текста на вывесках и плакатах  "
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/8e2/411/c33/8e2411c330e2df2cae3ac3f88b41c8e1.png",
        "alt": "Четырехэтапный конвейер подготовки данных отфильтровывает синтетический контент и использует языковые модели обработки изображений для создания подробных описаний изображений"
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/4cb/d16/b55/4cbd16b55a8a0cd63bc0cd43b07960ee.png",
        "alt": " сравнительных тестах LongCat-Image (зеленый) показывает себя не хуже более крупных моделей и часто превосходит их в рендеринге текста и редактировании изображений  "
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/96a/8c2/a91/96a8c2a91eb974ccb4768fb2722a64b5.png",
        "alt": "Сравнительные тесты показывают, как модели обрабатывают текст в сложных сценах, таких как граффити на кирпичных стенах и многоязычные меню  "
      },
      {
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/c93/00c/2a9/c9300c2a918140770249bcb950994df2.png",
        "alt": ""
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/dbc/4fb/26a/dbc4fb26a1e2a84d54aa9f63a2f912e9.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Китайская технологическая компания Meituan выпустила LongCat-Image, новую модель обработки изображений с открытым исходным кодом, которая бросает вызов распространенному в отрасли принципу «чем больше, тем лучше». Сообщается, что модель, имеющая всего 6 миллиардов параметров, значительно превосходит более крупных конкурентов как по фотореализму, так и по рендерингу текста, благодаря строгой обработке данных и продуманному подходу к обработке текста."
      },
      {
        "type": "text",
        "content": "В то время как конкуренты, такие как Tencent и Alibaba, продолжают создавать всё более крупные модели - Hunyuan3.0 вмещает до 80 миллиардов параметров - Meituan пошла в противоположном направлении. Команда утверждает, что масштабирование методом грубой силы тратит ресурсы оборудования, не улучшая при этом качество изображений. Вместо этого LongCat-Image использует архитектуру, аналогичную популярной Flux.1-dev, построенную на гибридном многомодальном диффузионном трансформаторе (MM-DiT)."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/390/52c/fb8/39052cfb8d530aa98890f29f45909b88.png",
        "alt": "LongCat-Image одинаково эффективно справляется как с фотореалистичными портретами и сложным освещением, так и с отображением текста на вывесках и плакатах  "
      },
      {
        "type": "text",
        "content": "Система обрабатывает данные изображения и текста через два отдельных пути внимания на начальных уровнях, прежде чем объединить их позже. Это обеспечивает текстовой подсказке более жесткий контроль над генерацией изображения без увеличения вычислительной нагрузки."
      },
      {
        "type": "text",
        "content": "По мнению исследователей, одной из самых больших проблем современных систем искусственного интеллекта для обработки изображений является загрязнение обучающих данных. Когда модели обучаются на изображениях, созданных другими системами ИИ, они перенимают пластиковую или жирную текстуру. Модель учится использовать упрощенные подходы вместо понимания реальной сложности."
      },
      {
        "type": "text",
        "content": "Решение команды было простым, но радикальным: они удалили весь контент, сгенерированный ИИ, из своего набора данных на этапах предварительного и промежуточного обучения. Компания Alibaba применила аналогичный подход к Qwen-Image. Только на заключительном этапе тонкой настройки они разрешили вернуть в набор данных тщательно отобранные высококачественные синтетические данные."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/8e2/411/c33/8e2411c330e2df2cae3ac3f88b41c8e1.png",
        "alt": "Четырехэтапный конвейер подготовки данных отфильтровывает синтетический контент и использует языковые модели обработки изображений для создания подробных описаний изображений"
      },
      {
        "type": "text",
        "content": "Разработчики также придумали новый приём обучения с подкреплением: модель обнаружения, которая наказывает генератор всякий раз, когда он обнаруживает артефакты ИИ. Это заставляет модель создавать текстуры, достаточно реалистичные, чтобы обмануть детектор."
      },
      {
        "type": "text",
        "content": "Результаты говорят сами за себя. В тестах производительности модель 6B регулярно превосходит гораздо более крупные модели, такие как Qwen-Image-20B и HunyuanImage-3.0. А благодаря своей высокой эффективности она использует гораздо меньше видеопамяти - хорошая новость для тех, кто хочет запускать ее локально."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/4cb/d16/b55/4cbd16b55a8a0cd63bc0cd43b07960ee.png",
        "alt": " сравнительных тестах LongCat-Image (зеленый) показывает себя не хуже более крупных моделей и часто превосходит их в рендеринге текста и редактировании изображений  "
      },
      {
        "type": "text",
        "content": "Одна из лучших особенностей модели - это способ обработки текста внутри изображений. Большинство моделей допускают ошибки в правописании, потому что рассматривают слова как абстрактные токены, а не как отдельные буквы. LongCat-Image использует гибридный подход. Она использует Qwen2.5-VL-7B для понимания общего текста запроса, но когда видит текст в кавычках , переключается на токенизатор на уровне символов. Вместо запоминания визуальных шаблонов для каждого возможного слова, модель формирует текст по буквам."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/96a/8c2/a91/96a8c2a91eb974ccb4768fb2722a64b5.png",
        "alt": "Сравнительные тесты показывают, как модели обрабатывают текст в сложных сценах, таких как граффити на кирпичных стенах и многоязычные меню  "
      },
      {
        "type": "text",
        "content": "Вместо того чтобы объединять все данные в одной модели, команда разработала автономный инструмент под названием LongCat-Image-Edit. Они обнаружили, что синтетические данные, необходимые для обучения редактированию, фактически ухудшают фотореалистичность основной модели."
      },
      {
        "type": "image",
        "url": "https://habrastorage.org/r/w1560/getpro/habr/upload_files/c93/00c/2a9/c9300c2a918140770249bcb950994df2.png",
        "alt": ""
      },
      {
        "type": "text",
        "content": "Модель редактирования начинает работу с контрольной точки промежуточного обучения - момента, когда система еще достаточно гибкая, чтобы осваивать новые навыки. Обучая ее задачам редактирования наряду с генерацией, модель учится следовать инструкциям, не забывая, как выглядят реальные изображения."
      },
      {
        "type": "text",
        "content": "Компания Meituan опубликовала веса для обеих моделей на GitHub и Hugging Face , а также контрольные точки в процессе обучения и полный код конвейера обучения."
      },
      {
        "type": "text",
        "content": "Делегируйте часть рутинных задач вместе с BotHub! Для доступа к сервису не требуется VPN и можно использовать российскую карту. По ссылке вы можете получить 100 000 бесплатных токенов для первых задач и приступить к работе с нейросетями прямо сейчас!"
      },
      {
        "type": "text",
        "content": "Источник"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/09/india-proposes-charging-openai-google-for-training-ai-on-copyrighted-content/",
    "site_type": "techcrunch",
    "title": "India proposes charging OpenAI, Google for training AI on copyrighted content",
    "date": "09.12.2025",
    "text": "India has proposed a mandatory royalty system for AI companies that train their models on copyrighted content — a move that could reshape how OpenAI and Google operate in what has already become one of their most important and fastest-growing markets globally.\n\nOn Tuesday, India’s Department for Promotion of Industry and Internal Trade released a proposed framework that would give AI companies access to all copyrighted works for training in exchange for paying royalties to a new collecting body composed of rights-holding organizations, with payments then distributed to creators. The proposal argues that this “mandatory blanket license” would lower compliance costs for AI firms while ensuring that writers, musicians, artists, and other rights holders are compensated when their work is scraped to train commercial models.\n\nIndia’s proposal comes amid mounting concerns in global markets over how AI companies train their models on copyrighted material, a practice that has triggered lawsuits from authors, news organizations, artists, and other rights holders in the U.S. and Europe. Courts and regulators are still weighing whether such training qualifies as fair use, leaving AI firms operating under legal uncertainty and allowing them to rapidly expand their business without clear regulations.\n\nUnlike the U.S. and the European Union, where policymakers are debating transparency obligations and fair-use boundaries, India is proposing one of the most interventionist approaches yet by giving AI companies automatic access to copyrighted material in exchange for mandatory payment.\n\nThe eight-member committee, formed by the Indian government in late April, argues the system would avoid years of legal uncertainty while ensuring creators are compensated from the outset.\n\nDefending the system, the committee says in a 125-page submission (PDF) that a blanket license “aims to provide an easy access to content for AI developers… reduce transaction costs… [and] ensure fair compensation for rightsholders,” calling it the least burdensome way to manage large-scale AI training. The submission adds that the single collecting body would function as a “single window,” eliminating the need for individual negotiations and enabling royalties to flow to both registered and unregistered creators.\n\nThe committee also points to India’s growing importance as a market for GenAI tools . Citing OpenAI CEO Sam Altman’s remark that India is the company’s second-largest market after the U.S. and “may well become our largest,” it argues that because AI firms derive significant revenue from Indian users while relying on Indian creators’ work to train their models, a portion of that value should flow back to those creators. That, it says, is part of the rationale for establishing a “balanced framework” that guarantees compensation.\n\nIndia’s proposal lands amid intensifying legal battles worldwide over whether AI companies can lawfully use copyrighted material to train their models.\n\nIn India, news agency ANI sued OpenAI in the Delhi High Court , arguing its articles were used without permission — a case that has prompted the court to examine whether AI training is itself an act of reproduction or protected by “fair dealing.” Courts in the U.S. and Europe are confronting similar disputes , with creators alleging that tech companies have built their models on unlicensed content .\n\nNot everyone is convinced by the Indian government’s proposed model, though.\n\nNasscom, the industry body representing technology firms, including Google and Microsoft, filed a formal dissent arguing that India should instead adopt a broad text-and-data-mining exception that would allow AI developers to train on copyrighted content as long as the material is lawfully accessed. It warned that a mandatory licensing regime could slow innovation and said rightsholders who object should be allowed to opt out rather than force companies to pay for all training data.\n\nThe Business Software Alliance, which represents global tech firms, including Adobe, Amazon Web Services, and Microsoft, pressed the Indian government to avoid a purely licensing-based regime. It urged India to introduce an explicit text-and-data-mining exception, arguing that “relying solely on direct or statutory licensing for AI training data may be impractical and may not yield the best outcomes.”\n\nLimiting AI models to smaller sets of licensed or public-domain material, BSA warned, could reduce model quality and “increase the risk that outputs simply reflect trends and biases of the limited training data sets,” adding that a clear TDM exception would better balance innovation and rights holders’ interests.\n\nThe committee did not consider both a broad text-and-data-mining exception and an opt-out model, arguing that such systems either undermine copyright protections or are impossible to enforce. Instead, it proposed a “hybrid model” that would grant AI firms automatic access to all lawfully available copyrighted works while requiring them to pay royalties into the central collecting body that distributes the proceeds to creators.\n\nThe Indian government has now opened the proposal for public consultation, giving companies and other stakeholders 30 days to submit their comments. After reviewing the feedback, the committee will finalize its recommendations before the framework is taken up by the government.\n\nOpenAI and Google did not respond to requests for comments.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2024/06/india-ai.jpg?w=1024",
        "alt": "Image Credits:Jagmeet Singh / TechCrunch"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2024/06/india-ai.jpg?w=1024",
        "alt": "Image Credits:Jagmeet Singh / TechCrunch"
      },
      {
        "type": "text",
        "content": "India has proposed a mandatory royalty system for AI companies that train their models on copyrighted content — a move that could reshape how OpenAI and Google operate in what has already become one of their most important and fastest-growing markets globally."
      },
      {
        "type": "text",
        "content": "On Tuesday, India’s Department for Promotion of Industry and Internal Trade released a proposed framework that would give AI companies access to all copyrighted works for training in exchange for paying royalties to a new collecting body composed of rights-holding organizations, with payments then distributed to creators. The proposal argues that this “mandatory blanket license” would lower compliance costs for AI firms while ensuring that writers, musicians, artists, and other rights holders are compensated when their work is scraped to train commercial models."
      },
      {
        "type": "text",
        "content": "India’s proposal comes amid mounting concerns in global markets over how AI companies train their models on copyrighted material, a practice that has triggered lawsuits from authors, news organizations, artists, and other rights holders in the U.S. and Europe. Courts and regulators are still weighing whether such training qualifies as fair use, leaving AI firms operating under legal uncertainty and allowing them to rapidly expand their business without clear regulations."
      },
      {
        "type": "text",
        "content": "Unlike the U.S. and the European Union, where policymakers are debating transparency obligations and fair-use boundaries, India is proposing one of the most interventionist approaches yet by giving AI companies automatic access to copyrighted material in exchange for mandatory payment."
      },
      {
        "type": "text",
        "content": "The eight-member committee, formed by the Indian government in late April, argues the system would avoid years of legal uncertainty while ensuring creators are compensated from the outset."
      },
      {
        "type": "text",
        "content": "Defending the system, the committee says in a 125-page submission (PDF) that a blanket license “aims to provide an easy access to content for AI developers… reduce transaction costs… [and] ensure fair compensation for rightsholders,” calling it the least burdensome way to manage large-scale AI training. The submission adds that the single collecting body would function as a “single window,” eliminating the need for individual negotiations and enabling royalties to flow to both registered and unregistered creators."
      },
      {
        "type": "text",
        "content": "The committee also points to India’s growing importance as a market for GenAI tools . Citing OpenAI CEO Sam Altman’s remark that India is the company’s second-largest market after the U.S. and “may well become our largest,” it argues that because AI firms derive significant revenue from Indian users while relying on Indian creators’ work to train their models, a portion of that value should flow back to those creators. That, it says, is part of the rationale for establishing a “balanced framework” that guarantees compensation."
      },
      {
        "type": "text",
        "content": "India’s proposal lands amid intensifying legal battles worldwide over whether AI companies can lawfully use copyrighted material to train their models."
      },
      {
        "type": "text",
        "content": "In India, news agency ANI sued OpenAI in the Delhi High Court , arguing its articles were used without permission — a case that has prompted the court to examine whether AI training is itself an act of reproduction or protected by “fair dealing.” Courts in the U.S. and Europe are confronting similar disputes , with creators alleging that tech companies have built their models on unlicensed content ."
      },
      {
        "type": "text",
        "content": "Not everyone is convinced by the Indian government’s proposed model, though."
      },
      {
        "type": "text",
        "content": "Nasscom, the industry body representing technology firms, including Google and Microsoft, filed a formal dissent arguing that India should instead adopt a broad text-and-data-mining exception that would allow AI developers to train on copyrighted content as long as the material is lawfully accessed. It warned that a mandatory licensing regime could slow innovation and said rightsholders who object should be allowed to opt out rather than force companies to pay for all training data."
      },
      {
        "type": "text",
        "content": "The Business Software Alliance, which represents global tech firms, including Adobe, Amazon Web Services, and Microsoft, pressed the Indian government to avoid a purely licensing-based regime. It urged India to introduce an explicit text-and-data-mining exception, arguing that “relying solely on direct or statutory licensing for AI training data may be impractical and may not yield the best outcomes.”"
      },
      {
        "type": "text",
        "content": "Limiting AI models to smaller sets of licensed or public-domain material, BSA warned, could reduce model quality and “increase the risk that outputs simply reflect trends and biases of the limited training data sets,” adding that a clear TDM exception would better balance innovation and rights holders’ interests."
      },
      {
        "type": "text",
        "content": "The committee did not consider both a broad text-and-data-mining exception and an opt-out model, arguing that such systems either undermine copyright protections or are impossible to enforce. Instead, it proposed a “hybrid model” that would grant AI firms automatic access to all lawfully available copyrighted works while requiring them to pay royalties into the central collecting body that distributes the proceeds to creators."
      },
      {
        "type": "text",
        "content": "The Indian government has now opened the proposal for public consultation, giving companies and other stakeholders 30 days to submit their comments. After reviewing the feedback, the committee will finalize its recommendations before the framework is taken up by the government."
      },
      {
        "type": "text",
        "content": "OpenAI and Google did not respond to requests for comments."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/09/microsoft-to-invest-17-5b-in-india-by-2029-as-ai-race-accelerates/",
    "site_type": "techcrunch",
    "title": "Microsoft to invest $17.5B in India by 2029 as AI race accelerates",
    "date": "09.12.2025",
    "text": "Microsoft plans to invest $17.5 billion in India over the next four years, expanding its AI and cloud footprint in the South Asian nation, whose vast online and smartphone user base is turning it into a critical battleground for global tech companies.\n\nAnnounced on Tuesday, the investment — Microsoft’s largest in Asia — will fund new data centers, AI infrastructure, and skilling programs from 2026 to 2029, building on the $3 billion the company committed in India in January.\n\nMicrosoft’s move comes as major U.S. tech companies ramp up spending on data centers and AI compute worldwide, with India emerging as a strategic prize thanks to its fast-growing developer base and one of the world’s largest pools of internet and smartphone users.\n\nThe latest push also puts pressure on rivals such as Google, Amazon, and OpenAI, which are growing their presence in India to tap demand for cloud services and AI tools from businesses, startups, and government agencies. Moreover, the investment aligns with New Delhi’s push to accelerate digital infrastructure and AI adoption across sectors, as India looks to position itself as a global technology hub while addressing concerns around data governance and equitable access.\n\nThe announcement comes during Microsoft CEO Satya Nadella’s visit to India and follows his meeting with Prime Minister Narendra Modi on Tuesday, ahead of a keynote in New Delhi on Wednesday. The Redmond-based company also said it will open a new data center region in Hyderabad by mid-2026, its largest in India, comprising three availability zones — a footprint the company described as roughly the size of two Eden Gardens stadiums. Microsoft said it will continue expanding its three existing data-center regions in Chennai, Hyderabad, and Pune.\n\nAs part of the push, Microsoft also announced it will work with the Ministry of Labour and Employment to integrate advanced AI capabilities into two of its flagship digital public platforms — e-Shram and the National Career Service — to offer AI-driven services to more than 310 million informal workers.\n\nThe two Indian government platforms will use Microsoft’s Azure OpenAI Service to provide multilingual access, AI-assisted job matching, predictive analytics on skill and demand trends, automated résumé creation, and personalized pathways, the company said.\n\nMicrosoft also said it is rolling out new sovereign cloud options for Indian customers, including a Sovereign Public Cloud now available across its India regions and a Sovereign Private Cloud powered by Azure Local for both connected and air-gapped operations. The offerings would help enterprises meet regulatory and data-residency requirements and support high-performance workloads with access to the latest Nvidia GPUs and Microsoft 365 services, the company noted.\n\nAdditionally, Microsoft said its skilling efforts are also accelerating, noting that through its “ADVANTA(I)GE India” initiative, it has trained 5.6 million people since January — well ahead of its goal of training 10 million by 2030 — with the programs enabling more than 125,000 individuals to secure jobs or start businesses. The company is doubling its earlier commitment and now aims to equip 20 million Indians with basic AI skills by 2030, working with government agencies, industry partners, and digital public platforms to broaden access to training.\n\nMicrosoft’s investment commitment comes just months after Google announced a $15 billion plan to build an AI hub and data-center infrastructure in India — the company’s largest investment in the country and one that follows its earlier $10 billion pledge in 2020.\n\nIn the recent months, India has emerged as an especially attractive market for global tech companies as they look for regions to expand their AI footprint, drawn by the country’s vast base of internet subscribers, hundreds of millions of smartphone users, a fast-growing startup ecosystem, and the Indian government’s aggressive digitization agenda — all of which promise both consumer scale and enterprise demand. The push has accelerated this year, with OpenAI and Anthropic setting up offices in India, and Google and Perplexity striking partnerships with major telecom operators Reliance Jio and Bharti Airtel, respectively, to deepen their reach in the market.\n\nHowever, even as global tech firms ramp up investment, hyperscalers are expected to face significant constraints in India, where data-center expansion is challenged by patchy power availability , high energy costs, and water scarcity in several regions — factors that could slow the build-out of AI infrastructure and raise operating expenses for cloud providers.\n\nNonetheless, the Indian government has been pushing aggressively to draw more big-tech investment, framing large-scale data-center and AI projects as central to its economic and digital-public-infrastructure ambitions. Despite the constraints, New Delhi has rolled out incentives for AI and semiconductor projects, eased some regulatory hurdles, and encouraged partnerships with domestic telecom and IT firms to anchor more of the global AI value chain in India.\n\nWhen it comes to AI, the world is optimistic about India! Had a very productive discussion with Mr. Satya Nadella. Happy to see India being the place where Microsoft will make its largest-ever investment in Asia. The youth of India will harness this opportunity to innovate… https://t.co/fMFcGQ8ctK\n\n“Microsoft has been part of India’s fabric for more than three decades,” said Puneet Chandok, president, Microsoft India and South Asia, in a prepared statement. “As the nation moves confidently into its AI-first future, we are proud to stand as a trusted partner in advancing the infrastructure, innovation, and opportunity that can power a billion dreams.”\n\nMicrosoft already employs more than 22,000 people across Bengaluru, Hyderabad, Pune, Gurugram, Noida, and other cities, including engineering teams that build AI products such as Copilot Studio, Azure AI Search, AI agents, speech and translation tools, and Azure Machine Learning for global markets, while also supporting the company’s domestic operations.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2020/01/GettyImages-1173107665-1.jpg?w=1024",
        "alt": "Image Credits:Mark Kauzlarich/Bloomberg / Getty Images"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2020/01/GettyImages-1173107665-1.jpg?w=1024",
        "alt": "Image Credits:Mark Kauzlarich/Bloomberg / Getty Images"
      },
      {
        "type": "text",
        "content": "Microsoft plans to invest $17.5 billion in India over the next four years, expanding its AI and cloud footprint in the South Asian nation, whose vast online and smartphone user base is turning it into a critical battleground for global tech companies."
      },
      {
        "type": "text",
        "content": "Announced on Tuesday, the investment — Microsoft’s largest in Asia — will fund new data centers, AI infrastructure, and skilling programs from 2026 to 2029, building on the $3 billion the company committed in India in January."
      },
      {
        "type": "text",
        "content": "Microsoft’s move comes as major U.S. tech companies ramp up spending on data centers and AI compute worldwide, with India emerging as a strategic prize thanks to its fast-growing developer base and one of the world’s largest pools of internet and smartphone users."
      },
      {
        "type": "text",
        "content": "The latest push also puts pressure on rivals such as Google, Amazon, and OpenAI, which are growing their presence in India to tap demand for cloud services and AI tools from businesses, startups, and government agencies. Moreover, the investment aligns with New Delhi’s push to accelerate digital infrastructure and AI adoption across sectors, as India looks to position itself as a global technology hub while addressing concerns around data governance and equitable access."
      },
      {
        "type": "text",
        "content": "The announcement comes during Microsoft CEO Satya Nadella’s visit to India and follows his meeting with Prime Minister Narendra Modi on Tuesday, ahead of a keynote in New Delhi on Wednesday. The Redmond-based company also said it will open a new data center region in Hyderabad by mid-2026, its largest in India, comprising three availability zones — a footprint the company described as roughly the size of two Eden Gardens stadiums. Microsoft said it will continue expanding its three existing data-center regions in Chennai, Hyderabad, and Pune."
      },
      {
        "type": "text",
        "content": "As part of the push, Microsoft also announced it will work with the Ministry of Labour and Employment to integrate advanced AI capabilities into two of its flagship digital public platforms — e-Shram and the National Career Service — to offer AI-driven services to more than 310 million informal workers."
      },
      {
        "type": "text",
        "content": "The two Indian government platforms will use Microsoft’s Azure OpenAI Service to provide multilingual access, AI-assisted job matching, predictive analytics on skill and demand trends, automated résumé creation, and personalized pathways, the company said."
      },
      {
        "type": "text",
        "content": "Microsoft also said it is rolling out new sovereign cloud options for Indian customers, including a Sovereign Public Cloud now available across its India regions and a Sovereign Private Cloud powered by Azure Local for both connected and air-gapped operations. The offerings would help enterprises meet regulatory and data-residency requirements and support high-performance workloads with access to the latest Nvidia GPUs and Microsoft 365 services, the company noted."
      },
      {
        "type": "text",
        "content": "Additionally, Microsoft said its skilling efforts are also accelerating, noting that through its “ADVANTA(I)GE India” initiative, it has trained 5.6 million people since January — well ahead of its goal of training 10 million by 2030 — with the programs enabling more than 125,000 individuals to secure jobs or start businesses. The company is doubling its earlier commitment and now aims to equip 20 million Indians with basic AI skills by 2030, working with government agencies, industry partners, and digital public platforms to broaden access to training."
      },
      {
        "type": "text",
        "content": "Microsoft’s investment commitment comes just months after Google announced a $15 billion plan to build an AI hub and data-center infrastructure in India — the company’s largest investment in the country and one that follows its earlier $10 billion pledge in 2020."
      },
      {
        "type": "text",
        "content": "In the recent months, India has emerged as an especially attractive market for global tech companies as they look for regions to expand their AI footprint, drawn by the country’s vast base of internet subscribers, hundreds of millions of smartphone users, a fast-growing startup ecosystem, and the Indian government’s aggressive digitization agenda — all of which promise both consumer scale and enterprise demand. The push has accelerated this year, with OpenAI and Anthropic setting up offices in India, and Google and Perplexity striking partnerships with major telecom operators Reliance Jio and Bharti Airtel, respectively, to deepen their reach in the market."
      },
      {
        "type": "text",
        "content": "However, even as global tech firms ramp up investment, hyperscalers are expected to face significant constraints in India, where data-center expansion is challenged by patchy power availability , high energy costs, and water scarcity in several regions — factors that could slow the build-out of AI infrastructure and raise operating expenses for cloud providers."
      },
      {
        "type": "text",
        "content": "Nonetheless, the Indian government has been pushing aggressively to draw more big-tech investment, framing large-scale data-center and AI projects as central to its economic and digital-public-infrastructure ambitions. Despite the constraints, New Delhi has rolled out incentives for AI and semiconductor projects, eased some regulatory hurdles, and encouraged partnerships with domestic telecom and IT firms to anchor more of the global AI value chain in India."
      },
      {
        "type": "text",
        "content": "When it comes to AI, the world is optimistic about India! Had a very productive discussion with Mr. Satya Nadella. Happy to see India being the place where Microsoft will make its largest-ever investment in Asia. The youth of India will harness this opportunity to innovate… https://t.co/fMFcGQ8ctK"
      },
      {
        "type": "text",
        "content": "“Microsoft has been part of India’s fabric for more than three decades,” said Puneet Chandok, president, Microsoft India and South Asia, in a prepared statement. “As the nation moves confidently into its AI-first future, we are proud to stand as a trusted partner in advancing the infrastructure, innovation, and opportunity that can power a billion dreams.”"
      },
      {
        "type": "text",
        "content": "Microsoft already employs more than 22,000 people across Bengaluru, Hyderabad, Pune, Gurugram, Noida, and other cities, including engineering teams that build AI products such as Copilot Studio, Azure AI Search, AI agents, speech and translation tools, and Azure Machine Learning for global markets, while also supporting the company’s domestic operations."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/09/eu-launches-antitrust-probe-into-googles-ai-search-tools/",
    "site_type": "techcrunch",
    "title": "EU launches antitrust probe into Google’s AI search tools",
    "date": "09.12.2025",
    "text": "Even as Big Tech and American tech elites criticize how the European Union is implementing rules to regulate tech and AI on the continent, the bloc isn’t letting competition concerns slide. The European Commission has launched an investigation into whether Google may have breached EU’s competition laws by using content from websites without compensating owners to generate answers for its AI summaries that appear above search results.\n\nThe EC also will look at how AI summaries use videos from YouTube to generate answers. The investigation will examine if Google is harming competition in the AI market by granting itself access to websites’ content, and imposing “unfair terms and conditions on publishers and content creators.”\n\n“The Commission will investigate to what extent the generation of AI Overviews and AI Mode by Google is based on web publishers’ content without appropriate compensation for that, and without the possibility for publishers to refuse without losing access to Google Search,” the bloc’s executive arm wrote in a statement.\n\nGoogle’s AI Overview and AI Mode are the two chief products being investigated here, and the EC highlights that the tech giant doesn’t leave websites or content producers with much choice since it directs a majority of web traffic, doesn’t pay them for using their content, and doesn’t allow YouTube uploads if you don’t let Google use that data.\n\nThe EU is also concerned over the fact that Google doesn’t allow rival AI companies to use YouTube content to train their own AI models.\n\n“This complaint risks stifling innovation in a market that is more competitive than ever,” a Google spokesperson said in an emailed statement. “Europeans deserve to benefit from the latest technologies and we will continue to work closely with the news and creative industries as they transition to the AI era.”\n\nThe investigation comes at a time when companies developing AI models and content are being sued for copyright infringement by publishers and websites. AI search tool Perplexity, for one, has been sued by several outlets , including The New York Times, Chicago Tribune, News Corp, New York Post, Merriam-Webster, Nikkei, and Reddit.\n\nThe EU’s investigation differs, however, because in many cases, these media companies are suing as a way to negotiate content-licensing deals with AI firms in hopes of compensating creators and being paid for their content. The EU is seeking to level the playing field for AI companies that compete with Google, which according to some reports , benefits from its reach by being able to train its AI models on much more of the internet than its rivals.\n\nUnder consistent and widespread criticism of its AI regulation, however, the EU is considering simplifying its AI rules , and has proposed to delay the implementation of rules for the use of AI in high-risk applications.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-2206888090.jpg?w=1024",
        "alt": "Image Credits:Smith Collection/Gado / Getty Images"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-2206888090.jpg?w=1024",
        "alt": "Image Credits:Smith Collection/Gado / Getty Images"
      },
      {
        "type": "text",
        "content": "Even as Big Tech and American tech elites criticize how the European Union is implementing rules to regulate tech and AI on the continent, the bloc isn’t letting competition concerns slide. The European Commission has launched an investigation into whether Google may have breached EU’s competition laws by using content from websites without compensating owners to generate answers for its AI summaries that appear above search results."
      },
      {
        "type": "text",
        "content": "The EC also will look at how AI summaries use videos from YouTube to generate answers. The investigation will examine if Google is harming competition in the AI market by granting itself access to websites’ content, and imposing “unfair terms and conditions on publishers and content creators.”"
      },
      {
        "type": "text",
        "content": "“The Commission will investigate to what extent the generation of AI Overviews and AI Mode by Google is based on web publishers’ content without appropriate compensation for that, and without the possibility for publishers to refuse without losing access to Google Search,” the bloc’s executive arm wrote in a statement."
      },
      {
        "type": "text",
        "content": "Google’s AI Overview and AI Mode are the two chief products being investigated here, and the EC highlights that the tech giant doesn’t leave websites or content producers with much choice since it directs a majority of web traffic, doesn’t pay them for using their content, and doesn’t allow YouTube uploads if you don’t let Google use that data."
      },
      {
        "type": "text",
        "content": "The EU is also concerned over the fact that Google doesn’t allow rival AI companies to use YouTube content to train their own AI models."
      },
      {
        "type": "text",
        "content": "“This complaint risks stifling innovation in a market that is more competitive than ever,” a Google spokesperson said in an emailed statement. “Europeans deserve to benefit from the latest technologies and we will continue to work closely with the news and creative industries as they transition to the AI era.”"
      },
      {
        "type": "text",
        "content": "The investigation comes at a time when companies developing AI models and content are being sued for copyright infringement by publishers and websites. AI search tool Perplexity, for one, has been sued by several outlets , including The New York Times, Chicago Tribune, News Corp, New York Post, Merriam-Webster, Nikkei, and Reddit."
      },
      {
        "type": "text",
        "content": "The EU’s investigation differs, however, because in many cases, these media companies are suing as a way to negotiate content-licensing deals with AI firms in hopes of compensating creators and being paid for their content. The EU is seeking to level the playing field for AI companies that compete with Google, which according to some reports , benefits from its reach by being able to train its AI models on much more of the internet than its rivals."
      },
      {
        "type": "text",
        "content": "Under consistent and widespread criticism of its AI regulation, however, the EU is considering simplifying its AI rules , and has proposed to delay the implementation of rules for the use of AI in high-risk applications."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/09/googles-first-ai-glasses-expected-next-year/",
    "site_type": "techcrunch",
    "title": "Google’s first AI glasses expected next year",
    "date": "09.12.2025",
    "text": "Google will launch its first AI glasses in 2026, according to a company blog post .\n\nAt Google’s I/O event in May, the company announced partnerships with Gentle Monster and Warby Parker to create consumer wearables based on Android XR, the operating system that powers Samsung’s Galaxy XR headset .\n\nBut you can’t wear a bulky headset while out in the real world, which makes smart glasses appealing as a less obtrusive smart wearable.\n\n“For AI and XR to be truly helpful, the hardware needs to fit seamlessly into your life and match your personal style,” Google writes . “We want to give you the freedom to choose the right balance of weight, style and immersion for your needs.”\n\nGoogle is working on various types of AI-powered glasses — one model is designed for screen-free assistance, using built-in speakers, microphones, and cameras to allow the user to communicate with Gemini and take photos. The other model has an in-lens display — which is only visible to the person wearing the glasses — that can show turn-by-turn directions or closed captioning.\n\nGoogle also shared a preview of the wired XR glasses from Xreal called Project Aura. This model situates itself between a bulky headset and an unobtrusive pair of glasses. Beyond just an in-lens display, the Project Aura glasses can function as an extended workplace or entertainment device, allowing the user to use Google’s suite of products or stream video as they would in a more advanced headset.\n\nWhile Meta has gotten off to an early lead in smart glasses development, Google now joins Apple and Snap among the companies expected to challenge Meta with their own hardware next year.\n\nMeta’s smart glasses have caught on in part thanks to its partnership with Ray-Ban, and it sells these products in retail stores. Google’s partnership with Warby Parker seems like it will follow a similar strategy, committing $75 million thus far to support the eyewear company’s product development and commercialization costs. If Warby Parker meets certain milestones, Google will commit an additional $75 million and take an equity stake in the brand.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/05/glasses_2745f1.jpg?w=1024",
        "alt": "Image Credits:Google"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/05/glasses_2745f1.jpg?w=1024",
        "alt": "Image Credits:Google"
      },
      {
        "type": "text",
        "content": "Google will launch its first AI glasses in 2026, according to a company blog post ."
      },
      {
        "type": "text",
        "content": "At Google’s I/O event in May, the company announced partnerships with Gentle Monster and Warby Parker to create consumer wearables based on Android XR, the operating system that powers Samsung’s Galaxy XR headset ."
      },
      {
        "type": "text",
        "content": "But you can’t wear a bulky headset while out in the real world, which makes smart glasses appealing as a less obtrusive smart wearable."
      },
      {
        "type": "text",
        "content": "“For AI and XR to be truly helpful, the hardware needs to fit seamlessly into your life and match your personal style,” Google writes . “We want to give you the freedom to choose the right balance of weight, style and immersion for your needs.”"
      },
      {
        "type": "text",
        "content": "Google is working on various types of AI-powered glasses — one model is designed for screen-free assistance, using built-in speakers, microphones, and cameras to allow the user to communicate with Gemini and take photos. The other model has an in-lens display — which is only visible to the person wearing the glasses — that can show turn-by-turn directions or closed captioning."
      },
      {
        "type": "text",
        "content": "Google also shared a preview of the wired XR glasses from Xreal called Project Aura. This model situates itself between a bulky headset and an unobtrusive pair of glasses. Beyond just an in-lens display, the Project Aura glasses can function as an extended workplace or entertainment device, allowing the user to use Google’s suite of products or stream video as they would in a more advanced headset."
      },
      {
        "type": "text",
        "content": "While Meta has gotten off to an early lead in smart glasses development, Google now joins Apple and Snap among the companies expected to challenge Meta with their own hardware next year."
      },
      {
        "type": "text",
        "content": "Meta’s smart glasses have caught on in part thanks to its partnership with Ray-Ban, and it sells these products in retail stores. Google’s partnership with Warby Parker seems like it will follow a similar strategy, committing $75 million thus far to support the eyewear company’s product development and commercialization costs. If Warby Parker meets certain milestones, Google will commit an additional $75 million and take an equity stake in the brand."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/09/amazon-adds-delivery-tracking-last-minute-adds-gift-ideas-to-alexa/",
    "site_type": "techcrunch",
    "title": "Amazon adds delivery tracking, last-minute adds, gift ideas to Alexa+",
    "date": "09.12.2025",
    "text": "Amazon has long had to deal with the reality that owners of its Echo smart speakers weren’t using its voice-controlled assistant, Alexa, to buy things, as it had hoped. The tech giant hasn’t given up on that dream yet: Its AI assistant, Alexa+, is getting new shopping-enabled features in the U.S. and Canada from today, including a shopping hub, tools to add items to recent orders, and personalized recommendations.\n\nThe company has already been working on features that make Alexa+ more of a shopping assistant with abilities like automated deal tracking and automatic purchases. The former feature lets you set Alexa to alert you when items in your cart or list drop below a certain price point. If you also have automatic purchases set up, Alexa can immediately order an item when it reaches that target price.\n\nNow, Amazon says it’s turning its Echo devices with a screen — the Echo Show 15 and 21 — into a shopping hub with an interface that it’s calling the “Shopping Essentials” experience. From this dashboard, Amazon shoppers can track deliveries in real time, see information about recent orders, get reminders about household essentials they need to reorder, and view their shopping list and saved items.\n\nThe screen would also let shoppers tap to see more products and add items directly to their cart and then check out.\n\nTo access the new experience, Amazon users can say “Alexa, where’s my stuff?” or “Open Shopping Essentials.” Soon, a shopping widget will be available to add to the Echo device’s home screen, too.\n\nAnother new feature rolling out now will allow Alexa device owners to add items to an upcoming delivery at any time until the item leaves the warehouse. This builds on a similar feature recently added to Amazon’s retail website and app, so it isn’t necessarily an Alexa+ exclusive, but the feature wasn’t live on Alexa devices so far.\n\nAlexa+ is also gaining the ability to recommend gifts. You’ll be able to describe who you’re shopping for, or the occasion, and Alexa+ will display product suggestions on the screen, organized into categories.\n\nAmazon says Alexa+ is now available to “tens of millions” of customers, and these new features are live for users in the U.S. and Canada. While not everyone has been happy with Alexa+, the company says the percentage of users who downgraded back to the AI-free interface remains in the “very low single digits.”",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/08/alexa-plus-event.png?w=1024",
        "alt": "Image Credits:Amazon"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2024/11/amazon-echo-show-15-21.png?w=680",
        "alt": "Image Credits:Amazon"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/08/alexa-plus-event.png?w=1024",
        "alt": "Image Credits:Amazon"
      },
      {
        "type": "text",
        "content": "Amazon has long had to deal with the reality that owners of its Echo smart speakers weren’t using its voice-controlled assistant, Alexa, to buy things, as it had hoped. The tech giant hasn’t given up on that dream yet: Its AI assistant, Alexa+, is getting new shopping-enabled features in the U.S. and Canada from today, including a shopping hub, tools to add items to recent orders, and personalized recommendations."
      },
      {
        "type": "text",
        "content": "The company has already been working on features that make Alexa+ more of a shopping assistant with abilities like automated deal tracking and automatic purchases. The former feature lets you set Alexa to alert you when items in your cart or list drop below a certain price point. If you also have automatic purchases set up, Alexa can immediately order an item when it reaches that target price."
      },
      {
        "type": "text",
        "content": "Now, Amazon says it’s turning its Echo devices with a screen — the Echo Show 15 and 21 — into a shopping hub with an interface that it’s calling the “Shopping Essentials” experience. From this dashboard, Amazon shoppers can track deliveries in real time, see information about recent orders, get reminders about household essentials they need to reorder, and view their shopping list and saved items."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2024/11/amazon-echo-show-15-21.png?w=680",
        "alt": "Image Credits:Amazon"
      },
      {
        "type": "text",
        "content": "The screen would also let shoppers tap to see more products and add items directly to their cart and then check out."
      },
      {
        "type": "text",
        "content": "To access the new experience, Amazon users can say “Alexa, where’s my stuff?” or “Open Shopping Essentials.” Soon, a shopping widget will be available to add to the Echo device’s home screen, too."
      },
      {
        "type": "text",
        "content": "Another new feature rolling out now will allow Alexa device owners to add items to an upcoming delivery at any time until the item leaves the warehouse. This builds on a similar feature recently added to Amazon’s retail website and app, so it isn’t necessarily an Alexa+ exclusive, but the feature wasn’t live on Alexa devices so far."
      },
      {
        "type": "text",
        "content": "Alexa+ is also gaining the ability to recommend gifts. You’ll be able to describe who you’re shopping for, or the occasion, and Alexa+ will display product suggestions on the screen, organized into categories."
      },
      {
        "type": "text",
        "content": "Amazon says Alexa+ is now available to “tens of millions” of customers, and these new features are live for users in the U.S. and Canada. While not everyone has been happy with Alexa+, the company says the percentage of users who downgraded back to the AI-free interface remains in the “very low single digits.”"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/09/openai-anthropic-and-block-join-new-linux-foundation-effort-to-standardize-the-ai-agent-era/",
    "site_type": "techcrunch",
    "title": "OpenAI, Anthropic, and Block join new Linux Foundation effort to standardize the AI agent era",
    "date": "09.12.2025",
    "text": "As AI moves beyond chatbots and toward systems that can take actions, the Linux Foundation is launching a new group dedicated to keeping AI agents from splintering into a mess of incompatible, locked-down products.\n\nThe group, dubbed the Agentic AI Foundation (AAIF), will act as a neutral home for open source projects related to AI agents. Anchoring the AAIF at launch are donations from Anthropic, Block, and OpenAI.\n\nAnthropic is donating its MCP (Model Context Protocol), a standard way to connect models and agents to tools and data; Block is contributing Goose, its open source agent framework; and OpenAI is bringing AGENTS.md to the table, its simple instruction file developers can add to a repository to tell AI coding tools how to behave. You can think of these tools as the basic plumbing of the agent era.\n\nOther members in the AAIF include AWS, Bloomberg, Cloudflare, and Google, signaling an industry-level push for shared guardrails so that AI agents can be trustworthy at scale.\n\nIn OpenAI engineer Nick Cooper’s view, protocols are essentially a shared language that lets different agents and systems work together without every developer reinventing integrations from scratch.\n\n“We need multiple [protocols] to negotiate, communicate, and work together to deliver value for people, and that sort of openness and communication is why it’s not ever going to be one provider, one host, one company,” Cooper told TechCrunch.\n\nJim Zemlin , executive director of the Linux Foundation, put it more bluntly in conversations around the launch: The goal is to avoid a future of “closed wall” proprietary stacks, where tool connections, agent behavior, and orchestration are locked behind a handful of platforms.\n\n“By bringing these projects together under the AAIF, we are now able to coordinate interoperability, safety patterns, and best practices specifically for AI agents,” Zemlin said.\n\nBlock — the fintech company behind Square and Cash App — isn’t known for AI infrastructure, but it’s making an openness play with Goose. AI tech lead Brad Axen frames it as proof that open alternatives can match proprietary agents at scale, with thousands of engineers using it weekly for coding, data analysis, and documentation.\n\nOpen sourcing Goose serves a dual purpose for Block.\n\n“Getting it out into the world gives us a place for other people to come help us make it better,” Axen told TechCrunch. “We have a lot of contributors from open source, and everything they do to improve it comes back to our company.”\n\nMeanwhile, donating Goose to the Linux Foundation gives Block access to community stress tests while positioning it as a working example of AAIF’s vision — an agent framework designed to plug into shared building blocks like MCP and AGENTS.md.\n\nAnthropic is making a similar move at the protocol layer, handing MCP to the Linux Foundation. The goal: make MCP the neutral infrastructure connecting AI models to tools, data, and applications without endless one-off adapters.\n\n“The main goal is to have enough adoption in the world that it’s the de facto standard,” MCP co-creator David Soria Parra told TechCrunch. “We’re all better off if we have an open integration center where you can build something once as a developer and use it across any client.”\n\nDonating MCP to AAIF signals that the protocol won’t be controlled by a single vendor.\n\nThat governance point is central to why the Linux Foundation created a new umbrella at all. The organization already hosts major AI and developer infrastructure projects — everything from PyTorch and Ray to Kubernetes — but says AAIF is specifically aimed at agent standards and orchestration, including shared safety patterns and interoperability.\n\nAAIF’s structure is funded through a “directed fund,” meaning companies can contribute money through membership dues. But Zemlin of the Linux Foundation argues that funding doesn’t equal control: Project roadmaps are set by technical steering committees, and no single member gets unilateral say over direction.\n\nStill, the big question is whether AAIF becomes real infrastructure or just another industry logo alliance.\n\n“An early indicator of success, in addition to adoption of these standards, would be the development and implementation of shared standards being used by vendor agents around the world,” Zemlin said.\n\nFor OpenAI’s Cooper, success would look like an evolution of standards: “I don’t want it to be a stagnant thing. I don’t want these protocols to be part of this foundation, and that’s where they sat for two years. They should evolve and continually accept further input.”\n\nThere’s also a more subtle consequence: Even with open governance, one company’s implementation could become the default simply because it ships fastest or gains the most usage. Zemlin says that’s not necessarily a bad thing, though. He points to open source history — like Kubernetes “winning” the container race — as evidence that “dominance emerges from merit and not vendor control.”\n\nFor developers and enterprises, the short-term appeal is clear: less time building custom connectors, more predictable agent behavior across codebases, and simpler deployment in security-conscious environments. The larger vision is more ambitious: If tools like MCP, AGENTS.md, and Goose become standard infrastructure, the agent landscape could shift from closed platforms to an open, mix-and-match software world reminiscent of the interoperable systems that built the modern web.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-12.11.34-PM.png?w=1024",
        "alt": "Image Credits:Anthropic"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-12.11.34-PM.png?w=1024",
        "alt": "Image Credits:Anthropic"
      },
      {
        "type": "text",
        "content": "As AI moves beyond chatbots and toward systems that can take actions, the Linux Foundation is launching a new group dedicated to keeping AI agents from splintering into a mess of incompatible, locked-down products."
      },
      {
        "type": "text",
        "content": "The group, dubbed the Agentic AI Foundation (AAIF), will act as a neutral home for open source projects related to AI agents. Anchoring the AAIF at launch are donations from Anthropic, Block, and OpenAI."
      },
      {
        "type": "text",
        "content": "Anthropic is donating its MCP (Model Context Protocol), a standard way to connect models and agents to tools and data; Block is contributing Goose, its open source agent framework; and OpenAI is bringing AGENTS.md to the table, its simple instruction file developers can add to a repository to tell AI coding tools how to behave. You can think of these tools as the basic plumbing of the agent era."
      },
      {
        "type": "text",
        "content": "Other members in the AAIF include AWS, Bloomberg, Cloudflare, and Google, signaling an industry-level push for shared guardrails so that AI agents can be trustworthy at scale."
      },
      {
        "type": "text",
        "content": "In OpenAI engineer Nick Cooper’s view, protocols are essentially a shared language that lets different agents and systems work together without every developer reinventing integrations from scratch."
      },
      {
        "type": "text",
        "content": "“We need multiple [protocols] to negotiate, communicate, and work together to deliver value for people, and that sort of openness and communication is why it’s not ever going to be one provider, one host, one company,” Cooper told TechCrunch."
      },
      {
        "type": "text",
        "content": "Jim Zemlin , executive director of the Linux Foundation, put it more bluntly in conversations around the launch: The goal is to avoid a future of “closed wall” proprietary stacks, where tool connections, agent behavior, and orchestration are locked behind a handful of platforms."
      },
      {
        "type": "text",
        "content": "“By bringing these projects together under the AAIF, we are now able to coordinate interoperability, safety patterns, and best practices specifically for AI agents,” Zemlin said."
      },
      {
        "type": "text",
        "content": "Block — the fintech company behind Square and Cash App — isn’t known for AI infrastructure, but it’s making an openness play with Goose. AI tech lead Brad Axen frames it as proof that open alternatives can match proprietary agents at scale, with thousands of engineers using it weekly for coding, data analysis, and documentation."
      },
      {
        "type": "text",
        "content": "Open sourcing Goose serves a dual purpose for Block."
      },
      {
        "type": "text",
        "content": "“Getting it out into the world gives us a place for other people to come help us make it better,” Axen told TechCrunch. “We have a lot of contributors from open source, and everything they do to improve it comes back to our company.”"
      },
      {
        "type": "text",
        "content": "Meanwhile, donating Goose to the Linux Foundation gives Block access to community stress tests while positioning it as a working example of AAIF’s vision — an agent framework designed to plug into shared building blocks like MCP and AGENTS.md."
      },
      {
        "type": "text",
        "content": "Anthropic is making a similar move at the protocol layer, handing MCP to the Linux Foundation. The goal: make MCP the neutral infrastructure connecting AI models to tools, data, and applications without endless one-off adapters."
      },
      {
        "type": "text",
        "content": "“The main goal is to have enough adoption in the world that it’s the de facto standard,” MCP co-creator David Soria Parra told TechCrunch. “We’re all better off if we have an open integration center where you can build something once as a developer and use it across any client.”"
      },
      {
        "type": "text",
        "content": "Donating MCP to AAIF signals that the protocol won’t be controlled by a single vendor."
      },
      {
        "type": "text",
        "content": "That governance point is central to why the Linux Foundation created a new umbrella at all. The organization already hosts major AI and developer infrastructure projects — everything from PyTorch and Ray to Kubernetes — but says AAIF is specifically aimed at agent standards and orchestration, including shared safety patterns and interoperability."
      },
      {
        "type": "text",
        "content": "AAIF’s structure is funded through a “directed fund,” meaning companies can contribute money through membership dues. But Zemlin of the Linux Foundation argues that funding doesn’t equal control: Project roadmaps are set by technical steering committees, and no single member gets unilateral say over direction."
      },
      {
        "type": "text",
        "content": "Still, the big question is whether AAIF becomes real infrastructure or just another industry logo alliance."
      },
      {
        "type": "text",
        "content": "“An early indicator of success, in addition to adoption of these standards, would be the development and implementation of shared standards being used by vendor agents around the world,” Zemlin said."
      },
      {
        "type": "text",
        "content": "For OpenAI’s Cooper, success would look like an evolution of standards: “I don’t want it to be a stagnant thing. I don’t want these protocols to be part of this foundation, and that’s where they sat for two years. They should evolve and continually accept further input.”"
      },
      {
        "type": "text",
        "content": "There’s also a more subtle consequence: Even with open governance, one company’s implementation could become the default simply because it ships fastest or gains the most usage. Zemlin says that’s not necessarily a bad thing, though. He points to open source history — like Kubernetes “winning” the container race — as evidence that “dominance emerges from merit and not vendor control.”"
      },
      {
        "type": "text",
        "content": "For developers and enterprises, the short-term appeal is clear: less time building custom connectors, more predictable agent behavior across codebases, and simpler deployment in security-conscious environments. The larger vision is more ambitious: If tools like MCP, AGENTS.md, and Goose become standard infrastructure, the agent landscape could shift from closed platforms to an open, mix-and-match software world reminiscent of the interoperable systems that built the modern web."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/09/slack-ceo-denise-dresser-to-join-openai-as-chief-revenue-officer/",
    "site_type": "techcrunch",
    "title": "Slack CEO Denise Dresser to join OpenAI as chief revenue officer",
    "date": "09.12.2025",
    "text": "OpenAI is hiring Slack CEO Denise Dresser as its new chief revenue officer. The news was first reported by Wired , then confirmed by OpenAI in a blog post.\n\nDresser’s new role comes after more than 14 years at Salesforce, Slack’s parent company. While at Slack, Dresser oversaw the introduction of several AI features.\n\nOpenAI says that Dresser will be responsible for the company’s revenue strategy in enterprise and customer success. That’s a pivotal role, given that the company has a rocky road ahead if it ever wants to turn a profit.\n\n“We’re on a path to put AI tools into the hands of millions of workers, across every industry,” Fidji Simo, OpenAI’s CEO of Applications, said in a statement. “Denise has led that kind of shift before, and her experience will help us make AI useful, reliable, and accessible for businesses everywhere.”\n\nLike Dresser, Simo also joined OpenAI this year after a long track record of high-profile leadership, most recently as CEO of Instacart, which has become a close partner of OpenAI.\n\nAccording to Wired, Slack’s chief product officer, Rob Seaman, will become interim CEO of Slack.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2024/11/54103320653_6eb7d509ce_k.jpg?w=1024",
        "alt": "Image Credits:Kimberly White / Getty Images"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2024/11/54103320653_6eb7d509ce_k.jpg?w=1024",
        "alt": "Image Credits:Kimberly White / Getty Images"
      },
      {
        "type": "text",
        "content": "OpenAI is hiring Slack CEO Denise Dresser as its new chief revenue officer. The news was first reported by Wired , then confirmed by OpenAI in a blog post."
      },
      {
        "type": "text",
        "content": "Dresser’s new role comes after more than 14 years at Salesforce, Slack’s parent company. While at Slack, Dresser oversaw the introduction of several AI features."
      },
      {
        "type": "text",
        "content": "OpenAI says that Dresser will be responsible for the company’s revenue strategy in enterprise and customer success. That’s a pivotal role, given that the company has a rocky road ahead if it ever wants to turn a profit."
      },
      {
        "type": "text",
        "content": "“We’re on a path to put AI tools into the hands of millions of workers, across every industry,” Fidji Simo, OpenAI’s CEO of Applications, said in a statement. “Denise has led that kind of shift before, and her experience will help us make AI useful, reliable, and accessible for businesses everywhere.”"
      },
      {
        "type": "text",
        "content": "Like Dresser, Simo also joined OpenAI this year after a long track record of high-profile leadership, most recently as CEO of Instacart, which has become a close partner of OpenAI."
      },
      {
        "type": "text",
        "content": "According to Wired, Slack’s chief product officer, Rob Seaman, will become interim CEO of Slack."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/09/rivian-is-building-its-own-ai-assistant/",
    "site_type": "techcrunch",
    "title": "Rivian is building its own AI assistant",
    "date": "09.12.2025",
    "text": "Rivian has spent nearly two years building its own AI assistant, an effort that remains separate from its multibillion-dollar technology joint venture with Volkswagen, TechCrunch has learned.\n\nRivian hasn’t revealed when it will put the AI assistant in consumers hands. However, in an interview earlier this year, Rivian’s software chief Wassym Bensaid told TechCrunch it was targeting the end of the year. The company will likely share more during its upcoming AI & Autonomy Day , which will be livestreamed starting at 9 a.m. PT December 11.\n\nRivian’s plans are reflective of the moment as the pace of development from foundational AI companies — the tech giants and startups like Anthropic, Google, Microsoft, Meta, and OpenAI that are building the core models and infrastructure — accelerates and industries scramble to keep up.\n\nBut as Bensaid noted to TechCrunch earlier this year, this isn’t some slapdash effort to stay on trend. Nor is it simply a chatbot thrown into the infotainment system. The company has put considerable thought, resources, and time into the product, Bensaid said, noting that it’s designed to be integrated with all vehicle controls.\n\nThe company started with an underlying philosophy to build an overall architecture that is model and platform agnostic, according to Bensaid. The Rivian AI assistant team, which is based out of the company’s Palo Alto office, soon realized effort and attention should also be directed toward developing the software layers that help coordinate various workflows as well as the control logic that resolves conflicts.\n\n“And that’s the in-vehicle platform we have built,” Bensaid said. “We use what the industry loves to now call an agentic framework; but we thought about that architecture since very early so that we can interface with different models.”\n\nThe in-house AI assistant program is consistent with Rivian’s push to become more vertically integrated. In 2024, Rivian overhauled its flagship R1T truck and R1S SUV , changing everything from the battery pack and suspension system to the electrical architecture, sensor stack, and software user interface.\n\nThe company has also put considerable resources toward developing and improving its own software stack, which includes everything related to real-time operating systems (RTOS) that manage the car, such as thermal dynamics, ADAS, and safety systems, as well as another layer related to the infotainment system.\n\nBensaid didn’t provide detailed information about the AI assistant, but he did say it includes a mix of models that handle specific tasks. The result is a hybrid software stack that combines edge AI, where tasks are handled on the device, and cloud AI, in which large models that require more compute are handled by remote servers.\n\nThis should mean a flexible, customized AI assistant that splits the workload between the edge and cloud.\n\nRivian developed much of the AI software stack in-house, including its own custom models and the “orchestration layer,” the conductor or traffic cop of sorts that makes sure the various AI models work together. Rivian tapped other companies for specific agentic AI functions.\n\nThe mission is to develop an AI assistant that increases customer trust and engagement, Bensaid said.\n\nFor now, the AI assistant is staying within Rivian. The company’s joint venture with Volkswagen is focused on software, but not an AI assistant or anything to do with automated driving.\n\nThe technology joint venture with Volkswagen, which was announced in 2024 and is worth up to $5.8 billion , is centered on the underlying electrical architecture and zonal compute, and infotainment. The joint venture officially kicked off in November 2024 and is expected to supply the electrical architecture and software for Volkswagen Group as early as 2027.\n\nAutonomy and AI are separate for now, but “it doesn’t mean that it may not be in the future,” Bensaid said.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/07/250522-ANASTASIA-BENSON-GOOGLE-MAPS-AEB08056-FINAL.jpg?w=1024",
        "alt": "Image Credits:Rivian"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/07/250522-ANASTASIA-BENSON-GOOGLE-MAPS-AEB08056-FINAL.jpg?w=1024",
        "alt": "Image Credits:Rivian"
      },
      {
        "type": "text",
        "content": "Rivian has spent nearly two years building its own AI assistant, an effort that remains separate from its multibillion-dollar technology joint venture with Volkswagen, TechCrunch has learned."
      },
      {
        "type": "text",
        "content": "Rivian hasn’t revealed when it will put the AI assistant in consumers hands. However, in an interview earlier this year, Rivian’s software chief Wassym Bensaid told TechCrunch it was targeting the end of the year. The company will likely share more during its upcoming AI & Autonomy Day , which will be livestreamed starting at 9 a.m. PT December 11."
      },
      {
        "type": "text",
        "content": "Rivian’s plans are reflective of the moment as the pace of development from foundational AI companies — the tech giants and startups like Anthropic, Google, Microsoft, Meta, and OpenAI that are building the core models and infrastructure — accelerates and industries scramble to keep up."
      },
      {
        "type": "text",
        "content": "But as Bensaid noted to TechCrunch earlier this year, this isn’t some slapdash effort to stay on trend. Nor is it simply a chatbot thrown into the infotainment system. The company has put considerable thought, resources, and time into the product, Bensaid said, noting that it’s designed to be integrated with all vehicle controls."
      },
      {
        "type": "text",
        "content": "The company started with an underlying philosophy to build an overall architecture that is model and platform agnostic, according to Bensaid. The Rivian AI assistant team, which is based out of the company’s Palo Alto office, soon realized effort and attention should also be directed toward developing the software layers that help coordinate various workflows as well as the control logic that resolves conflicts."
      },
      {
        "type": "text",
        "content": "“And that’s the in-vehicle platform we have built,” Bensaid said. “We use what the industry loves to now call an agentic framework; but we thought about that architecture since very early so that we can interface with different models.”"
      },
      {
        "type": "text",
        "content": "The in-house AI assistant program is consistent with Rivian’s push to become more vertically integrated. In 2024, Rivian overhauled its flagship R1T truck and R1S SUV , changing everything from the battery pack and suspension system to the electrical architecture, sensor stack, and software user interface."
      },
      {
        "type": "text",
        "content": "The company has also put considerable resources toward developing and improving its own software stack, which includes everything related to real-time operating systems (RTOS) that manage the car, such as thermal dynamics, ADAS, and safety systems, as well as another layer related to the infotainment system."
      },
      {
        "type": "text",
        "content": "Bensaid didn’t provide detailed information about the AI assistant, but he did say it includes a mix of models that handle specific tasks. The result is a hybrid software stack that combines edge AI, where tasks are handled on the device, and cloud AI, in which large models that require more compute are handled by remote servers."
      },
      {
        "type": "text",
        "content": "This should mean a flexible, customized AI assistant that splits the workload between the edge and cloud."
      },
      {
        "type": "text",
        "content": "Rivian developed much of the AI software stack in-house, including its own custom models and the “orchestration layer,” the conductor or traffic cop of sorts that makes sure the various AI models work together. Rivian tapped other companies for specific agentic AI functions."
      },
      {
        "type": "text",
        "content": "The mission is to develop an AI assistant that increases customer trust and engagement, Bensaid said."
      },
      {
        "type": "text",
        "content": "For now, the AI assistant is staying within Rivian. The company’s joint venture with Volkswagen is focused on software, but not an AI assistant or anything to do with automated driving."
      },
      {
        "type": "text",
        "content": "The technology joint venture with Volkswagen, which was announced in 2024 and is worth up to $5.8 billion , is centered on the underlying electrical architecture and zonal compute, and infotainment. The joint venture officially kicked off in November 2024 and is expected to supply the electrical architecture and software for Volkswagen Group as early as 2027."
      },
      {
        "type": "text",
        "content": "Autonomy and AI are separate for now, but “it doesn’t mean that it may not be in the future,” Bensaid said."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/09/hinge-ceo-steps-down-to-launch-overtone-an-ai-dating-app/",
    "site_type": "techcrunch",
    "title": "Hinge CEO steps down to launch Overtone, an AI dating app",
    "date": "09.12.2025",
    "text": "Hinge CEO Justin McLeod is stepping down from his role to launch a new AI dating product called Overtone.\n\nMatch Group, the dating giant that owns apps like Hinge, Tinder, and OkCupid, is backing Overtone with pre-seed financing and plans to take a “substantial ownership position,” according to a press release.\n\nWith Match’s support, Overtone was incubated as a project inside of Hinge. McLeod and a dedicated team spent the year developing the idea of Overtone, which is described as “an early-stage dating service focused on using AI and voice tools to help people connect in a more thoughtful and personal way.”\n\nMcLeod isn’t the only dating app founder branching out into new, standalone AI experiences. Whitney Wolfe Herd, founder of Bumble, said she wants to use AI to make “the world’s smartest and most emotionally intelligent matchmaker in existence.” Somewhat infamously, Wolfe Herd proposed the idea last year of singles using AI to stand-in for themselves and date other people’s AIs.\n\nIt’s not yet clear how Overtone will differentiate itself from other dating apps, which have been experimenting with AI features to compensate for the market’s growing malaise with online dating — especially among Gen Z .\n\nTinder has reported nine straight quarters of paying-subscriber declines and has leaned into AI with features that are supposed to help users get more matches. Hinge launched another AI feature just this week called “Convo Starters,” which is supposed to help daters come up with more interesting things to say than the usual small talk. Tinder and Facebook Dating have each experimented with AI-powered matching to combat “swipe fatigue.”\n\nCeding control of your dating experience is one thing, but other attempts at integrating AI into these apps get even more dubious.\n\nMatch CEO Spencer Rascoff said last month that a “major pillar of Tinder’s upcoming 2026 product experience” will be a feature called Chemistry. With the user’s permission, the feature will access users’ camera rolls to learn more about them. (For the record, we would advise that you do not give tech companies unfettered access to even more of your data.)\n\nMcLeod founded Hinge in 2011 as a dating app with a greater focus on building relationships than facilitating casual dates. The app, which is on track to hit $1 billion in revenue by 2027, was acquired by Match in 2019. Jackie Jantos, Hinge’s president and chief marketing officer, will take over as CEO. McLeod will remain in an advisory position at Hinge through March.\n\nThis summer, TechCrunch spoke to Jantos at SXSW London about how Hinge will address Gen Z, a market that’s growing increasingly disillusioned with meeting people online.\n\n“This is a generation that has grown up with a deep understanding of how digital experiences are created and what they are trying to get out of them,” Jantos told TechCrunch.\n\nGen Z wants transparency and authenticity from digital brands, according to Jantos. While some might see this as inherently incompatible with the company’s growing reliance on AI, Hinge’s AI recommendation feature, launched in March, drove a 15% increase in matches and contact exchanges in the first quarter of this year.\n\nBased on Jantos’ comments upon assuming her new role, it seems that Hinge will continue investing in these features under her leadership.\n\n“Our focus will remain on intentional innovation that is grounded in culture, creativity, and a deep understanding of how people connect today,” Jantos said in a statement.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2023/06/robots-love.jpg?w=1024",
        "alt": "Image Credits:Kilito Chan / Getty Images"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2023/06/robots-love.jpg?w=1024",
        "alt": "Image Credits:Kilito Chan / Getty Images"
      },
      {
        "type": "text",
        "content": "Hinge CEO Justin McLeod is stepping down from his role to launch a new AI dating product called Overtone."
      },
      {
        "type": "text",
        "content": "Match Group, the dating giant that owns apps like Hinge, Tinder, and OkCupid, is backing Overtone with pre-seed financing and plans to take a “substantial ownership position,” according to a press release."
      },
      {
        "type": "text",
        "content": "With Match’s support, Overtone was incubated as a project inside of Hinge. McLeod and a dedicated team spent the year developing the idea of Overtone, which is described as “an early-stage dating service focused on using AI and voice tools to help people connect in a more thoughtful and personal way.”"
      },
      {
        "type": "text",
        "content": "McLeod isn’t the only dating app founder branching out into new, standalone AI experiences. Whitney Wolfe Herd, founder of Bumble, said she wants to use AI to make “the world’s smartest and most emotionally intelligent matchmaker in existence.” Somewhat infamously, Wolfe Herd proposed the idea last year of singles using AI to stand-in for themselves and date other people’s AIs."
      },
      {
        "type": "text",
        "content": "It’s not yet clear how Overtone will differentiate itself from other dating apps, which have been experimenting with AI features to compensate for the market’s growing malaise with online dating — especially among Gen Z ."
      },
      {
        "type": "text",
        "content": "Tinder has reported nine straight quarters of paying-subscriber declines and has leaned into AI with features that are supposed to help users get more matches. Hinge launched another AI feature just this week called “Convo Starters,” which is supposed to help daters come up with more interesting things to say than the usual small talk. Tinder and Facebook Dating have each experimented with AI-powered matching to combat “swipe fatigue.”"
      },
      {
        "type": "text",
        "content": "Ceding control of your dating experience is one thing, but other attempts at integrating AI into these apps get even more dubious."
      },
      {
        "type": "text",
        "content": "Match CEO Spencer Rascoff said last month that a “major pillar of Tinder’s upcoming 2026 product experience” will be a feature called Chemistry. With the user’s permission, the feature will access users’ camera rolls to learn more about them. (For the record, we would advise that you do not give tech companies unfettered access to even more of your data.)"
      },
      {
        "type": "text",
        "content": "McLeod founded Hinge in 2011 as a dating app with a greater focus on building relationships than facilitating casual dates. The app, which is on track to hit $1 billion in revenue by 2027, was acquired by Match in 2019. Jackie Jantos, Hinge’s president and chief marketing officer, will take over as CEO. McLeod will remain in an advisory position at Hinge through March."
      },
      {
        "type": "text",
        "content": "This summer, TechCrunch spoke to Jantos at SXSW London about how Hinge will address Gen Z, a market that’s growing increasingly disillusioned with meeting people online."
      },
      {
        "type": "text",
        "content": "“This is a generation that has grown up with a deep understanding of how digital experiences are created and what they are trying to get out of them,” Jantos told TechCrunch."
      },
      {
        "type": "text",
        "content": "Gen Z wants transparency and authenticity from digital brands, according to Jantos. While some might see this as inherently incompatible with the company’s growing reliance on AI, Hinge’s AI recommendation feature, launched in March, drove a 15% increase in matches and contact exchanges in the first quarter of this year."
      },
      {
        "type": "text",
        "content": "Based on Jantos’ comments upon assuming her new role, it seems that Hinge will continue investing in these features under her leadership."
      },
      {
        "type": "text",
        "content": "“Our focus will remain on intentional innovation that is grounded in culture, creativity, and a deep understanding of how people connect today,” Jantos said in a statement."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/09/unconventional-ai-confirms-its-massive-475m-seed-round/",
    "site_type": "techcrunch",
    "title": "Unconventional AI confirms its massive  $475M seed round",
    "date": "09.12.2025",
    "text": "Naveen Rao, the former head of AI at Databricks, has raised $475 million in seed capital at a $4.5 billion valuation for his new startup, Unconventional AI .\n\nThe round was led by Andreessen Horowitz and Lightspeed Ventures, with participation from Lux Capital and DCVC. The funding is a first installment toward the goal of up to $1 billion for the round, Rao told Bloomberg .\n\nTechCrunch was first to report, back in October, that Unconventional AI was seeking this mega funding for Rao’s new startup, although the final valuation is marginally lower than the $5 billion sources told us he was seeking. If he does eventually raise as much as $1 billion, we’ll see how that impacts his company’s value.\n\nUnconventional AI is set on building a new, energy-efficient computer for AI. Rao previously wrote on X that his goal is to create a computer that is “as efficient as biology.”\n\nDatabricks acquired Rao’s previous startup, MosaicML in 2023 , for $1.3 billion. Prior to MosaicML, Rao co-founded the machine learning platform Nervana Systems , which Intel Corp. acquired in 2016 for reportedly more than $400 million.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2024/04/Naveen-Rao-headshot.png?w=1024",
        "alt": "Image Credits:Naveen Rao"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2024/04/Naveen-Rao-headshot.png?w=1024",
        "alt": "Image Credits:Naveen Rao"
      },
      {
        "type": "text",
        "content": "Naveen Rao, the former head of AI at Databricks, has raised $475 million in seed capital at a $4.5 billion valuation for his new startup, Unconventional AI ."
      },
      {
        "type": "text",
        "content": "The round was led by Andreessen Horowitz and Lightspeed Ventures, with participation from Lux Capital and DCVC. The funding is a first installment toward the goal of up to $1 billion for the round, Rao told Bloomberg ."
      },
      {
        "type": "text",
        "content": "TechCrunch was first to report, back in October, that Unconventional AI was seeking this mega funding for Rao’s new startup, although the final valuation is marginally lower than the $5 billion sources told us he was seeking. If he does eventually raise as much as $1 billion, we’ll see how that impacts his company’s value."
      },
      {
        "type": "text",
        "content": "Unconventional AI is set on building a new, energy-efficient computer for AI. Rao previously wrote on X that his goal is to create a computer that is “as efficient as biology.”"
      },
      {
        "type": "text",
        "content": "Databricks acquired Rao’s previous startup, MosaicML in 2023 , for $1.3 billion. Prior to MosaicML, Rao co-founded the machine learning platform Nervana Systems , which Intel Corp. acquired in 2016 for reportedly more than $400 million."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/10/fertility-startup-inito-wants-to-use-ai-designed-antibodies-to-expand-at-home-health-tests/",
    "site_type": "techcrunch",
    "title": "Fertility startup Inito wants to use AI-designed antibodies to expand at-home health tests",
    "date": "10.12.2025",
    "text": "Fertility startup Inito has raised $29 million in Series B funding to scale its at-home health diagnostics platform and use AI-designed antibodies to unlock new kinds of at-home tests.\n\nWhen the company launched its first fertility monitor in 2021, its goal was to make quantitative fertility hormone testing possible at home.\n\nWhile standard at-home ovulation tests predict fertile days by tracking estrogen and luteinizing hormone (LH), they don’t measure the hormone that confirms your ovulation, which is progesterone metabolite PdG. Inito allows women to measure estrogen, LH, progesterone (PdG), and FSH (follicle-stimulating hormone) on a single test strip. Inito’s AI models interpret these levels to reveal fertility hormone patterns, track fertile days, and confirm ovulation.\n\nFast-forward to today, the startup has become a popular option for women looking to track fertility hormones, analyzing more than 30 million fertility hormone data points since 2021.\n\nThe startup is now doubling down on its belief that healthcare should start at home and is working to evolve Inito from a fertility tracker into a broader hormone and at-home health diagnostics platform.\n\nInito is using the new funding to invest in and develop AI-engineered antibodies, which it says will allow for the development of new types of tests and improve the accuracy of existing ones.\n\nFor context, when you take a test at a lab or a home, antibodies are what bind to the target molecule (such as estrogen or testosterone) to produce a signal. Traditional antibodies are grown inside animals and then screened manually in the lab, which can be a slow and expensive process. Conventional antibodies also lack the sensitivity to make at-home tests for a large number of biomarkers.\n\nInito says AI-designed antibodies change the game. Instead of “raising” antibodies in animals, they can be treated almost like software.\n\n“We predict how proteins fold in 3D, design synthetic antibodies using AI, and test millions of variants virtually before making a single one in the lab,” explained Inito co-founder and CTO Varun Venkatesan in an email. “This produces antibodies that are far more sensitive, consistent, and stable than anything developed through traditional methods.”\n\nFor Inito, this will unlock the next generation of at-home tests. Venkatesan says the company is already using these methods in R&D and that its wet lab is showing promising results.\n\n“The real vision is bigger than adding new tests,” said Inito co-founder and CEO Aayush Rai. “The endgame is to redefine diagnostics altogether. If you want to understand what’s happening inside your body at every life stage and health need, you shouldn’t be limited by clinic appointments, lab schedules, or rigid testing systems. You should be able to measure, track, and get insights about your body from home, with lab-grade confidence.”\n\nInito wants to expand its vision beyond a “trying to conceive” platform. Its reader and app will soon power tests for pregnancy, menopause, and broader at-home health monitoring.\n\n“Our long-term roadmap goes far beyond fertility,” said Rai. “We’re building toward a platform that can measure and interpret the full spectrum of hormones that shape health across a lifetime. Pregnancy progression, menopause, and broader endocrine markers like testosterone — all powered by AI antibodies and imaging that sets Inito apart today.”\n\nThe company is going to use part of the funding to scale manufacturing and grow globally to meet growing demand across the United States and new international markets.\n\nInito’s latest funding round was led by Bertelsmann India Investments and Fireside Ventures. The investment brings Inito’s total funding to around $45 million. The company previously raised $6 million in funding led by Fireside Ventures and $9 million from Y Combinator, former Nurx CEO Varsha Rao, and a dozen physicians and family offices.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/inito.png?w=1024",
        "alt": "Image Credits:Inito"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/2024_10_03_Inito_2461-Edit-2.jpg?w=680",
        "alt": "Image Credits:Inito"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/DSC00143-1.jpg?w=680",
        "alt": "Image Credits:Inito"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/inito.png?w=1024",
        "alt": "Image Credits:Inito"
      },
      {
        "type": "text",
        "content": "Fertility startup Inito has raised $29 million in Series B funding to scale its at-home health diagnostics platform and use AI-designed antibodies to unlock new kinds of at-home tests."
      },
      {
        "type": "text",
        "content": "When the company launched its first fertility monitor in 2021, its goal was to make quantitative fertility hormone testing possible at home."
      },
      {
        "type": "text",
        "content": "While standard at-home ovulation tests predict fertile days by tracking estrogen and luteinizing hormone (LH), they don’t measure the hormone that confirms your ovulation, which is progesterone metabolite PdG. Inito allows women to measure estrogen, LH, progesterone (PdG), and FSH (follicle-stimulating hormone) on a single test strip. Inito’s AI models interpret these levels to reveal fertility hormone patterns, track fertile days, and confirm ovulation."
      },
      {
        "type": "text",
        "content": "Fast-forward to today, the startup has become a popular option for women looking to track fertility hormones, analyzing more than 30 million fertility hormone data points since 2021."
      },
      {
        "type": "text",
        "content": "The startup is now doubling down on its belief that healthcare should start at home and is working to evolve Inito from a fertility tracker into a broader hormone and at-home health diagnostics platform."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/2024_10_03_Inito_2461-Edit-2.jpg?w=680",
        "alt": "Image Credits:Inito"
      },
      {
        "type": "text",
        "content": "Inito is using the new funding to invest in and develop AI-engineered antibodies, which it says will allow for the development of new types of tests and improve the accuracy of existing ones."
      },
      {
        "type": "text",
        "content": "For context, when you take a test at a lab or a home, antibodies are what bind to the target molecule (such as estrogen or testosterone) to produce a signal. Traditional antibodies are grown inside animals and then screened manually in the lab, which can be a slow and expensive process. Conventional antibodies also lack the sensitivity to make at-home tests for a large number of biomarkers."
      },
      {
        "type": "text",
        "content": "Inito says AI-designed antibodies change the game. Instead of “raising” antibodies in animals, they can be treated almost like software."
      },
      {
        "type": "text",
        "content": "“We predict how proteins fold in 3D, design synthetic antibodies using AI, and test millions of variants virtually before making a single one in the lab,” explained Inito co-founder and CTO Varun Venkatesan in an email. “This produces antibodies that are far more sensitive, consistent, and stable than anything developed through traditional methods.”"
      },
      {
        "type": "text",
        "content": "For Inito, this will unlock the next generation of at-home tests. Venkatesan says the company is already using these methods in R&D and that its wet lab is showing promising results."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/DSC00143-1.jpg?w=680",
        "alt": "Image Credits:Inito"
      },
      {
        "type": "text",
        "content": "“The real vision is bigger than adding new tests,” said Inito co-founder and CEO Aayush Rai. “The endgame is to redefine diagnostics altogether. If you want to understand what’s happening inside your body at every life stage and health need, you shouldn’t be limited by clinic appointments, lab schedules, or rigid testing systems. You should be able to measure, track, and get insights about your body from home, with lab-grade confidence.”"
      },
      {
        "type": "text",
        "content": "Inito wants to expand its vision beyond a “trying to conceive” platform. Its reader and app will soon power tests for pregnancy, menopause, and broader at-home health monitoring."
      },
      {
        "type": "text",
        "content": "“Our long-term roadmap goes far beyond fertility,” said Rai. “We’re building toward a platform that can measure and interpret the full spectrum of hormones that shape health across a lifetime. Pregnancy progression, menopause, and broader endocrine markers like testosterone — all powered by AI antibodies and imaging that sets Inito apart today.”"
      },
      {
        "type": "text",
        "content": "The company is going to use part of the funding to scale manufacturing and grow globally to meet growing demand across the United States and new international markets."
      },
      {
        "type": "text",
        "content": "Inito’s latest funding round was led by Bertelsmann India Investments and Fireside Ventures. The investment brings Inito’s total funding to around $45 million. The company previously raised $6 million in funding led by Fireside Ventures and $9 million from Y Combinator, former Nurx CEO Varsha Rao, and a dozen physicians and family offices."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/10/adobe-brings-photoshop-express-and-acrobat-features-to-chatgpt/",
    "site_type": "techcrunch",
    "title": "Adobe brings Photoshop, Express, and Acrobat features to ChatGPT",
    "date": "10.12.2025",
    "text": "Adobe is doubling down on its strategy of using AI to bring more users into its product ecosystem. The company on Wednesday said it is adding features from Photoshop, Express, and Acrobat to ChatGPT, letting users ask the chatbot to use these apps to edit images, modify PDFs, or animate elements.\n\nSo users can now tell ChatGPT to use Photoshop to edit specific parts of images; remove or blur the background; adjust exposure, brightness, and contrast; or apply various effects. The chatbot will also give you an option to modify the intensity of effects using sliders.\n\nWith support for Express, ChatGPT now lets users tell it to do stuff like fetch existing designs from its library, put together themed creatives, animate elements, and edit designs.\n\nAnd with Acrobat’s PDF editing capabilities, ChatGPT can do things like merge files, edit or extract text or tables within a file, and more.\n\nIf at any point you don’t feel like struggling with ChatGPT, you get an option to continue working inside Adobe’s apps to finish the job yourself or use features that are not available within ChatGPT.\n\nAdobe said these features will be available globally. While features from all three Adobe apps are available on ChatGPT’s desktop, web, and iOS apps, only Adobe Express is currently supported on ChatGPT for Android. The company says support for Photoshop and Express is coming soon.\n\nAdobe has shipped many AI-powered features and products this year. In October, the company released AI assistants for Express and Photoshop , and also teased a cross-app assistant, dubbed Project Moonlight.\n\nOpenAI started supporting third-party apps inside ChatGPT in October, launching with Canva, Spotify, Expedia, and Figma. But as more companies work to bake in their apps within the popular chatbot, the challenge for companies will be getting people to use their app instead of a rival’s inside ChatGPT. For instance, you can use both Canva and Photoshop to edit images with the chatbot, but if you don’t use either of these apps, you might not have a preference.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/PS_Make-Colors-More-Vibrant_16x9.jpeg?w=1024",
        "alt": "Image Credits:Adobe"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/PS_Selective_16x9.jpeg?w=680",
        "alt": "Image Credits:Adobe"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/AC_Combine-Files_16x9.jpeg?w=680",
        "alt": "Image Credits:Adobe"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/PS_Make-Colors-More-Vibrant_16x9.jpeg?w=1024",
        "alt": "Image Credits:Adobe"
      },
      {
        "type": "text",
        "content": "Adobe is doubling down on its strategy of using AI to bring more users into its product ecosystem. The company on Wednesday said it is adding features from Photoshop, Express, and Acrobat to ChatGPT, letting users ask the chatbot to use these apps to edit images, modify PDFs, or animate elements."
      },
      {
        "type": "text",
        "content": "So users can now tell ChatGPT to use Photoshop to edit specific parts of images; remove or blur the background; adjust exposure, brightness, and contrast; or apply various effects. The chatbot will also give you an option to modify the intensity of effects using sliders."
      },
      {
        "type": "text",
        "content": "With support for Express, ChatGPT now lets users tell it to do stuff like fetch existing designs from its library, put together themed creatives, animate elements, and edit designs."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/PS_Selective_16x9.jpeg?w=680",
        "alt": "Image Credits:Adobe"
      },
      {
        "type": "text",
        "content": "And with Acrobat’s PDF editing capabilities, ChatGPT can do things like merge files, edit or extract text or tables within a file, and more."
      },
      {
        "type": "text",
        "content": "If at any point you don’t feel like struggling with ChatGPT, you get an option to continue working inside Adobe’s apps to finish the job yourself or use features that are not available within ChatGPT."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/AC_Combine-Files_16x9.jpeg?w=680",
        "alt": "Image Credits:Adobe"
      },
      {
        "type": "text",
        "content": "Adobe said these features will be available globally. While features from all three Adobe apps are available on ChatGPT’s desktop, web, and iOS apps, only Adobe Express is currently supported on ChatGPT for Android. The company says support for Photoshop and Express is coming soon."
      },
      {
        "type": "text",
        "content": "Adobe has shipped many AI-powered features and products this year. In October, the company released AI assistants for Express and Photoshop , and also teased a cross-app assistant, dubbed Project Moonlight."
      },
      {
        "type": "text",
        "content": "OpenAI started supporting third-party apps inside ChatGPT in October, launching with Canva, Spotify, Expedia, and Figma. But as more companies work to bake in their apps within the popular chatbot, the challenge for companies will be getting people to use their app instead of a rival’s inside ChatGPT. For instance, you can use both Canva and Photoshop to edit images with the chatbot, but if you don’t use either of these apps, you might not have a preference."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/10/google-launches-sub-5-ai-plus-plan-in-india-to-compete-with-chatgpt-go/",
    "site_type": "techcrunch",
    "title": "Google launches sub-$5 AI Plus plan in India to compete with ChatGPT Go",
    "date": "10.12.2025",
    "text": "Google on Wednesday launched its more affordable AI Plus plan in India as it seeks to compete on pricing with other low-cost AI offerings, like OpenAI’s ChatGPT Go subscription.\n\nFor new users in India, Google is offering AI Plus for ₹199 ($2.21) per month, for the first six months, after which you’ll have to pay ₹399 ($4.44) a month.\n\nThe AI Plus plan gives users higher limits for Gemini 3 Pro and image editing model Nano Banana Pro; access to video generation in the Gemini and Flow apps; expanded access to deep research in NotebookLM; and 200GB of storage across Photos, Drive, and Gmail. Subscribers can give access to up to five family members, too.\n\nBefore this, Google’s cheapest AI subscription available in the country was AI Pro, which was priced at ₹1,950 per month ($21.69).\n\nBut Google is late to customize its pricing for India, as the AI Plus plan was first introduced in Indonesia in September and offered it in other countries that same month . OpenAI debuted its sub-$5 ChatGPT Go plan in India in August and made it available in other countries in subsequent months. The plan allows users to have 10 times more limits than the free plan for messages, image generation, and file uploads.\n\nAI companies have been busy offering freebies in India as they seek to tap its huge population for users. First, Perplexity partnered with telco Airtel to offer its Pro plan for a year to all Airtel customers for free. OpenAI also offered ChatGPT Go free for a year to existing subscribers of Go and new users.\n\nMeanwhile, Google teamed up with telco Reliance Jio to offer its AI Pro plan for 18 months at no cost to users on a certain carrier plan.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/10/gemini.jpg?w=1024",
        "alt": "Image Credits:Jagmeet Singh / TechCrunch"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/10/gemini.jpg?w=1024",
        "alt": "Image Credits:Jagmeet Singh / TechCrunch"
      },
      {
        "type": "text",
        "content": "Google on Wednesday launched its more affordable AI Plus plan in India as it seeks to compete on pricing with other low-cost AI offerings, like OpenAI’s ChatGPT Go subscription."
      },
      {
        "type": "text",
        "content": "For new users in India, Google is offering AI Plus for ₹199 ($2.21) per month, for the first six months, after which you’ll have to pay ₹399 ($4.44) a month."
      },
      {
        "type": "text",
        "content": "The AI Plus plan gives users higher limits for Gemini 3 Pro and image editing model Nano Banana Pro; access to video generation in the Gemini and Flow apps; expanded access to deep research in NotebookLM; and 200GB of storage across Photos, Drive, and Gmail. Subscribers can give access to up to five family members, too."
      },
      {
        "type": "text",
        "content": "Before this, Google’s cheapest AI subscription available in the country was AI Pro, which was priced at ₹1,950 per month ($21.69)."
      },
      {
        "type": "text",
        "content": "But Google is late to customize its pricing for India, as the AI Plus plan was first introduced in Indonesia in September and offered it in other countries that same month . OpenAI debuted its sub-$5 ChatGPT Go plan in India in August and made it available in other countries in subsequent months. The plan allows users to have 10 times more limits than the free plan for messages, image generation, and file uploads."
      },
      {
        "type": "text",
        "content": "AI companies have been busy offering freebies in India as they seek to tap its huge population for users. First, Perplexity partnered with telco Airtel to offer its Pro plan for a year to all Airtel customers for free. OpenAI also offered ChatGPT Go free for a year to existing subscribers of Go and new users."
      },
      {
        "type": "text",
        "content": "Meanwhile, Google teamed up with telco Reliance Jio to offer its AI Pro plan for 18 months at no cost to users on a certain carrier plan."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/10/figma-launches-new-ai-powered-object-removal-and-image-extension/",
    "site_type": "techcrunch",
    "title": "Figma launches new AI-powered object removal and image extension",
    "date": "10.12.2025",
    "text": "Design tool Figma launched new AI-powered image-editing features today, including the ability to remove and isolate objects and expand images.\n\nThe company said that these features will save the hassle of exporting images to other tools for editing and importing them back. It added that generation models like Nano Banana are good for creating images, but users often need granular tools for editing that work without any text prompts.\n\nFigma has improved its lasso tool for selection. Now you can use it to select an object, remove it, or isolate it to move it around. When you move around the object, the image still retains other characteristics, such as background and color. Users can also select an object to adjust factors like lighting, shadow, color, or focus.\n\nThe company is also bringing an image-expansion feature to its design suite. This feature is handy when you are adjusting a creative for a particular format and need to fill in the background or other details. For instance, creating a web banner or mobile banner from a 1×1 image. It essentially saves you from constantly cropping an image and adjusting elements within it.\n\nBesides adding these features, Figma is collating all its image-editing tools in one toolbar for easier access. You can select objects or parts of images, change background color, and add annotations or text using this toolbar. The company said removing background is one of the most common actions on the platform, and that is why it is getting a prominent spot on the new toolbar.\n\nRivals like Adobe and Canva have had object removal for a few years now. Figma is finally catching up to offer these features.\n\nThe company said the new image-editing features are available on Figma Design and Draw, with plans to make them available across Figma tools next year.\n\nFigma’s launch comes on the same day as Adobe making some of these features available to users within ChatGPT . Figma was one of the launch partners of the app, calling on ChatGPT in October. It is not clear if these new functions will be available to users using Figma within OpenAI’s tool.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/AI-Image-toolbar.jpeg?w=1024",
        "alt": "Image Credits:Figma"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/ezgif-8bda4c8d57256f21.gif?w=680",
        "alt": "Image Credits:Figma"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/AI-Image-toolbar.jpeg?w=1024",
        "alt": "Image Credits:Figma"
      },
      {
        "type": "text",
        "content": "Design tool Figma launched new AI-powered image-editing features today, including the ability to remove and isolate objects and expand images."
      },
      {
        "type": "text",
        "content": "The company said that these features will save the hassle of exporting images to other tools for editing and importing them back. It added that generation models like Nano Banana are good for creating images, but users often need granular tools for editing that work without any text prompts."
      },
      {
        "type": "text",
        "content": "Figma has improved its lasso tool for selection. Now you can use it to select an object, remove it, or isolate it to move it around. When you move around the object, the image still retains other characteristics, such as background and color. Users can also select an object to adjust factors like lighting, shadow, color, or focus."
      },
      {
        "type": "text",
        "content": "The company is also bringing an image-expansion feature to its design suite. This feature is handy when you are adjusting a creative for a particular format and need to fill in the background or other details. For instance, creating a web banner or mobile banner from a 1×1 image. It essentially saves you from constantly cropping an image and adjusting elements within it."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/ezgif-8bda4c8d57256f21.gif?w=680",
        "alt": "Image Credits:Figma"
      },
      {
        "type": "text",
        "content": "Besides adding these features, Figma is collating all its image-editing tools in one toolbar for easier access. You can select objects or parts of images, change background color, and add annotations or text using this toolbar. The company said removing background is one of the most common actions on the platform, and that is why it is getting a prominent spot on the new toolbar."
      },
      {
        "type": "text",
        "content": "Rivals like Adobe and Canva have had object removal for a few years now. Figma is finally catching up to offer these features."
      },
      {
        "type": "text",
        "content": "The company said the new image-editing features are available on Figma Design and Draw, with plans to make them available across Figma tools next year."
      },
      {
        "type": "text",
        "content": "Figma’s launch comes on the same day as Adobe making some of these features available to users within ChatGPT . Figma was one of the launch partners of the app, calling on ChatGPT in October. It is not clear if these new functions will be available to users using Figma within OpenAI’s tool."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/10/oboe-raises-16-million-from-a16z-for-its-ai-powered-course-generation-platform/",
    "site_type": "techcrunch",
    "title": "Oboe raises $16 million from a16z for its AI-powered course-generation platform",
    "date": "10.12.2025",
    "text": "Oboe , a learning startup from Anchor co-founders and former Spotify execs Nir Zicherman and Michael Mignano, said today it has raised $16 million in Series A funding led by a16z, with participation from existing investors Eniac, Haystack, Offline, and Factorial. The round also saw investments from individuals such as Adam D’Angelo, Garry Tan, Lenny Rachitsky, Mati Staniszewski, Mikey Shulman, Jared Hecht, and M.G. Siegler.\n\nThat platform, which officially launched in September , allows users to define a learning goal and use AI to create a course for them.\n\nThe startup’s fundraise comes three months after the app’s launch and a year after it raised a $4 million seed round . Zicherman said the reason behind the fresh round is to fuel growth at scale.\n\n“We want to reach billions of people who want to learn about new topics, and it is a very big opportunity,” he told TechCrunch over a call. “We need to execute faster and reach a larger audience at scale to achieve this vision.”\n\n“We have a team of successful consumer product builders that can execute well, which gives us an edge over other startups in the market,” he added.\n\nBryan Kim, a partner at a16z, said that he was impressed by how fast Oboe started generating content on a topic and didn’t make users wait behind a loading indicator.\n\n“We have had a thesis around how AI-aided learning can help people explore new topics and have been looking for the right company. After Oboe launched, we tried it and loved the product. We wanted to back a founder who was ambitious, flexible in adopting different form factors, and understood AI to build a big platform. We found that in Oboe,” he told TechCrunch over a call.\n\nThe company is making changes to its core course-generation experience, as well. Earlier, Oboe generated different text and audio formats for users in different styles. Plus, it capped course generation based on the payment plan.\n\nWith the new version, the app will first understand your goal and then generate chapters based on that to help you learn about those topics. What’s more, users will see other modalities like quizzes appear within the course material seamlessly. For some courses, Oboe will also generate flashcards for you to easily remember course materials.\n\nIn terms of audio, instead of having to choose between a podcast format and a lecture format, the company generates a podcast for you and changes its tone based on the learning material and other signals from users.\n\nZicherman said that the company observed that there is a high demand among users to learn about STEM topics (Science, Technology, Engineering, and Math). The startup has worked on sourcing the best material for these topics, including programming.\n\nHe said that good teachers decide what is the best way of learning for students and the company is taking that approach of designing courses for learners.\n\nOboe is revamping its pricing model to give unlimited course generation to users.\n\nHowever, if they want to dive deeper into a topic, they can pay $15 a month ($144 per year) to access more course chapters. There is also a $40 per month ($384 per year) Pro plan, which gives unlimited chapter access and allows users to export or download courses for consumption outside Oboe. The startup said this is suitable for students who want to take printouts of the study material and consume it offline.\n\nZicherman said that right now Oboe is offering courses in English, but it wants to reach different parts of the world better with localized courses and language support. The platform is currently available on the web, with mobile support planned for the future.\n\nIn the last few years, multiple tools, including Google’s NotebookLM and ex-Google employees’ Huxe , allow you to enter a prompt to get a podcast episode to explore a topic. While these are one-off generations, Oboe’s approach is to let you dive deeper into the topic with chapter-based learning.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Blog2.0-Header2.jpeg?w=1024",
        "alt": "Image Credits:Oboe"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Blog2.0-ChapterList.jpeg?w=680",
        "alt": "Image Credits:Oboe"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Blog2.0-Quiz.jpeg?w=680",
        "alt": "Image Credits:Oboe"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Blog2.0-Asymptote.jpeg?w=680",
        "alt": "Image Credits:Oboe"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Blog2.0-Header2.jpeg?w=1024",
        "alt": "Image Credits:Oboe"
      },
      {
        "type": "text",
        "content": "Oboe , a learning startup from Anchor co-founders and former Spotify execs Nir Zicherman and Michael Mignano, said today it has raised $16 million in Series A funding led by a16z, with participation from existing investors Eniac, Haystack, Offline, and Factorial. The round also saw investments from individuals such as Adam D’Angelo, Garry Tan, Lenny Rachitsky, Mati Staniszewski, Mikey Shulman, Jared Hecht, and M.G. Siegler."
      },
      {
        "type": "text",
        "content": "That platform, which officially launched in September , allows users to define a learning goal and use AI to create a course for them."
      },
      {
        "type": "text",
        "content": "The startup’s fundraise comes three months after the app’s launch and a year after it raised a $4 million seed round . Zicherman said the reason behind the fresh round is to fuel growth at scale."
      },
      {
        "type": "text",
        "content": "“We want to reach billions of people who want to learn about new topics, and it is a very big opportunity,” he told TechCrunch over a call. “We need to execute faster and reach a larger audience at scale to achieve this vision.”"
      },
      {
        "type": "text",
        "content": "“We have a team of successful consumer product builders that can execute well, which gives us an edge over other startups in the market,” he added."
      },
      {
        "type": "text",
        "content": "Bryan Kim, a partner at a16z, said that he was impressed by how fast Oboe started generating content on a topic and didn’t make users wait behind a loading indicator."
      },
      {
        "type": "text",
        "content": "“We have had a thesis around how AI-aided learning can help people explore new topics and have been looking for the right company. After Oboe launched, we tried it and loved the product. We wanted to back a founder who was ambitious, flexible in adopting different form factors, and understood AI to build a big platform. We found that in Oboe,” he told TechCrunch over a call."
      },
      {
        "type": "text",
        "content": "The company is making changes to its core course-generation experience, as well. Earlier, Oboe generated different text and audio formats for users in different styles. Plus, it capped course generation based on the payment plan."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Blog2.0-ChapterList.jpeg?w=680",
        "alt": "Image Credits:Oboe"
      },
      {
        "type": "text",
        "content": "With the new version, the app will first understand your goal and then generate chapters based on that to help you learn about those topics. What’s more, users will see other modalities like quizzes appear within the course material seamlessly. For some courses, Oboe will also generate flashcards for you to easily remember course materials."
      },
      {
        "type": "text",
        "content": "In terms of audio, instead of having to choose between a podcast format and a lecture format, the company generates a podcast for you and changes its tone based on the learning material and other signals from users."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Blog2.0-Quiz.jpeg?w=680",
        "alt": "Image Credits:Oboe"
      },
      {
        "type": "text",
        "content": "Zicherman said that the company observed that there is a high demand among users to learn about STEM topics (Science, Technology, Engineering, and Math). The startup has worked on sourcing the best material for these topics, including programming."
      },
      {
        "type": "text",
        "content": "He said that good teachers decide what is the best way of learning for students and the company is taking that approach of designing courses for learners."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Blog2.0-Asymptote.jpeg?w=680",
        "alt": "Image Credits:Oboe"
      },
      {
        "type": "text",
        "content": "Oboe is revamping its pricing model to give unlimited course generation to users."
      },
      {
        "type": "text",
        "content": "However, if they want to dive deeper into a topic, they can pay $15 a month ($144 per year) to access more course chapters. There is also a $40 per month ($384 per year) Pro plan, which gives unlimited chapter access and allows users to export or download courses for consumption outside Oboe. The startup said this is suitable for students who want to take printouts of the study material and consume it offline."
      },
      {
        "type": "text",
        "content": "Zicherman said that right now Oboe is offering courses in English, but it wants to reach different parts of the world better with localized courses and language support. The platform is currently available on the web, with mobile support planned for the future."
      },
      {
        "type": "text",
        "content": "In the last few years, multiple tools, including Google’s NotebookLM and ex-Google employees’ Huxe , allow you to enter a prompt to get a podcast episode to explore a topic. While these are one-off generations, Oboe’s approach is to let you dive deeper into the topic with chapter-based learning."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/10/google-is-going-all-in-on-mcp-servers-agent-ready-by-design/",
    "site_type": "techcrunch",
    "title": "Google launches managed MCP servers that let AI agents simply plug into its tools",
    "date": "10.12.2025",
    "text": "AI agents are being sold as the solution for planning trips, answering business questions, and solving problems of all kinds, but getting them to work with tools and data outside their chat interfaces has been tricky. Developers have to patch together various connectors and keep them running, but that’s a fragile approach that’s hard to scale and creates governance headaches.\n\nGoogle claims it’s trying to solve that by launching its own fully managed, remote MCP servers that would make its Google and Cloud services — like Maps and BigQuery — easier for agents to plug into.\n\nThe move follows the launch of Google’s latest Gemini 3 model , and the company is looking to pair stronger reasoning with more dependable connections to real-world tools and data.\n\n“We are making Google agent-ready by design,” Steren Giannini, product management director at Google Cloud, told TechCrunch.\n\nInstead of spending a week or two setting up connectors, developers can now essentially paste in a URL to a managed endpoint, Giannini said.\n\nAt launch, Google is starting with MCP servers for Maps, BigQuery, Compute Engine, and Kubernetes Engine. In practice, this might look like an analytics assistant querying BigQuery directly, or an ops agent interacting with infrastructure services.\n\nIn the case of Maps, Giannini said, without the MCP, developers would rely on the model’s built-in knowledge. “But by giving your agent […] a tool like the Google Maps MCP server, then it gets grounded on actual, up-to-date location information for places or trips planning,” he added.\n\nWhile the MCP servers will eventually be offered across all of Google’s tools, they are initially launching under public preview, meaning they’re not yet fully covered by Google Cloud terms of service. They are, however, being offered at no extra cost to enterprise customers that already pay for Google services.\n\n“We expect to bring them to general availability very soon in the new year,” Giannini said, adding that he expects more MCP servers to trickle in every week.\n\nMCP, which stands for Model Context Protocol, was developed by Anthropic about a year ago as an open source standard to connect AI systems with data and tools. The protocol has been widely adopted across the agent tooling world, and Anthropic earlier this week donated MCP to a new Linux Foundation fund dedicated to open sourcing and standardizing AI agent infrastructure.\n\n“The beauty of MCP is that, because it’s a standard, if Google provides a server, it can connect to any client,” Giannini said. “I’m looking forward to seeing how many more clients will emerge.”\n\nOne can think of MCP clients as the AI apps on the other end of the wire that talk to MCP servers and call the tools they offer. For Google, that includes Gemini CLI and AI Studio. Giannini said he’s also tried it with Anthropic’s Claude and OpenAI’s ChatGPT as clients, and “they just work.”\n\nGoogle argues this isn’t just about connecting agents to its services. The bigger enterprise play is Apigee, its API management product , which many companies already use to issue API keys, set quotas, and monitor traffic.\n\nGiannini said Apigee can essentially “translate” a standard API into an MCP server, turning endpoints like a product catalog API into tools an agent can discover and use, with existing security and governance controls layered on top.\n\nIn other words, the same API guardrails companies use for human-built apps could now apply to AI agents, too.\n\nGoogle’s new MCP servers are protected by a permission mechanism called Google Cloud IAM, which explicitly protects what an agent can do with that server. They are also protected by Google Cloud Model Armor, which Giannini describes as a firewall dedicated to agentic workloads that defends against advanced agentic threats like prompt injection and data exfiltration. Administrators can also rely on audit logging for additional observability.\n\nGoogle plans to expand MCP support beyond the initial set of servers. In the next few months, the company will roll out support for services across areas like storage, databases, logging and monitoring, and security.\n\n“We built the plumbing so that developers don’t have to,” Giannini said.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2024/07/GettyImages-2148009757.jpg?w=1024",
        "alt": "Image Credits:Krisztian Bocsi/Bloomberg / Getty Images"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2024/07/GettyImages-2148009757.jpg?w=1024",
        "alt": "Image Credits:Krisztian Bocsi/Bloomberg / Getty Images"
      },
      {
        "type": "text",
        "content": "AI agents are being sold as the solution for planning trips, answering business questions, and solving problems of all kinds, but getting them to work with tools and data outside their chat interfaces has been tricky. Developers have to patch together various connectors and keep them running, but that’s a fragile approach that’s hard to scale and creates governance headaches."
      },
      {
        "type": "text",
        "content": "Google claims it’s trying to solve that by launching its own fully managed, remote MCP servers that would make its Google and Cloud services — like Maps and BigQuery — easier for agents to plug into."
      },
      {
        "type": "text",
        "content": "The move follows the launch of Google’s latest Gemini 3 model , and the company is looking to pair stronger reasoning with more dependable connections to real-world tools and data."
      },
      {
        "type": "text",
        "content": "“We are making Google agent-ready by design,” Steren Giannini, product management director at Google Cloud, told TechCrunch."
      },
      {
        "type": "text",
        "content": "Instead of spending a week or two setting up connectors, developers can now essentially paste in a URL to a managed endpoint, Giannini said."
      },
      {
        "type": "text",
        "content": "At launch, Google is starting with MCP servers for Maps, BigQuery, Compute Engine, and Kubernetes Engine. In practice, this might look like an analytics assistant querying BigQuery directly, or an ops agent interacting with infrastructure services."
      },
      {
        "type": "text",
        "content": "In the case of Maps, Giannini said, without the MCP, developers would rely on the model’s built-in knowledge. “But by giving your agent […] a tool like the Google Maps MCP server, then it gets grounded on actual, up-to-date location information for places or trips planning,” he added."
      },
      {
        "type": "text",
        "content": "While the MCP servers will eventually be offered across all of Google’s tools, they are initially launching under public preview, meaning they’re not yet fully covered by Google Cloud terms of service. They are, however, being offered at no extra cost to enterprise customers that already pay for Google services."
      },
      {
        "type": "text",
        "content": "“We expect to bring them to general availability very soon in the new year,” Giannini said, adding that he expects more MCP servers to trickle in every week."
      },
      {
        "type": "text",
        "content": "MCP, which stands for Model Context Protocol, was developed by Anthropic about a year ago as an open source standard to connect AI systems with data and tools. The protocol has been widely adopted across the agent tooling world, and Anthropic earlier this week donated MCP to a new Linux Foundation fund dedicated to open sourcing and standardizing AI agent infrastructure."
      },
      {
        "type": "text",
        "content": "“The beauty of MCP is that, because it’s a standard, if Google provides a server, it can connect to any client,” Giannini said. “I’m looking forward to seeing how many more clients will emerge.”"
      },
      {
        "type": "text",
        "content": "One can think of MCP clients as the AI apps on the other end of the wire that talk to MCP servers and call the tools they offer. For Google, that includes Gemini CLI and AI Studio. Giannini said he’s also tried it with Anthropic’s Claude and OpenAI’s ChatGPT as clients, and “they just work.”"
      },
      {
        "type": "text",
        "content": "Google argues this isn’t just about connecting agents to its services. The bigger enterprise play is Apigee, its API management product , which many companies already use to issue API keys, set quotas, and monitor traffic."
      },
      {
        "type": "text",
        "content": "Giannini said Apigee can essentially “translate” a standard API into an MCP server, turning endpoints like a product catalog API into tools an agent can discover and use, with existing security and governance controls layered on top."
      },
      {
        "type": "text",
        "content": "In other words, the same API guardrails companies use for human-built apps could now apply to AI agents, too."
      },
      {
        "type": "text",
        "content": "Google’s new MCP servers are protected by a permission mechanism called Google Cloud IAM, which explicitly protects what an agent can do with that server. They are also protected by Google Cloud Model Armor, which Giannini describes as a firewall dedicated to agentic workloads that defends against advanced agentic threats like prompt injection and data exfiltration. Administrators can also rely on audit logging for additional observability."
      },
      {
        "type": "text",
        "content": "Google plans to expand MCP support beyond the initial set of servers. In the next few months, the company will roll out support for services across areas like storage, databases, logging and monitoring, and security."
      },
      {
        "type": "text",
        "content": "“We built the plumbing so that developers don’t have to,” Giannini said."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/10/google-is-testing-ai-powered-article-overviews-on-select-publications-google-news-pages/",
    "site_type": "techcrunch",
    "title": "Google is testing AI-powered article overviews on select publications’ Google News pages",
    "date": "10.12.2025",
    "text": "Google is testing AI-powered article overviews on participating publications’ Google News pages as part of a new pilot program, the search giant announced on Wednesday.\n\nNews publishers participating in the pilot program include Der Spiegel, El País, Folha, Infobae, Kompas, The Guardian, The Times of India, The Washington Examiner, and The Washington Post, among others.\n\nThe purpose of the new commercial partnership program is to “explore how AI can drive more engaged audiences,” Google said in a blog post . As part of the new AI pilot program, the company will work with publishers to experiment with new features in Google News.\n\nBy adding AI-powered article overviews, Google says users will get more context before they click through to read an article. While AI-generated summaries may lead to fewer clicks on news articles, publications participating in the commercial pilot program will receive direct payments from Google, which could make up for the potential decrease in traffic to their sites.\n\nThe AI-powered article overviews will only appear on participating publications’ Google News pages, and not anywhere else on Google News or in Search.\n\nThis isn’t the first time that Google has introduced AI summaries for news. In July, the company rolled out AI summaries in Discover , the main news feed inside Google’s search app. With this change, users no longer see a single headline from a major publication in the feed. Instead, they see the logos of multiple news publishers in the top-left corner, followed by an AI-generated summary that cites those sources.\n\nGoogle is also experimenting with audio briefings for people who prefer listening to the news rather than reading it, as part of the new pilot program.\n\nThe company says these features will include clear attribution and a link to articles.\n\nAdditionally, Google is partnering with organizations such as Estadão, Antara, Yonhap, and The Associated Press to incorporate real-time information and enhance results in the Gemini app.\n\n“As the way people consume information evolves, we’ll continue to improve our products for people around the world and engage with feedback from stakeholders across the ecosystem,” Google wrote in its blog post. “We’re doing this work in collaboration with websites and creators of all sizes, from major news publishers to new and emerging voices.”\n\nAs part of Google’s Wednesday announcement, the company said that it’s launching its “Preferred Sources” feature globally after first launching it in the U.S. and India in August. The feature allows users to select their favorite news sites and blogs to appear in the Top Stories section of Google search results.\n\nIn the coming days, the feature will be available for English-language users worldwide, and Google plans to roll it out to all supported languages early next year.\n\nGoogle will now also highlight links from your news subscriptions and show these links in a dedicated carousel in the Gemini app in the coming weeks, with AI Overviews and AI Mode to follow.\n\nWhile these features make it easy for users to access news from their preferred sources, they also risk confining them to an ideological bubble that limits their exposure to different perspectives.\n\nGoogle also announced that it’s increasing the number of inline links in AI Mode. Additionally, it’s introducing “contextual introductions” for embedded links, which are brief explanations that explain why a link could be useful to explore.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2196352264.jpg?w=1024",
        "alt": "Image Credits:Matthias Balk/picture alliance / Getty Images"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-10-at-10.59.58-AM.png?w=680",
        "alt": "Image Credits:Google"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2196352264.jpg?w=1024",
        "alt": "Image Credits:Matthias Balk/picture alliance / Getty Images"
      },
      {
        "type": "text",
        "content": "Google is testing AI-powered article overviews on participating publications’ Google News pages as part of a new pilot program, the search giant announced on Wednesday."
      },
      {
        "type": "text",
        "content": "News publishers participating in the pilot program include Der Spiegel, El País, Folha, Infobae, Kompas, The Guardian, The Times of India, The Washington Examiner, and The Washington Post, among others."
      },
      {
        "type": "text",
        "content": "The purpose of the new commercial partnership program is to “explore how AI can drive more engaged audiences,” Google said in a blog post . As part of the new AI pilot program, the company will work with publishers to experiment with new features in Google News."
      },
      {
        "type": "text",
        "content": "By adding AI-powered article overviews, Google says users will get more context before they click through to read an article. While AI-generated summaries may lead to fewer clicks on news articles, publications participating in the commercial pilot program will receive direct payments from Google, which could make up for the potential decrease in traffic to their sites."
      },
      {
        "type": "text",
        "content": "The AI-powered article overviews will only appear on participating publications’ Google News pages, and not anywhere else on Google News or in Search."
      },
      {
        "type": "text",
        "content": "This isn’t the first time that Google has introduced AI summaries for news. In July, the company rolled out AI summaries in Discover , the main news feed inside Google’s search app. With this change, users no longer see a single headline from a major publication in the feed. Instead, they see the logos of multiple news publishers in the top-left corner, followed by an AI-generated summary that cites those sources."
      },
      {
        "type": "text",
        "content": "Google is also experimenting with audio briefings for people who prefer listening to the news rather than reading it, as part of the new pilot program."
      },
      {
        "type": "text",
        "content": "The company says these features will include clear attribution and a link to articles."
      },
      {
        "type": "text",
        "content": "Additionally, Google is partnering with organizations such as Estadão, Antara, Yonhap, and The Associated Press to incorporate real-time information and enhance results in the Gemini app."
      },
      {
        "type": "text",
        "content": "“As the way people consume information evolves, we’ll continue to improve our products for people around the world and engage with feedback from stakeholders across the ecosystem,” Google wrote in its blog post. “We’re doing this work in collaboration with websites and creators of all sizes, from major news publishers to new and emerging voices.”"
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-10-at-10.59.58-AM.png?w=680",
        "alt": "Image Credits:Google"
      },
      {
        "type": "text",
        "content": "As part of Google’s Wednesday announcement, the company said that it’s launching its “Preferred Sources” feature globally after first launching it in the U.S. and India in August. The feature allows users to select their favorite news sites and blogs to appear in the Top Stories section of Google search results."
      },
      {
        "type": "text",
        "content": "In the coming days, the feature will be available for English-language users worldwide, and Google plans to roll it out to all supported languages early next year."
      },
      {
        "type": "text",
        "content": "Google will now also highlight links from your news subscriptions and show these links in a dedicated carousel in the Gemini app in the coming weeks, with AI Overviews and AI Mode to follow."
      },
      {
        "type": "text",
        "content": "While these features make it easy for users to access news from their preferred sources, they also risk confining them to an ideological bubble that limits their exposure to different perspectives."
      },
      {
        "type": "text",
        "content": "Google also announced that it’s increasing the number of inline links in AI Mode. Additionally, it’s introducing “contextual introductions” for embedded links, which are brief explanations that explain why a link could be useful to explore."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/10/spotify-tests-more-personalized-ai-powered-prompted-playlists/",
    "site_type": "techcrunch",
    "title": "Spotify tests more personalized, AI-powered ‘Prompted Playlists’",
    "date": "10.12.2025",
    "text": "Spotify announced on Wednesday that, for the first time, it’s giving users more control over the streaming service’s algorithm. That’s at least how the company is framing the launch of its new “Promoted Playlists,” a feature that will initially be available to Premium subscribers in New Zealand.\n\nThe feature, which is currently available in English only, is still in beta and will evolve before rolling out to other markets, according to Spotify.\n\nThe new tool allows users to describe what they want to hear in a personalized playlist that reflects the “full arc” of their tastes, according to the company. That means the playlist focuses not only on the songs you like now, but your entire Spotify listening history from day one — something that differentiates the feature from other playlists, the company says.\n\nThe feature is an evolution from Spotify’s existing AI playlist option, which debuted last year , and also works through written prompts. As with AI playlists, the new Prompted Playlists allow users to request what they want to hear with written instructions. However, they can now write much longer prompts with more specific instructions. That’s because the new AI feature factors in world knowledge, a rep from Spotify explained to TechCrunch.\n\nIn addition, the ability to go further back in your listening history and schedule how often the playlist refreshes makes it different from Spotify’s other AI playlist offerings.\n\nFor instance, Spotify suggests subscribers can use the new feature to ask for something like, “music from my top artists from the last five years,” then amend the prompt to include a request for “deep cuts I haven’t heard yet.”\n\nIn another example of a longer prompt, Spotify said you could ask for “high-energy pop and hip-hop for a 30-minute 5K run that keeps a steady pace before easing into relaxing songs for a cool-down” or “music from this year’s biggest films and most-talked-about TV shows that match my taste.”\n\nIn addition, you can continue to fine-tune the prompt to make it even more specific, and can set how often you want it to refresh, like daily or weekly. The idea is that users can essentially make their own version of something like Spotify’s flagship playlist, Discover Weekly, but one that’s focused on a type of music, genre, or time period they’d like to track, or their own version of something like Spotify’s genre-focused Daily Mixes.\n\nThe company says the playlist will include descriptions and context so you know why you’re getting the recommendation. Plus, it will offer a set of prompts to help users get started.\n\nSpotify isn’t the only social app pitching how it’s letting users take control of its algorithm . Instagram today also introduced a new feature that lets users control what type of reels they see. Bluesky, a decentralized X competitor, also lets users swap out its algorithm for one of their own.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Prompted-Playlist-Static-Image-1-Correct-1234832-scaled-1.jpg?w=1024",
        "alt": "Image Credits:Spotify"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Prompted-Playlist-GIF-102924.gif?w=680",
        "alt": "Image Credits:Spotify"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Prompted-Playlist-Static-Image-1-Correct-1234832-scaled-1.jpg?w=1024",
        "alt": "Image Credits:Spotify"
      },
      {
        "type": "text",
        "content": "Spotify announced on Wednesday that, for the first time, it’s giving users more control over the streaming service’s algorithm. That’s at least how the company is framing the launch of its new “Promoted Playlists,” a feature that will initially be available to Premium subscribers in New Zealand."
      },
      {
        "type": "text",
        "content": "The feature, which is currently available in English only, is still in beta and will evolve before rolling out to other markets, according to Spotify."
      },
      {
        "type": "text",
        "content": "The new tool allows users to describe what they want to hear in a personalized playlist that reflects the “full arc” of their tastes, according to the company. That means the playlist focuses not only on the songs you like now, but your entire Spotify listening history from day one — something that differentiates the feature from other playlists, the company says."
      },
      {
        "type": "text",
        "content": "The feature is an evolution from Spotify’s existing AI playlist option, which debuted last year , and also works through written prompts. As with AI playlists, the new Prompted Playlists allow users to request what they want to hear with written instructions. However, they can now write much longer prompts with more specific instructions. That’s because the new AI feature factors in world knowledge, a rep from Spotify explained to TechCrunch."
      },
      {
        "type": "text",
        "content": "In addition, the ability to go further back in your listening history and schedule how often the playlist refreshes makes it different from Spotify’s other AI playlist offerings."
      },
      {
        "type": "text",
        "content": "For instance, Spotify suggests subscribers can use the new feature to ask for something like, “music from my top artists from the last five years,” then amend the prompt to include a request for “deep cuts I haven’t heard yet.”"
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Prompted-Playlist-GIF-102924.gif?w=680",
        "alt": "Image Credits:Spotify"
      },
      {
        "type": "text",
        "content": "In another example of a longer prompt, Spotify said you could ask for “high-energy pop and hip-hop for a 30-minute 5K run that keeps a steady pace before easing into relaxing songs for a cool-down” or “music from this year’s biggest films and most-talked-about TV shows that match my taste.”"
      },
      {
        "type": "text",
        "content": "In addition, you can continue to fine-tune the prompt to make it even more specific, and can set how often you want it to refresh, like daily or weekly. The idea is that users can essentially make their own version of something like Spotify’s flagship playlist, Discover Weekly, but one that’s focused on a type of music, genre, or time period they’d like to track, or their own version of something like Spotify’s genre-focused Daily Mixes."
      },
      {
        "type": "text",
        "content": "The company says the playlist will include descriptions and context so you know why you’re getting the recommendation. Plus, it will offer a set of prompts to help users get started."
      },
      {
        "type": "text",
        "content": "Spotify isn’t the only social app pitching how it’s letting users take control of its algorithm . Instagram today also introduced a new feature that lets users control what type of reels they see. Bluesky, a decentralized X competitor, also lets users swap out its algorithm for one of their own."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/10/state-attorneys-general-warn-microsoft-openai-google-and-other-ai-giants-to-fix-delusional-outputs/",
    "site_type": "techcrunch",
    "title": "State attorneys general warn Microsoft, OpenAI, Google, and other AI giants to fix ‘delusional’ outputs",
    "date": "10.12.2025",
    "text": "After a string of disturbing mental health incidents involving AI chatbots, a group of state attorneys general have sent a letter to the AI industry’s top companies, with a warning to fix “delusional outputs” or risk being in breach of state law.\n\nThe letter , signed by dozens of AGs from U.S. states and territories with the National Association of Attorneys General, asks the companies, including Microsoft, OpenAI, Google, and 10 other major AI firms, to implement a variety of new internal safeguards to protect their users. Anthropic, Apple, Chai AI, Character Technologies, Luka, Meta, Nomi AI, Perplexity AI, Replika, and xAI were also included in the letter.\n\nThe letter comes as a fight over AI regulations has been brewing between state and federal government.\n\nThose safeguards include transparent third-party audits of large language models that look for signs of delusional or sycophantic ideations, as well as new incident reporting procedures designed to notify users when chatbots produce psychologically harmful outputs. Those third parties, which could include academic and civil society groups, should be allowed to “evaluate systems pre-release without retaliation and to publish their findings without prior approval from the company,” the letter states.\n\n“GenAI has the potential to change how the world works in a positive way. But it also has caused—and has the potential to cause—serious harm, especially to vulnerable populations,” the letter states, pointing to a number of well-publicized incidents over the past year — including suicides and murder — in which violence has been linked to excessive AI use, the letter states. “In many of these incidents, the GenAI products generated sycophantic and delusional outputs that either encouraged users’ delusions or assured users that they were not delusional.”\n\nAGs also suggest companies treat mental health incidents the same way tech companies handle cybersecurity incidents — with clear and transparent incident reporting policies and procedures.\n\nCompanies should develop and publish “detection and response timelines for sycophantic and delusional outputs,” the letter states. In a similar fashion to how data breaches are currently handled, companies should also “promptly, clearly, and directly notify users if they were exposed to potentially harmful sycophantic or delusional outputs,” the letter says.\n\nAnother ask is that the companies develop “reasonable and appropriate safety tests” on GenAI models to “ensure the models do not produce potentially harmful sycophantic and delusional outputs.” These tests should be conducted before the models are ever offered to the public, it adds.\n\nTechCrunch was unable to reach Google, Microsoft, or OpenAI for comment prior to publication. The article will be updated if the companies respond.\n\nTech companies developing AI have had a much warmer reception at the federal level.\n\nThe Trump administration has made it known it is unabashedly pro-AI , and, over the past year, multiple attempts have been made to pass a nationwide moratorium on state-level AI regulations. So far, those attempts have failed — thanks, in part, to pressure from state officials .\n\nNot to be deterred, Trump announced Monday he plans to pass an executive order next week that will limit the ability of states to regulate AI. The president said in a post on Truth Social he hoped his EO would stop AI from being “DESTROYED IN ITS INFANCY.”",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2195918462.jpg?w=1024",
        "alt": "Image Credits:Silas Stein/picture alliance / Getty Images"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2195918462.jpg?w=1024",
        "alt": "Image Credits:Silas Stein/picture alliance / Getty Images"
      },
      {
        "type": "text",
        "content": "After a string of disturbing mental health incidents involving AI chatbots, a group of state attorneys general have sent a letter to the AI industry’s top companies, with a warning to fix “delusional outputs” or risk being in breach of state law."
      },
      {
        "type": "text",
        "content": "The letter , signed by dozens of AGs from U.S. states and territories with the National Association of Attorneys General, asks the companies, including Microsoft, OpenAI, Google, and 10 other major AI firms, to implement a variety of new internal safeguards to protect their users. Anthropic, Apple, Chai AI, Character Technologies, Luka, Meta, Nomi AI, Perplexity AI, Replika, and xAI were also included in the letter."
      },
      {
        "type": "text",
        "content": "The letter comes as a fight over AI regulations has been brewing between state and federal government."
      },
      {
        "type": "text",
        "content": "Those safeguards include transparent third-party audits of large language models that look for signs of delusional or sycophantic ideations, as well as new incident reporting procedures designed to notify users when chatbots produce psychologically harmful outputs. Those third parties, which could include academic and civil society groups, should be allowed to “evaluate systems pre-release without retaliation and to publish their findings without prior approval from the company,” the letter states."
      },
      {
        "type": "text",
        "content": "“GenAI has the potential to change how the world works in a positive way. But it also has caused—and has the potential to cause—serious harm, especially to vulnerable populations,” the letter states, pointing to a number of well-publicized incidents over the past year — including suicides and murder — in which violence has been linked to excessive AI use, the letter states. “In many of these incidents, the GenAI products generated sycophantic and delusional outputs that either encouraged users’ delusions or assured users that they were not delusional.”"
      },
      {
        "type": "text",
        "content": "AGs also suggest companies treat mental health incidents the same way tech companies handle cybersecurity incidents — with clear and transparent incident reporting policies and procedures."
      },
      {
        "type": "text",
        "content": "Companies should develop and publish “detection and response timelines for sycophantic and delusional outputs,” the letter states. In a similar fashion to how data breaches are currently handled, companies should also “promptly, clearly, and directly notify users if they were exposed to potentially harmful sycophantic or delusional outputs,” the letter says."
      },
      {
        "type": "text",
        "content": "Another ask is that the companies develop “reasonable and appropriate safety tests” on GenAI models to “ensure the models do not produce potentially harmful sycophantic and delusional outputs.” These tests should be conducted before the models are ever offered to the public, it adds."
      },
      {
        "type": "text",
        "content": "TechCrunch was unable to reach Google, Microsoft, or OpenAI for comment prior to publication. The article will be updated if the companies respond."
      },
      {
        "type": "text",
        "content": "Tech companies developing AI have had a much warmer reception at the federal level."
      },
      {
        "type": "text",
        "content": "The Trump administration has made it known it is unabashedly pro-AI , and, over the past year, multiple attempts have been made to pass a nationwide moratorium on state-level AI regulations. So far, those attempts have failed — thanks, in part, to pressure from state officials ."
      },
      {
        "type": "text",
        "content": "Not to be deterred, Trump announced Monday he plans to pass an executive order next week that will limit the ability of states to regulate AI. The president said in a post on Truth Social he hoped his EO would stop AI from being “DESTROYED IN ITS INFANCY.”"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/11/opera-wants-you-to-pay-20-a-month-to-use-its-ai-powered-browser-neon/",
    "site_type": "techcrunch",
    "title": "Opera wants you to pay $20 a month to use its AI-powered browser Neon",
    "date": "11.12.2025",
    "text": "Following a couple of months’ testing, Norway-based browser company Opera has finally made its AI-powered browser, Neon, available to the public — though you’ll have to shell out $19.90 per month to use it.\n\nOpera first unveiled Neon earlier this year in May and launched it in early access to select users in October .\n\nSimilar to other AI-first browsers like Perplexity’s Comet , OpenAI’s Atlas , and The Browser Company’s Dia , Neon bakes in an AI chatbot into its interface, letting you ask it answers about pages, use it to create mini apps and videos, and get it to do tasks for you. The browser uses your browsing history as context, so you can do things like ask it to fetch details from a YouTube video you watched last week or the post that you read yesterday.\n\nYou can also build “Cards” for repeatable tasks using prompts, and the browser offers a deep research agent that can get you detailed information about any topic. The browser also has a new tab organizational feature called Tasks, which are contained workspaces of AI chats and tabs. This feature is more like Tab Groups combined with Arc Browser’s Spaces feature , which has its own context for AI.\n\nIn addition to the AI features, the subscription gives users access to top models like Gemini 3 Pro, GPT-5.1, Veo 3.1, and Nano Banana Pro. Subscribers will also get access to Opera’s Discord community and direct access to its developers.\n\n“Opera Neon is a product for people who like to be the first to the newest AI tech. It’s a rapidly evolving project with significant updates released every week. We’ve been shaping it with our Founders community for a while and are now excited to share the early access to it with a larger audience,” Krystian Kolondra, EVP of browsers at Opera, said in a statement.\n\nThe company noted that its other products, like Opera One, Opera GX, and Opera Air, also have free AI features, like a chat-based assistant.\n\nMeanwhile, browser incumbents are taking a slower approach to adding AI features to their products. Earlier this week, Google detailed the security work it is doing to protect users against different attack surfaces that agentic features are prone to, and Brave said on Wednesday it is previewing its agentic features in a nightly build , and provides an isolated browsing profile for using AI features so users can keep their regular, non-AI usage separate.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Agent-Selector.jpeg?w=1024",
        "alt": "Image Credits:Opera"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Cards.jpeg?w=680",
        "alt": "Image Credits: Opera"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Agent-Selector.jpeg?w=1024",
        "alt": "Image Credits:Opera"
      },
      {
        "type": "text",
        "content": "Following a couple of months’ testing, Norway-based browser company Opera has finally made its AI-powered browser, Neon, available to the public — though you’ll have to shell out $19.90 per month to use it."
      },
      {
        "type": "text",
        "content": "Opera first unveiled Neon earlier this year in May and launched it in early access to select users in October ."
      },
      {
        "type": "text",
        "content": "Similar to other AI-first browsers like Perplexity’s Comet , OpenAI’s Atlas , and The Browser Company’s Dia , Neon bakes in an AI chatbot into its interface, letting you ask it answers about pages, use it to create mini apps and videos, and get it to do tasks for you. The browser uses your browsing history as context, so you can do things like ask it to fetch details from a YouTube video you watched last week or the post that you read yesterday."
      },
      {
        "type": "text",
        "content": "You can also build “Cards” for repeatable tasks using prompts, and the browser offers a deep research agent that can get you detailed information about any topic. The browser also has a new tab organizational feature called Tasks, which are contained workspaces of AI chats and tabs. This feature is more like Tab Groups combined with Arc Browser’s Spaces feature , which has its own context for AI."
      },
      {
        "type": "text",
        "content": "In addition to the AI features, the subscription gives users access to top models like Gemini 3 Pro, GPT-5.1, Veo 3.1, and Nano Banana Pro. Subscribers will also get access to Opera’s Discord community and direct access to its developers."
      },
      {
        "type": "text",
        "content": "“Opera Neon is a product for people who like to be the first to the newest AI tech. It’s a rapidly evolving project with significant updates released every week. We’ve been shaping it with our Founders community for a while and are now excited to share the early access to it with a larger audience,” Krystian Kolondra, EVP of browsers at Opera, said in a statement."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Cards.jpeg?w=680",
        "alt": "Image Credits: Opera"
      },
      {
        "type": "text",
        "content": "The company noted that its other products, like Opera One, Opera GX, and Opera Air, also have free AI features, like a chat-based assistant."
      },
      {
        "type": "text",
        "content": "Meanwhile, browser incumbents are taking a slower approach to adding AI features to their products. Earlier this week, Google detailed the security work it is doing to protect users against different attack surfaces that agentic features are prone to, and Brave said on Wednesday it is previewing its agentic features in a nightly build , and provides an isolated browsing profile for using AI features so users can keep their regular, non-AI usage separate."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/11/disney-signs-deal-with-openai-to-allow-sora-to-generate-ai-videos-featuring-its-characters/",
    "site_type": "techcrunch",
    "title": "Disney signs deal with OpenAI to allow Sora to generate AI videos featuring its characters",
    "date": "11.12.2025",
    "text": "The Walt Disney Company announced on Thursday that it has signed a three-year partnership with OpenAI that will bring its iconic characters to the company’s Sora AI video generator. Disney is also making a $1 billion equity investment in OpenAI.\n\nDisney CEO Bob Iger told CNBC that the deal comes with about a year of exclusivity. After the year is up, Disney can license its IP to other AI companies.\n\nLaunched in September, Sora allows users to create short videos using simple prompts. With this new agreement, users will be able to draw on more than 200 animated, masked, and creature characters from Disney, Marvel, Pixar, and Star Wars, including costumes, props, vehicles, and more.\n\nThese characters include iconic faces like Mickey Mouse, Ariel, Belle, Cinderella, Baymax, and Simba, as well as characters from Encanto, Frozen, Inside Out, Moana, Monsters, Inc., Toy Story, Up, and Zootopia. Users will also be able to draw on animated or illustrated versions of Marvel and Lucasfilm characters like Black Panther, Captain America, Deadpool, Groot, Iron Man, Darth Vader, Han Solo, Stormtroopers, and more.\n\nUsers will also be able to draw on these characters while using ChatGPT Images, the feature in ChatGPT that allows users to create visuals using text prompts.\n\nThe agreement does not include any talent likenesses or voices, Disney says.\n\n“The rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works,” said Disney CEO Bob Iger in a statement.\n\nDisney says that alongside the agreement, it will “become a major customer of OpenAI,” as it will use its APIs to build new products, tools, and experiences, including for Disney+.\n\n“Disney is the global gold standard for storytelling, and we’re excited to partner to allow Sora and ChatGPT Images to expand the way people create and experience great content,” said Sam Altman, co-founder and CEO of OpenAI, in a statement. “This agreement shows how AI companies and creative leaders can work together responsibly to promote innovation that benefits society, respect the importance of creativity, and help works reach vast new audiences.”\n\nIt’s worth noting that Disney has sued the generative AI platform Midjourney for ignoring requests to stop violating its intellectual property rights. Disney also sent a cease-and-desist letter to Character.AI , urging the chatbot company to remove Disney characters from among the millions of AI companions on its platform.\n\nDisney’s agreement with OpenAI indicates the company isn’t fully closing the door on AI platforms.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1387623215.jpg?w=1024",
        "alt": "Image Credits:Yujie Chen / Getty Images"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1387623215.jpg?w=1024",
        "alt": "Image Credits:Yujie Chen / Getty Images"
      },
      {
        "type": "text",
        "content": "The Walt Disney Company announced on Thursday that it has signed a three-year partnership with OpenAI that will bring its iconic characters to the company’s Sora AI video generator. Disney is also making a $1 billion equity investment in OpenAI."
      },
      {
        "type": "text",
        "content": "Disney CEO Bob Iger told CNBC that the deal comes with about a year of exclusivity. After the year is up, Disney can license its IP to other AI companies."
      },
      {
        "type": "text",
        "content": "Launched in September, Sora allows users to create short videos using simple prompts. With this new agreement, users will be able to draw on more than 200 animated, masked, and creature characters from Disney, Marvel, Pixar, and Star Wars, including costumes, props, vehicles, and more."
      },
      {
        "type": "text",
        "content": "These characters include iconic faces like Mickey Mouse, Ariel, Belle, Cinderella, Baymax, and Simba, as well as characters from Encanto, Frozen, Inside Out, Moana, Monsters, Inc., Toy Story, Up, and Zootopia. Users will also be able to draw on animated or illustrated versions of Marvel and Lucasfilm characters like Black Panther, Captain America, Deadpool, Groot, Iron Man, Darth Vader, Han Solo, Stormtroopers, and more."
      },
      {
        "type": "text",
        "content": "Users will also be able to draw on these characters while using ChatGPT Images, the feature in ChatGPT that allows users to create visuals using text prompts."
      },
      {
        "type": "text",
        "content": "The agreement does not include any talent likenesses or voices, Disney says."
      },
      {
        "type": "text",
        "content": "“The rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works,” said Disney CEO Bob Iger in a statement."
      },
      {
        "type": "text",
        "content": "Disney says that alongside the agreement, it will “become a major customer of OpenAI,” as it will use its APIs to build new products, tools, and experiences, including for Disney+."
      },
      {
        "type": "text",
        "content": "“Disney is the global gold standard for storytelling, and we’re excited to partner to allow Sora and ChatGPT Images to expand the way people create and experience great content,” said Sam Altman, co-founder and CEO of OpenAI, in a statement. “This agreement shows how AI companies and creative leaders can work together responsibly to promote innovation that benefits society, respect the importance of creativity, and help works reach vast new audiences.”"
      },
      {
        "type": "text",
        "content": "It’s worth noting that Disney has sued the generative AI platform Midjourney for ignoring requests to stop violating its intellectual property rights. Disney also sent a cease-and-desist letter to Character.AI , urging the chatbot company to remove Disney characters from among the millions of AI companions on its platform."
      },
      {
        "type": "text",
        "content": "Disney’s agreement with OpenAI indicates the company isn’t fully closing the door on AI platforms."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/11/google-debuts-disco-a-gemini-powered-tool-for-making-web-apps-from-browser-tabs/",
    "site_type": "techcrunch",
    "title": "Google debuts ‘Disco,’ a Gemini-powered tool for making web apps from browser tabs",
    "date": "11.12.2025",
    "text": "Google on Thursday introduced a new AI experiment for the web browser: the Gemini-powered product Disco, which helps to turn your open tabs into custom applications. With Disco, you can create what Google is calling “GenTabs,” a tool that proactively suggests interactive web apps that can help you complete tasks related to what you’re browsing and allows you to build your own apps via written prompts.\n\nFor instance, if you’re studying a particular subject, GenTabs might suggest building a web app to visualize the information, which could help you better understand the core principles.\n\nOr, in a less academic scenario, you could use GenTabs to help you create a meal plan from a series of online recipes or help you plan a trip when you’re researching travel.\n\nThese are things that you can already do today with some AI-powered chatbots, but GenTabs builds these custom experiences on the fly using Gemini 3, using the information in your browser and in your Gemini chat history. After the app is built, you can also continue to refine it using natural language commands.\n\nThe resulting generative elements in the GenTabs experience will link back to the original sources, Google notes.\n\nLike others in the AI market, Google has been experimenting with bringing AI deeper into the web-browsing experience. Instead of building its own stand-alone AI browser, like Perplexity’s Comet or ChatGPT Atlas, Google integrated its AI assistant Gemini into the Chrome browser , where it can optionally be used to ask questions about the web page you’re on.\n\nWith GenTabs, the focus is not only on what you’re currently viewing, but also on your overall browsing, spanning multiple tabs — whether that’s research, learning, or something else.\n\nHowever, the feature is only initially going to be available to a small number of testers through Google Labs, who will offer feedback about the experience. The company says that interesting ideas that are developed through Disco may one day find their way into other, larger Google products.\n\nIt also suggests that GenTabs will be one of many Disco features to come over time, noting that GenTabs is the “first feature” being tested.\n\nTo access Disco, users will need to join a waitlist to download the app, starting on macOS.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2017/04/gettyimages-592356083.jpg?w=1024",
        "alt": "Image Credits:Michele Palazzo / EyeEm"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/1_Solar-System.gif?w=680",
        "alt": "Image Credits:Google"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/3_Meal-Plan.gif?w=680",
        "alt": "Image Credits:Google"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2017/04/gettyimages-592356083.jpg?w=1024",
        "alt": "Image Credits:Michele Palazzo / EyeEm"
      },
      {
        "type": "text",
        "content": "Google on Thursday introduced a new AI experiment for the web browser: the Gemini-powered product Disco, which helps to turn your open tabs into custom applications. With Disco, you can create what Google is calling “GenTabs,” a tool that proactively suggests interactive web apps that can help you complete tasks related to what you’re browsing and allows you to build your own apps via written prompts."
      },
      {
        "type": "text",
        "content": "For instance, if you’re studying a particular subject, GenTabs might suggest building a web app to visualize the information, which could help you better understand the core principles."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/1_Solar-System.gif?w=680",
        "alt": "Image Credits:Google"
      },
      {
        "type": "text",
        "content": "Or, in a less academic scenario, you could use GenTabs to help you create a meal plan from a series of online recipes or help you plan a trip when you’re researching travel."
      },
      {
        "type": "text",
        "content": "These are things that you can already do today with some AI-powered chatbots, but GenTabs builds these custom experiences on the fly using Gemini 3, using the information in your browser and in your Gemini chat history. After the app is built, you can also continue to refine it using natural language commands."
      },
      {
        "type": "text",
        "content": "The resulting generative elements in the GenTabs experience will link back to the original sources, Google notes."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/3_Meal-Plan.gif?w=680",
        "alt": "Image Credits:Google"
      },
      {
        "type": "text",
        "content": "Like others in the AI market, Google has been experimenting with bringing AI deeper into the web-browsing experience. Instead of building its own stand-alone AI browser, like Perplexity’s Comet or ChatGPT Atlas, Google integrated its AI assistant Gemini into the Chrome browser , where it can optionally be used to ask questions about the web page you’re on."
      },
      {
        "type": "text",
        "content": "With GenTabs, the focus is not only on what you’re currently viewing, but also on your overall browsing, spanning multiple tabs — whether that’s research, learning, or something else."
      },
      {
        "type": "text",
        "content": "However, the feature is only initially going to be available to a small number of testers through Google Labs, who will offer feedback about the experience. The company says that interesting ideas that are developed through Disco may one day find their way into other, larger Google products."
      },
      {
        "type": "text",
        "content": "It also suggests that GenTabs will be one of many Disco features to come over time, noting that GenTabs is the “first feature” being tested."
      },
      {
        "type": "text",
        "content": "To access Disco, users will need to join a waitlist to download the app, starting on macOS."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/11/googles-ai-try-on-feature-for-clothes-now-works-with-just-a-selfie/",
    "site_type": "techcrunch",
    "title": "Google’s AI try-on feature for clothes now works with just a selfie",
    "date": "11.12.2025",
    "text": "Google is updating its AI try-on feature to let you virtually try on clothes using just a selfie, the company announced on Thursday . In the past, users had to upload a full-body picture of themselves to virtually try on a piece of clothing. Now they can use a selfie and Nano Banana, Google’s Gemini 2.5 Flash Image model, to generate a full-body digital version of themselves for virtual try-ons.\n\nUsers can select their usual clothing size, and the feature will then generate several images. From there, users can choose one to make it their default try-on photo.\n\nIf desired, users still have the option to use a full-body photo or select from a range of models with diverse body types.\n\nThe new capability is launching in the United States today.\n\nGoogle first launched the try-on feature in July, allowing users to try on apparel items from its Shopping Graph across Search, Google Shopping, and Google Images. To use the feature, users need to tap on a product listing or apparel product result and select the “try it on” icon.\n\nThe move comes as Google has been investing in the virtual AI try-on space, as the company has a separate app dedicated specifically to that purpose. The app, called Doppl , is designed to help visualize how different outfits might look on you using AI.\n\nEarlier this week, the tech giant updated it with a shoppable discovery feed that displays recommendations so users can discover and virtually try on new items. Nearly everything in the feed is shoppable, with direct links to merchants.\n\nThe discovery feed features AI-generated videos of real products and suggests outfits based on your personalized style. While some may not be fond of an AI-generated feed, Google likely views it as a way to showcase products in a format that people are already familiar with, thanks to platforms like TikTok and Instagram.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2198713751.jpg?w=1024",
        "alt": "Image Credits:Klaudia Radecka/NurPhoto / Getty Images"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-11-at-12.55.46-PM.png?w=446",
        "alt": "Image Credits:Google"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2198713751.jpg?w=1024",
        "alt": "Image Credits:Klaudia Radecka/NurPhoto / Getty Images"
      },
      {
        "type": "text",
        "content": "Google is updating its AI try-on feature to let you virtually try on clothes using just a selfie, the company announced on Thursday . In the past, users had to upload a full-body picture of themselves to virtually try on a piece of clothing. Now they can use a selfie and Nano Banana, Google’s Gemini 2.5 Flash Image model, to generate a full-body digital version of themselves for virtual try-ons."
      },
      {
        "type": "text",
        "content": "Users can select their usual clothing size, and the feature will then generate several images. From there, users can choose one to make it their default try-on photo."
      },
      {
        "type": "text",
        "content": "If desired, users still have the option to use a full-body photo or select from a range of models with diverse body types."
      },
      {
        "type": "text",
        "content": "The new capability is launching in the United States today."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-11-at-12.55.46-PM.png?w=446",
        "alt": "Image Credits:Google"
      },
      {
        "type": "text",
        "content": "Google first launched the try-on feature in July, allowing users to try on apparel items from its Shopping Graph across Search, Google Shopping, and Google Images. To use the feature, users need to tap on a product listing or apparel product result and select the “try it on” icon."
      },
      {
        "type": "text",
        "content": "The move comes as Google has been investing in the virtual AI try-on space, as the company has a separate app dedicated specifically to that purpose. The app, called Doppl , is designed to help visualize how different outfits might look on you using AI."
      },
      {
        "type": "text",
        "content": "Earlier this week, the tech giant updated it with a shoppable discovery feed that displays recommendations so users can discover and virtually try on new items. Nearly everything in the feed is shoppable, with direct links to merchants."
      },
      {
        "type": "text",
        "content": "The discovery feed features AI-generated videos of real products and suggests outfits based on your personalized style. While some may not be fond of an AI-generated feed, Google likely views it as a way to showcase products in a format that people are already familiar with, thanks to platforms like TikTok and Instagram."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/11/google-launched-its-deepest-ai-research-agent-yet-on-the-same-day-openai-dropped-gpt-5-2/",
    "site_type": "techcrunch",
    "title": "Google launched its deepest AI research agent yet — on the same day OpenAI dropped GPT-5.2",
    "date": "11.12.2025",
    "text": "Google released on Thursday a “reimagined” version of its research agent Gemini Deep Research based on its much-ballyhooed state-of-the-art foundation model , Gemini 3 Pro.\n\nThis new agent isn’t just designed to produce research reports — although it can still do that. It now allows developers to embed Google’s SATA-model research capabilities into their own apps. That capability is made possible through Google’s new Interactions API , which is designed to give devs more control in the coming agentic AI era.\n\nThe new Gemini Deep Research tool is an agent equipped to synthesize mountains of information and handle a large context dump in the prompt. Google says it’s used by customers for tasks ranging from due diligence to drug toxicity safety research.\n\nGoogle also says it will soon be integrating this new deep research agent into services, including Google Search, Google Finance, its Gemini App, and its popular NotebookLM. This is another step toward preparing for a world where humans don’t Google anything anymore — their AI agents do.\n\nThe tech giant says that Deep Research benefits from Gemini 3 Pro’s status as its “most factual” model that is trained to minimize hallucinations during complex tasks.\n\nAI hallucinations — where the LLM just makes stuff up — are an especially crucial issue for long-running, deep reasoning agentic tasks, in which many autonomous decisions are made over minutes, hours, or longer. The more choices an LLM has to make, the greater the chance that even one hallucinated choice will invalidate the entire output.\n\nTo prove its progress claims, Google has also created yet another benchmark (as if the AI world needs another one). The new benchmark is unimaginatively named DeepSearchQA and is intended to test agents on complex, multi-step information-seeking tasks. Google has open sourced this benchmark.\n\nIt also tested Deep Research on Humanity’s Last Exam, a much more interestingly named, independent benchmark of general knowledge filled with impossibly niche tasks; and BrowserComp, a benchmark for browser-based agentic tasks.\n\nAs you might expect, Google’s new agent bested the competition on its own benchmark, and Humanity’s. However, OpenAI’s ChatGPT 5 Pro was a surprisingly close second all the way around and slightly bested Google on BrowserComp.\n\nBut those benchmark comparisons were obsolete almost the moment Google published them. Because on the same day, OpenAI launched its highly anticipated GPT 5.2 — codenamed Garlic. OpenAI says its newest model bests its rivals — especially Google — on a suite of the typical benchmarks, including OpenAI’s homegrown one.\n\nPerhaps one of the most interesting parts of this announcement was the timing. Knowing that the world was awaiting the release of Garlic, Google dropped some AI news of its own.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2022/05/GettyImages-1147600063.jpg?w=1024",
        "alt": "Image Credits:Justin Sullivan / Getty Images"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2022/05/GettyImages-1147600063.jpg?w=1024",
        "alt": "Image Credits:Justin Sullivan / Getty Images"
      },
      {
        "type": "text",
        "content": "Google released on Thursday a “reimagined” version of its research agent Gemini Deep Research based on its much-ballyhooed state-of-the-art foundation model , Gemini 3 Pro."
      },
      {
        "type": "text",
        "content": "This new agent isn’t just designed to produce research reports — although it can still do that. It now allows developers to embed Google’s SATA-model research capabilities into their own apps. That capability is made possible through Google’s new Interactions API , which is designed to give devs more control in the coming agentic AI era."
      },
      {
        "type": "text",
        "content": "The new Gemini Deep Research tool is an agent equipped to synthesize mountains of information and handle a large context dump in the prompt. Google says it’s used by customers for tasks ranging from due diligence to drug toxicity safety research."
      },
      {
        "type": "text",
        "content": "Google also says it will soon be integrating this new deep research agent into services, including Google Search, Google Finance, its Gemini App, and its popular NotebookLM. This is another step toward preparing for a world where humans don’t Google anything anymore — their AI agents do."
      },
      {
        "type": "text",
        "content": "The tech giant says that Deep Research benefits from Gemini 3 Pro’s status as its “most factual” model that is trained to minimize hallucinations during complex tasks."
      },
      {
        "type": "text",
        "content": "AI hallucinations — where the LLM just makes stuff up — are an especially crucial issue for long-running, deep reasoning agentic tasks, in which many autonomous decisions are made over minutes, hours, or longer. The more choices an LLM has to make, the greater the chance that even one hallucinated choice will invalidate the entire output."
      },
      {
        "type": "text",
        "content": "To prove its progress claims, Google has also created yet another benchmark (as if the AI world needs another one). The new benchmark is unimaginatively named DeepSearchQA and is intended to test agents on complex, multi-step information-seeking tasks. Google has open sourced this benchmark."
      },
      {
        "type": "text",
        "content": "It also tested Deep Research on Humanity’s Last Exam, a much more interestingly named, independent benchmark of general knowledge filled with impossibly niche tasks; and BrowserComp, a benchmark for browser-based agentic tasks."
      },
      {
        "type": "text",
        "content": "As you might expect, Google’s new agent bested the competition on its own benchmark, and Humanity’s. However, OpenAI’s ChatGPT 5 Pro was a surprisingly close second all the way around and slightly bested Google on BrowserComp."
      },
      {
        "type": "text",
        "content": "But those benchmark comparisons were obsolete almost the moment Google published them. Because on the same day, OpenAI launched its highly anticipated GPT 5.2 — codenamed Garlic. OpenAI says its newest model bests its rivals — especially Google — on a suite of the typical benchmarks, including OpenAI’s homegrown one."
      },
      {
        "type": "text",
        "content": "Perhaps one of the most interesting parts of this announcement was the timing. Knowing that the world was awaiting the release of Garlic, Google dropped some AI news of its own."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/12/google-translate-now-lets-you-hear-real-time-translations-in-your-headphones/",
    "site_type": "techcrunch",
    "title": "Google Translate now lets you hear real-time translations in your headphones",
    "date": "12.12.2025",
    "text": "Google is rolling out a beta experience that lets you hear real-time translations in your headphones, the company announced on Friday. The tech giant is also bringing advanced Gemini capabilities to Google Translate and expanding its language-learning tools in the Translate app.\n\nThe new real-time headphone translations experience keeps each speaker’s tone, emphasis, and cadence intact, so it’s easier to follow the conversation and tell who’s saying what, Google says. The new capability essentially turns any pair of headphones into a real-time, one-way translation device.\n\n“Whether you’re trying to have a conversation in a different language, listen to a speech or lecture while abroad, or watch a TV show or film in another language, you can now put in your headphones, open the Translate app, tap ‘Live translate’ and hear a real-time translation in your preferred language,” said Rose Yao, Google VP Product Management, Search Verticals, in the blog post.\n\nThe beta is rolling out now in the Translate app on Android in the U.S., Mexico, and India. The feature works with any pair of headphones and supports more than 70 languages.\n\nThe company plans to bring the capability to iOS and more countries in 2026.\n\nAs for the advanced Gemini capabilities coming to Translate, Google says they will enable smarter, more natural, and accurate text translations. They’ll also enable improved translations of phrases with more nuanced meanings, like slang, idioms, or local expressions.\n\nFor example, if you’re trying to translate an English idiom like “stealing my thunder,” you’ll now get a more accurate translation instead of a literal word-for-word translation, as Gemini will parse the context to capture what the idiom really means.\n\nThis update is rolling out now in the U.S. and India, translating between English and nearly 20 languages, including Spanish, Arabic, Chinese, Japanese, and German. The update is available in the Translate app on Android, iOS, and on the web.\n\nGoogle is also expanding its language learning tools to almost 20 new countries, including Germany, India, Sweden, and Taiwan. English speakers can now practice German, while Bengali, Mandarin Chinese (Simplified), Dutch, German, Hindi, Italian, Romanian, and Swedish speakers can practice English.\n\nThe tech giant is also adding improved feedback so you can get helpful tips based on your speaking practice.\n\nAdditionally, Google is adding a feature that tracks how many days in a row you’ve been learning, making it easier to see your progress and stay consistent. While the tools were already designed to take on Duolingo, this new feature brings the experience even closer to the popular language-learning app.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2196352264.jpg?w=1024",
        "alt": "Image Credits:Matthias Balk/picture alliance / Getty Images"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-12-at-10.02.57-AM.png?w=680",
        "alt": "Image Credits:Google"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Text-Translations-Still.png?w=544",
        "alt": "Image Credits:Google"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2196352264.jpg?w=1024",
        "alt": "Image Credits:Matthias Balk/picture alliance / Getty Images"
      },
      {
        "type": "text",
        "content": "Google is rolling out a beta experience that lets you hear real-time translations in your headphones, the company announced on Friday. The tech giant is also bringing advanced Gemini capabilities to Google Translate and expanding its language-learning tools in the Translate app."
      },
      {
        "type": "text",
        "content": "The new real-time headphone translations experience keeps each speaker’s tone, emphasis, and cadence intact, so it’s easier to follow the conversation and tell who’s saying what, Google says. The new capability essentially turns any pair of headphones into a real-time, one-way translation device."
      },
      {
        "type": "text",
        "content": "“Whether you’re trying to have a conversation in a different language, listen to a speech or lecture while abroad, or watch a TV show or film in another language, you can now put in your headphones, open the Translate app, tap ‘Live translate’ and hear a real-time translation in your preferred language,” said Rose Yao, Google VP Product Management, Search Verticals, in the blog post."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-12-at-10.02.57-AM.png?w=680",
        "alt": "Image Credits:Google"
      },
      {
        "type": "text",
        "content": "The beta is rolling out now in the Translate app on Android in the U.S., Mexico, and India. The feature works with any pair of headphones and supports more than 70 languages."
      },
      {
        "type": "text",
        "content": "The company plans to bring the capability to iOS and more countries in 2026."
      },
      {
        "type": "text",
        "content": "As for the advanced Gemini capabilities coming to Translate, Google says they will enable smarter, more natural, and accurate text translations. They’ll also enable improved translations of phrases with more nuanced meanings, like slang, idioms, or local expressions."
      },
      {
        "type": "text",
        "content": "For example, if you’re trying to translate an English idiom like “stealing my thunder,” you’ll now get a more accurate translation instead of a literal word-for-word translation, as Gemini will parse the context to capture what the idiom really means."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/Text-Translations-Still.png?w=544",
        "alt": "Image Credits:Google"
      },
      {
        "type": "text",
        "content": "This update is rolling out now in the U.S. and India, translating between English and nearly 20 languages, including Spanish, Arabic, Chinese, Japanese, and German. The update is available in the Translate app on Android, iOS, and on the web."
      },
      {
        "type": "text",
        "content": "Google is also expanding its language learning tools to almost 20 new countries, including Germany, India, Sweden, and Taiwan. English speakers can now practice German, while Bengali, Mandarin Chinese (Simplified), Dutch, German, Hindi, Italian, Romanian, and Swedish speakers can practice English."
      },
      {
        "type": "text",
        "content": "The tech giant is also adding improved feedback so you can get helpful tips based on your speaking practice."
      },
      {
        "type": "text",
        "content": "Additionally, Google is adding a feature that tracks how many days in a row you’ve been learning, making it easier to see your progress and stay consistent. While the tools were already designed to take on Duolingo, this new feature brings the experience even closer to the popular language-learning app."
      }
    ],
    "status": "success"
  },
  {
    "url": "https://vc.ru/services/2645016-ii-pomoshchnik-2gis-dlya-otvetov-na-voprosy-o-zavedeniakh?from=rss",
    "site_type": "vcru",
    "title": "«2ГИС» запустил ИИ-помощника для ответов на вопросы пользователей о заведениях",
    "date": "11.12.2025",
    "text": "В будущем планируют добавить функцию записи на услуги.\n\nВ сервис добавили ИИ-ассистента, который отвечает на вопросы об организациях. Например, может подсказать часы работы, цены входных билетов, детали заселения в отелях, меню в ресторанах и кафе.\n\nЧтобы перейти в чат с ИИ-помощником, нужно выбрать раздел «Вопросы» в карточке заведения или места — в том числе парка или достопримечательности. Там также есть подборка готовых ответов на популярные запросы.\n\nОбновления доступны на сайте и в приложении на iOS и Android.\n\nКомпания планирует, что в будущем виртуальный помощник сможет записывать пользователей в автосервис или салоны красоты, а также искать вакансии и уточнять стоимость недвижимости внутри 2ГИС.\n\n#новости",
    "images": [
      {
        "url": "https://leonardo.osnova.io/d3a78933-79e2-530c-9ca2-768dc00a6c5f/-/scale_crop/592x/",
        "alt": "«2ГИС» запустил ИИ-помощника для ответов на вопросы пользователей о заведениях"
      }
    ],
    "structured_content": [
      {
        "type": "text",
        "content": "В будущем планируют добавить функцию записи на услуги."
      },
      {
        "type": "list",
        "content": [
          "В сервис добавили ИИ-ассистента, который отвечает на вопросы об организациях. Например, может подсказать часы работы, цены входных билетов, детали заселения в отелях, меню в ресторанах и кафе.",
          "Чтобы перейти в чат с ИИ-помощником, нужно выбрать раздел «Вопросы» в карточке заведения или места — в том числе парка или достопримечательности. Там также есть подборка готовых ответов на популярные запросы."
        ]
      },
      {
        "type": "image",
        "url": "https://leonardo.osnova.io/d3a78933-79e2-530c-9ca2-768dc00a6c5f/-/scale_crop/592x/",
        "alt": "«2ГИС» запустил ИИ-помощника для ответов на вопросы пользователей о заведениях"
      },
      {
        "type": "list",
        "content": [
          "Обновления доступны на сайте и в приложении на iOS и Android.",
          "Компания планирует, что в будущем виртуальный помощник сможет записывать пользователей в автосервис или салоны красоты, а также искать вакансии и уточнять стоимость недвижимости внутри 2ГИС."
        ]
      },
      {
        "type": "text",
        "content": "#новости"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://vc.ru/services/2645130-yandeks-karty-ii-pomoshchnik-dosug?from=rss",
    "site_type": "vcru",
    "title": "«Яндекс Карты» добавили чат с ИИ-помощником — он может составить подборку мест или план активностей на день",
    "date": "11.12.2025",
    "text": "Функция работает на базе Alice AI.\n\n«Яндекс Карты» начали тестировать чат с ИИ-помощником в ноябре 2025 года, теперь функция доступна всем пользователям, рассказали в сервисе.\n\nИконка «Спросить AI» появилась в приложении для iOS и Android. Инструмент предназначен для «планирования досуга и помощи в задачах, для которых недостаточно одного поискового запроса», объясняют в «Яндексе».\n\nНапример, пользователи могут спросить «куда сходить с детьми в районе дома в субботу вечером» или попросить составить план на день из разных активностей.\n\nАссистент проанализирует информацию в «Картах», включая описания заведений и отзывы, предложит подборку мест и объяснит, почему каждое из них может подойти. Из чата пользователи могут перейти к организации, забронировать столик или построить маршрут.\n\nСобственного ИИ-помощника для ответов на вопросы о заведениях также представил «2ГИС». Он может подсказать часы работы, цены входных билетов, детали заселения в отели, меню в ресторанах и кафе.\n\nИ переименовал все модели.\n\nИ переименовал все модели.\n\n#новости #яндекскарты",
    "images": [
      {
        "url": "https://leonardo.osnova.io/72700366-5292-560b-a23c-7a9ce42ac0e7/-/scale_crop/592x/",
        "alt": "Источник: «Яндекс»"
      },
      {
        "url": "https://leonardo.osnova.io/bff582be-fb77-5202-afe9-10a6a016ba05/-/scale_crop/592x/",
        "alt": "Источник: Wylsacom"
      }
    ],
    "structured_content": [
      {
        "type": "text",
        "content": "Функция работает на базе Alice AI."
      },
      {
        "type": "image",
        "url": "https://leonardo.osnova.io/72700366-5292-560b-a23c-7a9ce42ac0e7/-/scale_crop/592x/",
        "alt": "Источник: «Яндекс»"
      },
      {
        "type": "list",
        "content": [
          "«Яндекс Карты» начали тестировать чат с ИИ-помощником в ноябре 2025 года, теперь функция доступна всем пользователям, рассказали в сервисе.",
          "Иконка «Спросить AI» появилась в приложении для iOS и Android. Инструмент предназначен для «планирования досуга и помощи в задачах, для которых недостаточно одного поискового запроса», объясняют в «Яндексе».",
          "Например, пользователи могут спросить «куда сходить с детьми в районе дома в субботу вечером» или попросить составить план на день из разных активностей.",
          "Ассистент проанализирует информацию в «Картах», включая описания заведений и отзывы, предложит подборку мест и объяснит, почему каждое из них может подойти. Из чата пользователи могут перейти к организации, забронировать столик или построить маршрут.",
          "Собственного ИИ-помощника для ответов на вопросы о заведениях также представил «2ГИС». Он может подсказать часы работы, цены входных билетов, детали заселения в отели, меню в ресторанах и кафе."
        ]
      },
      {
        "type": "text",
        "content": "И переименовал все модели."
      },
      {
        "type": "image",
        "url": "https://leonardo.osnova.io/bff582be-fb77-5202-afe9-10a6a016ba05/-/scale_crop/592x/",
        "alt": "Источник: Wylsacom"
      },
      {
        "type": "text",
        "content": "И переименовал все модели."
      },
      {
        "type": "image",
        "url": "https://leonardo.osnova.io/bff582be-fb77-5202-afe9-10a6a016ba05/-/scale_crop/592x/",
        "alt": "Источник: Wylsacom"
      },
      {
        "type": "text",
        "content": "#новости #яндекскарты"
      }
    ],
    "status": "success"
  },
  {
    "url": "https://techcrunch.com/2025/12/10/figma-launches-new-ai-powered-object-removal-and-image-extension/",
    "site_type": "techcrunch",
    "title": "Figma launches new AI-powered object removal and image extension",
    "date": "10.12.2025",
    "text": "Design tool Figma launched new AI-powered image-editing features today, including the ability to remove and isolate objects and expand images.\n\nThe company said that these features will save the hassle of exporting images to other tools for editing and importing them back. It added that generation models like Nano Banana are good for creating images, but users often need granular tools for editing that work without any text prompts.\n\nFigma has improved its lasso tool for selection. Now you can use it to select an object, remove it, or isolate it to move it around. When you move around the object, the image still retains other characteristics, such as background and color. Users can also select an object to adjust factors like lighting, shadow, color, or focus.\n\nThe company is also bringing an image-expansion feature to its design suite. This feature is handy when you are adjusting a creative for a particular format and need to fill in the background or other details. For instance, creating a web banner or mobile banner from a 1×1 image. It essentially saves you from constantly cropping an image and adjusting elements within it.\n\nBesides adding these features, Figma is collating all its image-editing tools in one toolbar for easier access. You can select objects or parts of images, change background color, and add annotations or text using this toolbar. The company said removing background is one of the most common actions on the platform, and that is why it is getting a prominent spot on the new toolbar.\n\nRivals like Adobe and Canva have had object removal for a few years now. Figma is finally catching up to offer these features.\n\nThe company said the new image-editing features are available on Figma Design and Draw, with plans to make them available across Figma tools next year.\n\nFigma’s launch comes on the same day as Adobe making some of these features available to users within ChatGPT . Figma was one of the launch partners of the app, calling on ChatGPT in October. It is not clear if these new functions will be available to users using Figma within OpenAI’s tool.",
    "images": [
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/AI-Image-toolbar.jpeg?w=1024",
        "alt": "Image Credits:Figma"
      },
      {
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/ezgif-8bda4c8d57256f21.gif?w=680",
        "alt": "Image Credits:Figma"
      }
    ],
    "structured_content": [
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/AI-Image-toolbar.jpeg?w=1024",
        "alt": "Image Credits:Figma"
      },
      {
        "type": "text",
        "content": "Design tool Figma launched new AI-powered image-editing features today, including the ability to remove and isolate objects and expand images."
      },
      {
        "type": "text",
        "content": "The company said that these features will save the hassle of exporting images to other tools for editing and importing them back. It added that generation models like Nano Banana are good for creating images, but users often need granular tools for editing that work without any text prompts."
      },
      {
        "type": "text",
        "content": "Figma has improved its lasso tool for selection. Now you can use it to select an object, remove it, or isolate it to move it around. When you move around the object, the image still retains other characteristics, such as background and color. Users can also select an object to adjust factors like lighting, shadow, color, or focus."
      },
      {
        "type": "text",
        "content": "The company is also bringing an image-expansion feature to its design suite. This feature is handy when you are adjusting a creative for a particular format and need to fill in the background or other details. For instance, creating a web banner or mobile banner from a 1×1 image. It essentially saves you from constantly cropping an image and adjusting elements within it."
      },
      {
        "type": "image",
        "url": "https://techcrunch.com/wp-content/uploads/2025/12/ezgif-8bda4c8d57256f21.gif?w=680",
        "alt": "Image Credits:Figma"
      },
      {
        "type": "text",
        "content": "Besides adding these features, Figma is collating all its image-editing tools in one toolbar for easier access. You can select objects or parts of images, change background color, and add annotations or text using this toolbar. The company said removing background is one of the most common actions on the platform, and that is why it is getting a prominent spot on the new toolbar."
      },
      {
        "type": "text",
        "content": "Rivals like Adobe and Canva have had object removal for a few years now. Figma is finally catching up to offer these features."
      },
      {
        "type": "text",
        "content": "The company said the new image-editing features are available on Figma Design and Draw, with plans to make them available across Figma tools next year."
      },
      {
        "type": "text",
        "content": "Figma’s launch comes on the same day as Adobe making some of these features available to users within ChatGPT . Figma was one of the launch partners of the app, calling on ChatGPT in October. It is not clear if these new functions will be available to users using Figma within OpenAI’s tool."
      }
    ],
    "status": "success"
  }
]