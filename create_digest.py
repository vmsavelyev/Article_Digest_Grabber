#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è AI Digest –≤ Notion

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
1. –°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É "AI Digest - Week X YEAR" –≤ Database "–õ–∏—á–Ω—ã–π –±–ª–æ–≥"
2. –ó–∞–ø–æ–ª–Ω—è–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —à–∞–±–ª–æ–Ω —Å —Å–µ–∫—Ü–∏—è–º–∏ Research, Notes, Draft
3. –°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Database "–û–±–∑–æ—Ä —Ä—ã–Ω–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è" –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥
4. –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –¥–∞—Ç–∞–º –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ —Å–µ–∫—Ü–∏—é Draft
"""

import os
import re
import sys
import httpx
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from collections import defaultdict
from notion_client import Client


def markdown_to_notion_blocks(markdown_content: str) -> List[dict]:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç Markdown –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –±–ª–æ–∫–∏ Notion API.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: –∑–∞–≥–æ–ª–æ–≤–∫–∏ H1-H3, —Å–ø–∏—Å–∫–∏, toggle (blockquote), –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
    """
    blocks = []
    lines = markdown_content.split('\n')

    i = 0
    while i < len(lines):
        line = lines[i]
        stripped = line.strip()

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        if not stripped:
            i += 1
            continue

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ H3 (### )
        if stripped.startswith('### '):
            blocks.append({
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": stripped[4:].strip()}}]
                }
            })
            i += 1
            continue

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ H2 (## )
        if stripped.startswith('## '):
            blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": stripped[3:].strip()}}]
                }
            })
            i += 1
            continue

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ H1 (# )
        if stripped.startswith('# '):
            blocks.append({
                "object": "block",
                "type": "heading_1",
                "heading_1": {
                    "rich_text": [{"type": "text", "text": {"content": stripped[2:].strip()}}]
                }
            })
            i += 1
            continue

        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ![alt](url)
        img_match = re.match(r'!\[([^\]]*)\]\(([^)]+)\)', stripped)
        if img_match:
            blocks.append({
                "object": "block",
                "type": "image",
                "image": {
                    "type": "external",
                    "external": {"url": img_match.group(2)}
                }
            })
            i += 1
            continue

        # –ú–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ (- item)
        if stripped.startswith('- '):
            while i < len(lines) and lines[i].strip().startswith('- '):
                item_text = lines[i].strip()[2:].strip()
                if item_text:
                    blocks.append({
                        "object": "block",
                        "type": "bulleted_list_item",
                        "bulleted_list_item": {
                            "rich_text": [{"type": "text", "text": {"content": item_text}}]
                        }
                    })
                i += 1
            continue

        # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ (1. item)
        if re.match(r'^\d+\.\s', stripped):
            while i < len(lines) and re.match(r'^\d+\.\s', lines[i].strip()):
                item_text = re.sub(r'^\d+\.\s', '', lines[i].strip()).strip()
                if item_text:
                    blocks.append({
                        "object": "block",
                        "type": "numbered_list_item",
                        "numbered_list_item": {
                            "rich_text": [{"type": "text", "text": {"content": item_text}}]
                        }
                    })
                i += 1
            continue

        # Toggle –±–ª–æ–∫ (> –∑–∞–≥–æ–ª–æ–≤–æ–∫) ‚Äî blockquote —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è toggle
        if stripped.startswith('> '):
            toggle_title = stripped[2:].strip()
            toggle_content_lines = []
            i += 1

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            while i < len(lines) and lines[i].strip() == '':
                i += 1

            # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ toggle, –∑–∞–≥–æ–ª–æ–≤–∫–∞ H1, –∏–ª–∏ –∫–æ–Ω—Ü–∞
            while i < len(lines):
                current = lines[i]
                current_stripped = current.strip()
                # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ –Ω–æ–≤–æ–º toggle –∏–ª–∏ H1 –∑–∞–≥–æ–ª–æ–≤–∫–µ
                if current_stripped.startswith('> ') or (current_stripped.startswith('# ') and not current_stripped.startswith('## ') and not current_stripped.startswith('### ')):
                    break
                toggle_content_lines.append(current)
                i += 1

            # –£–±–∏—Ä–∞–µ–º trailing –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            while toggle_content_lines and toggle_content_lines[-1].strip() == '':
                toggle_content_lines.pop()

            # –°–æ–∑–¥–∞—ë–º toggle —Å –≤–ª–æ–∂–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
            toggle_block = {
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [{"type": "text", "text": {"content": toggle_title}}],
                    "children": []
                }
            }

            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ toggle –∫–∞–∫ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –±–ª–æ–∫–∏
            if toggle_content_lines:
                content_text = '\n'.join(toggle_content_lines)
                toggle_block["toggle"]["children"].append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": content_text}}]
                    }
                })

            blocks.append(toggle_block)
            continue

        # –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è (---)
        if stripped == '---' or stripped == '***' or stripped == '___':
            blocks.append({
                "object": "block",
                "type": "divider",
                "divider": {}
            })
            i += 1
            continue

        # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Äî –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": [{"type": "text", "text": {"content": stripped}}]
            }
        })
        i += 1

    return blocks


def extract_database_id(url_or_id: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç Database ID –∏–∑ URL –∏–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç ID –∫–∞–∫ –µ—Å—Ç—å

    Args:
        url_or_id: URL Database –∏–ª–∏ Database ID

    Returns:
        Database ID (32 —Å–∏–º–≤–æ–ª–∞)
    """
    if not url_or_id:
        return ""

    url_or_id = url_or_id.strip()

    # –ï—Å–ª–∏ —ç—Ç–æ URL, –∏–∑–≤–ª–µ–∫–∞–µ–º ID
    if url_or_id.startswith('http'):
        # –£–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
        url_without_params = url_or_id.split('?')[0].split('#')[0]

        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç URL
        parts = url_without_params.rstrip('/').split('/')
        if len(parts) > 0:
            last_part = parts[-1]
            # –£–±–∏—Ä–∞–µ–º –¥–µ—Ñ–∏—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª–∏–Ω—ã
            id_without_dashes = last_part.replace('-', '')
            if len(id_without_dashes) == 32:
                return last_part

    # –ï—Å–ª–∏ —ç—Ç–æ ID –Ω–∞–ø—Ä—è–º—É—é
    id_without_dashes = url_or_id.replace('-', '')
    if len(id_without_dashes) == 32:
        return url_or_id

    return url_or_id


class TitleVerifier:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π"""

    def __init__(self, max_concurrent: int = 5):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.max_concurrent = max_concurrent

    def detect_site_type(self, url: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Å–∞–π—Ç–∞ –ø–æ URL"""
        domain = urlparse(url).netloc.lower()

        if 'vc.ru' in domain:
            return 'vcru'
        elif 'techcrunch.com' in domain:
            return 'techcrunch'
        elif 'habr.com' in domain:
            return 'habr'
        else:
            return 'unknown'

    def extract_title_from_soup(self, soup: BeautifulSoup, url: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ HTML –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Å–∞–π—Ç–∞"""
        site_type = self.detect_site_type(url)

        if site_type == 'vcru':
            title_tag = soup.find('h1', class_=lambda x: x and 'content-title' in x)
            if title_tag:
                # –£–¥–∞–ª—è–µ–º –∏–∫–æ–Ω–∫–∏
                title_copy = BeautifulSoup(str(title_tag), 'html.parser')
                for icon in title_copy.find_all('span', class_='content-title__editorial-icon'):
                    icon.decompose()
                for svg in title_copy.find_all('svg'):
                    svg.decompose()
                for use in title_copy.find_all('use'):
                    use.decompose()
                title = title_copy.get_text(separator=' ', strip=True)
                return re.sub(r'\s+', ' ', title).strip()

        elif site_type == 'techcrunch':
            title_tag = soup.find('h1', class_='wp-block-post-title')
            if title_tag:
                return title_tag.get_text(strip=True)

        elif site_type == 'habr':
            title_tag = soup.find('h1', class_='tm-title')
            if title_tag:
                span = title_tag.find('span')
                if span:
                    return span.get_text(strip=True)
                return title_tag.get_text(strip=True)

        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
        for tag in ['h1', 'title']:
            title_tag = soup.find(tag)
            if title_tag:
                return title_tag.get_text(strip=True)

        return None

    async def fetch_title(self, session: aiohttp.ClientSession, url: str) -> Tuple[str, Optional[str], Optional[str]]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç title —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ URL

        Returns:
            Tuple[url, title, error]
        """
        try:
            async with session.get(url, headers=self.headers, timeout=aiohttp.ClientTimeout(total=15)) as response:
                response.raise_for_status()
                content = await response.read()
                soup = BeautifulSoup(content, 'html.parser')
                title = self.extract_title_from_soup(soup, url)
                return (url, title, None)
        except aiohttp.ClientError as e:
            return (url, None, f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        except Exception as e:
            return (url, None, f"–û—à–∏–±–∫–∞: {e}")

    def normalize_title(self, title: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not title:
            return ""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
        normalized = title.lower()
        normalized = re.sub(r'\s+', ' ', normalized)
        normalized = normalized.strip()
        return normalized

    def titles_match(self, digest_title: str, actual_title: str, threshold: float = 0.7) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π

        Args:
            digest_title: –ù–∞–∑–≤–∞–Ω–∏–µ –∏–∑ Digest
            actual_title: –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (0.0 - 1.0)

        Returns:
            True –µ—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç
        """
        if not digest_title or not actual_title:
            return False

        norm_digest = self.normalize_title(digest_title)
        norm_actual = self.normalize_title(actual_title)

        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if norm_digest == norm_actual:
            return True

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –≤ –¥—Ä—É–≥–æ–µ
        if norm_digest in norm_actual or norm_actual in norm_digest:
            return True

        # –†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ —á–µ—Ä–µ–∑ –æ–±—â–∏–µ —Å–ª–æ–≤–∞
        digest_words = set(norm_digest.split())
        actual_words = set(norm_actual.split())

        if not digest_words or not actual_words:
            return False

        common_words = digest_words & actual_words
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
        min_size = min(len(digest_words), len(actual_words))
        similarity = len(common_words) / min_size if min_size > 0 else 0

        return similarity >= threshold

    async def verify_titles_async(self, news_items: List[Dict], log_file: Optional[str] = None) -> List[Dict]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–ª—è —Å–ø–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π

        Args:
            news_items: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–ª—è–º–∏ name –∏ url
            log_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π [{name, url, actual_title, error}]
        """
        mismatches = []
        all_results = []  # –î–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        items_with_urls = [(item['name'], item['url']) for item in news_items if item.get('url')]

        if not items_with_urls:
            return mismatches

        async with aiohttp.ClientSession() as session:
            semaphore = asyncio.Semaphore(self.max_concurrent)

            async def fetch_with_semaphore(name: str, url: str) -> Dict:
                async with semaphore:
                    url_result, actual_title, error = await self.fetch_title(session, url)
                    return {
                        'name': name,
                        'url': url,
                        'actual_title': actual_title,
                        'error': error
                    }

            tasks = [fetch_with_semaphore(name, url) for name, url in items_with_urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in results:
                if isinstance(result, Exception):
                    continue

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
                if result.get('error'):
                    match_status = 'ERROR'
                    is_match = False
                elif result.get('actual_title'):
                    is_match = self.titles_match(result['name'], result['actual_title'])
                    match_status = 'MATCH' if is_match else 'MISMATCH'
                else:
                    match_status = 'NO_TITLE'
                    is_match = False

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                all_results.append({
                    'name': result['name'],
                    'url': result['url'],
                    'actual_title': result.get('actual_title'),
                    'error': result.get('error'),
                    'status': match_status
                })

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π
                if result.get('error'):
                    mismatches.append({
                        'name': result['name'],
                        'url': result['url'],
                        'actual_title': None,
                        'error': result['error']
                    })
                elif result.get('actual_title') and not is_match:
                    mismatches.append({
                        'name': result['name'],
                        'url': result['url'],
                        'actual_title': result['actual_title'],
                        'error': None
                    })

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
        if log_file:
            self._write_log(log_file, all_results)

        return mismatches

    def _write_log(self, log_file: str, results: List[Dict]):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ –ª–æ–≥-—Ñ–∞–π–ª"""
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(f"# –õ–æ–≥ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π\n")
            f.write(f"# –î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# –í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ: {len(results)}\n")

            match_count = sum(1 for r in results if r['status'] == 'MATCH')
            mismatch_count = sum(1 for r in results if r['status'] == 'MISMATCH')
            error_count = sum(1 for r in results if r['status'] == 'ERROR')

            f.write(f"# –°–æ–≤–ø–∞–¥–µ–Ω–∏–π: {match_count}\n")
            f.write(f"# –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π: {mismatch_count}\n")
            f.write(f"# –û—à–∏–±–æ–∫: {error_count}\n")
            f.write("=" * 80 + "\n\n")

            for i, result in enumerate(results, 1):
                status_icon = {
                    'MATCH': '‚úÖ',
                    'MISMATCH': '‚ùå',
                    'ERROR': '‚ö†Ô∏è',
                    'NO_TITLE': '‚ùì'
                }.get(result['status'], '?')

                f.write(f"{i}. [{result['status']}] {status_icon}\n")
                f.write(f"   URL: {result['url']}\n")
                f.write(f"   –ù–∞–∑–≤–∞–Ω–∏–µ –≤ Digest:    {result['name']}\n")
                if result.get('error'):
                    f.write(f"   –û—à–∏–±–∫–∞: {result['error']}\n")
                elif result.get('actual_title'):
                    f.write(f"   –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ: {result['actual_title']}\n")
                else:
                    f.write(f"   –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ: (–Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å)\n")
                f.write("\n")

    def verify_titles(self, news_items: List[Dict], log_file: Optional[str] = None) -> List[Dict]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–∑–≤–∞–Ω–∏–π"""
        return asyncio.run(self.verify_titles_async(news_items, log_file))


class DigestCreator:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è AI Digest –≤ Notion"""

    def __init__(self, notion_token: str, blog_db_id: str, news_db_id: str, template_path: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è

        Args:
            notion_token: API —Ç–æ–∫–µ–Ω Notion
            blog_db_id: ID –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö "–õ–∏—á–Ω—ã–π –±–ª–æ–≥"
            news_db_id: ID –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö "–û–±–∑–æ—Ä —Ä—ã–Ω–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"
            template_path: –ü—É—Ç—å –∫ Markdown —Ñ–∞–π–ª—É —à–∞–±–ª–æ–Ω–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.notion = Client(auth=notion_token)
        self.notion_token = notion_token
        self.blog_db_id = blog_db_id
        self.news_db_id = news_db_id
        self.template_path = template_path

    def get_current_week_info(self) -> Tuple[int, int]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π –Ω–µ–¥–µ–ª–∏ –∏ –≥–æ–¥

        Returns:
            Tuple[week_number, year]
        """
        now = datetime.now()
        week_number = now.isocalendar()[1]
        year = now.year
        return week_number, year

    def create_digest_page(self, week_number: int, year: int) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É AI Digest –≤ Database "–õ–∏—á–Ω—ã–π –±–ª–æ–≥"

        Args:
            week_number: –ù–æ–º–µ—Ä –Ω–µ–¥–µ–ª–∏
            year: –ì–æ–¥

        Returns:
            ID —Å–æ–∑–¥–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        """
        title = f"AI Digest - Week {week_number} {year}"

        # Properties –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        properties = {
            "Name": {
                "title": [
                    {
                        "type": "text",
                        "text": {"content": title}
                    }
                ]
            },
            "Type": {
                "select": {"name": "Blog Post"}
            },
            "–¢–µ–º–∞—Ç–∏–∫–∞": {
                "multi_select": [{"name": "–ù–æ–≤–æ—Å—Ç–∏"}]
            },
            "Status": {
                "select": {"name": "In Progress"}
            }
        }

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —à–∞–±–ª–æ–Ω –∏–∑ Markdown —Ñ–∞–π–ª–∞ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –±–ª–æ–∫–∏ Notion
        template_blocks = self._load_template_blocks()

        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        response = self.notion.pages.create(
            parent={"database_id": self.blog_db_id},
            properties=properties,
            children=template_blocks
        )

        return response["id"]

    def _load_template_blocks(self) -> List[dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —à–∞–±–ª–æ–Ω –∏–∑ Markdown —Ñ–∞–π–ª–∞ –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ –±–ª–æ–∫–∏ Notion"""
        if not self.template_path:
            raise FileNotFoundError("–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —à–∞–±–ª–æ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω")

        if not os.path.exists(self.template_path):
            raise FileNotFoundError(f"–§–∞–π–ª —à–∞–±–ª–æ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.template_path}")

        with open(self.template_path, 'r', encoding='utf-8') as f:
            markdown_content = f.read()

        return markdown_to_notion_blocks(markdown_content)

    def fetch_news_from_database(self, start_date: datetime, end_date: datetime) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Database "–û–±–∑–æ—Ä —Ä—ã–Ω–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"
        –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥

        Args:
            start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
            end_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞

        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–ª—è–º–∏ name, url, date
        """
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ
        filter_params = {
            "and": [
                {
                    "property": "Date",
                    "date": {
                        "on_or_after": start_date.strftime("%Y-%m-%d")
                    }
                },
                {
                    "property": "Date",
                    "date": {
                        "on_or_before": end_date.strftime("%Y-%m-%d")
                    }
                }
            ]
        }

        news_items = []
        has_more = True
        next_cursor = None

        while has_more:
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞
            body = {
                "filter": filter_params,
                "sorts": [
                    {
                        "property": "Date",
                        "direction": "descending"
                    }
                ]
            }

            if next_cursor:
                body["start_cursor"] = next_cursor

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º httpx –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∫ Notion API
            headers = {
                "Authorization": f"Bearer {self.notion_token}",
                "Content-Type": "application/json",
                "Notion-Version": "2022-06-28"
            }

            with httpx.Client() as client:
                api_response = client.post(
                    f"https://api.notion.com/v1/databases/{self.news_db_id}/query",
                    headers=headers,
                    json=body,
                    timeout=30.0
                )
                api_response.raise_for_status()
                response = api_response.json()

            for page in response.get("results", []):
                properties = page.get("properties", {})

                # –ò–∑–≤–ª–µ–∫–∞–µ–º Name (title)
                name = ""
                name_prop = properties.get("Name", {})
                if name_prop.get("type") == "title":
                    title_array = name_prop.get("title", [])
                    name = "".join([t.get("plain_text", "") for t in title_array])

                # –ò–∑–≤–ª–µ–∫–∞–µ–º URL
                url = ""
                url_prop = properties.get("URL", {})
                if url_prop.get("type") == "url":
                    url = url_prop.get("url", "") or ""

                # –ò–∑–≤–ª–µ–∫–∞–µ–º Date
                date_str = ""
                date_prop = properties.get("Date", {})
                if date_prop.get("type") == "date":
                    date_obj = date_prop.get("date", {})
                    if date_obj and date_obj.get("start"):
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ YYYY-MM-DD –≤ DD.MM.YYYY
                        try:
                            dt = datetime.strptime(date_obj["start"], "%Y-%m-%d")
                            date_str = dt.strftime("%d.%m.%Y")
                        except:
                            date_str = date_obj["start"]

                if name:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ
                    news_items.append({
                        "name": name,
                        "url": url,
                        "date": date_str
                    })

            has_more = response.get("has_more", False)
            next_cursor = response.get("next_cursor")

        return news_items

    def aggregate_news_by_date(self, news_items: List[Dict]) -> Dict[str, List[Dict]]:
        """
        –ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –¥–∞—Ç–∞–º

        Args:
            news_items: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π

        Returns:
            –°–ª–æ–≤–∞—Ä—å {–¥–∞—Ç–∞: [–Ω–æ–≤–æ—Å—Ç–∏]}
        """
        aggregated = defaultdict(list)

        for item in news_items:
            date = item.get("date", "–ë–µ–∑ –¥–∞—Ç—ã")
            aggregated[date].append(item)

        return dict(aggregated)

    def format_news_as_markdown_blocks(self, aggregated_news: Dict[str, List[Dict]]) -> List[dict]:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ –±–ª–æ–∫–∏ Notion

        Args:
            aggregated_news: –°–ª–æ–≤–∞—Ä—å {–¥–∞—Ç–∞: [–Ω–æ–≤–æ—Å—Ç–∏]}

        Returns:
            –°–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ Notion
        """
        blocks = []

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∞—Ç—ã –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (–Ω–æ–≤—ã–µ —Å–≤–µ—Ä—Ö—É)
        sorted_dates = sorted(
            aggregated_news.keys(),
            key=lambda x: datetime.strptime(x, "%d.%m.%Y") if x != "–ë–µ–∑ –¥–∞—Ç—ã" else datetime.min,
            reverse=True
        )

        for date in sorted_dates:
            news_list = aggregated_news[date]

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å –¥–∞—Ç–æ–π (### DD.MM.YYYY)
            blocks.append({
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": date}}]
                }
            })

            # –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
            for news in news_list:
                name = news.get("name", "")
                url = news.get("url", "")

                if url:
                    # –°–æ–∑–¥–∞–µ–º —Å—Å—ã–ª–∫—É –≤–Ω—É—Ç—Ä–∏ –Ω–∞–∑–≤–∞–Ω–∏—è
                    blocks.append({
                        "object": "block",
                        "type": "bulleted_list_item",
                        "bulleted_list_item": {
                            "rich_text": [
                                {
                                    "type": "text",
                                    "text": {
                                        "content": name,
                                        "link": {"url": url}
                                    }
                                }
                            ]
                        }
                    })
                else:
                    # –ë–µ–∑ —Å—Å—ã–ª–∫–∏
                    blocks.append({
                        "object": "block",
                        "type": "bulleted_list_item",
                        "bulleted_list_item": {
                            "rich_text": [
                                {
                                    "type": "text",
                                    "text": {"content": name}
                                }
                            ]
                        }
                    })

            # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É –¥–∞—Ç–∞–º–∏
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {"rich_text": []}
            })

        return blocks

    def append_blocks_to_page(self, page_id: str, blocks: List[dict]):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –±–ª–æ–∫–∏ –≤ –∫–æ–Ω–µ—Ü —Å—Ç—Ä–∞–Ω–∏—Ü—ã

        Args:
            page_id: ID —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            blocks: –°–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
        """
        # Notion API –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –¥–æ 100 –±–ª–æ–∫–æ–≤ –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å
        batch_size = 100

        for i in range(0, len(blocks), batch_size):
            batch = blocks[i:i + batch_size]
            self.notion.blocks.children.append(
                block_id=page_id,
                children=batch
            )


def parse_date(date_str: str) -> Optional[datetime]:
    """
    –ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY

    Args:
        date_str: –°—Ç—Ä–æ–∫–∞ —Å –¥–∞—Ç–æ–π

    Returns:
        datetime –æ–±—ä–µ–∫—Ç –∏–ª–∏ None
    """
    try:
        return datetime.strptime(date_str.strip(), "%d.%m.%Y")
    except ValueError:
        return None


def get_database_urls_from_user() -> Tuple[str, str]:
    """
    –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç URL –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    Returns:
        Tuple[blog_db_id, news_db_id]
    """
    print("\n" + "=" * 60)
    print("–í–≤–µ–¥–∏—Ç–µ URL –∏–ª–∏ ID –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö Notion")
    print("=" * 60)

    # –ë–∞–∑–∞ "–õ–∏—á–Ω—ã–π –±–ª–æ–≥"
    while True:
        blog_input = input("\nURL Database '–õ–∏—á–Ω—ã–π –±–ª–æ–≥': ").strip()
        if blog_input:
            blog_db_id = extract_database_id(blog_input)
            if blog_db_id and len(blog_db_id.replace('-', '')) == 32:
                break
        print("–û—à–∏–±–∫–∞: –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL –∏–ª–∏ ID –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")

    # –ë–∞–∑–∞ "–û–±–∑–æ—Ä —Ä—ã–Ω–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"
    while True:
        news_input = input("URL Database '–û–±–∑–æ—Ä —Ä—ã–Ω–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è': ").strip()
        if news_input:
            news_db_id = extract_database_id(news_input)
            if news_db_id and len(news_db_id.replace('-', '')) == 32:
                break
        print("–û—à–∏–±–∫–∞: –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL –∏–ª–∏ ID –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")

    return blog_db_id, news_db_id


def get_date_range_from_user() -> Tuple[datetime, datetime]:
    """
    –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    Returns:
        Tuple[start_date, end_date]
    """
    print("\n" + "=" * 60)
    print("–í–≤–µ–¥–∏—Ç–µ –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç –¥–ª—è —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π")
    print("–§–æ—Ä–º–∞—Ç: DD.MM.YYYY")
    print("=" * 60)

    # –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
    while True:
        start_input = input("\n–ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ (DD.MM.YYYY): ").strip()
        start_date = parse_date(start_input)
        if start_date:
            break
        print("–û—à–∏–±–∫–∞: –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ DD.MM.YYYY")

    # –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞
    while True:
        end_input = input("–ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ (DD.MM.YYYY): ").strip()
        end_date = parse_date(end_input)
        if end_date:
            if end_date >= start_date:
                break
            print("–û—à–∏–±–∫–∞: –∫–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å >= –Ω–∞—á–∞–ª—å–Ω–æ–π")
        else:
            print("–û—à–∏–±–∫–∞: –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ DD.MM.YYYY")

    return start_date, end_date


def get_template_path_from_user() -> str:
    """
    –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —à–∞–±–ª–æ–Ω–∞ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    Returns:
        –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —à–∞–±–ª–æ–Ω–∞
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))

    print("\n" + "=" * 60)
    print("–í—ã–±–æ—Ä —Ñ–∞–π–ª–∞ —à–∞–±–ª–æ–Ω–∞")
    print("=" * 60)

    while True:
        template_input = input("\n–í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —à–∞–±–ª–æ–Ω–∞: ").strip()

        if not template_input:
            print("–û—à–∏–±–∫–∞: –ø—É—Ç—å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
            continue

        # –†–∞—Å—à–∏—Ä—è–µ–º ~ –¥–æ –¥–æ–º–∞—à–Ω–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        template_path = os.path.expanduser(template_input)

        # –ï—Å–ª–∏ –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π, –¥–µ–ª–∞–µ–º –µ–≥–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–∫—Ä–∏–ø—Ç–∞
        if not os.path.isabs(template_path):
            template_path = os.path.join(script_dir, template_path)

        if not os.path.exists(template_path):
            print(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {template_path}")
            continue

        return template_path


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    notion_token = os.getenv("NOTION_TOKEN")

    if len(sys.argv) > 1:
        notion_token = sys.argv[1]

    if not notion_token:
        print("–û—à–∏–±–∫–∞: NOTION_TOKEN –Ω–µ —É–∫–∞–∑–∞–Ω")
        print("\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("  export NOTION_TOKEN='your_token'")
        print("  python3 create_digest.py")
        print("\n–ò–ª–∏:")
        print("  python3 create_digest.py <NOTION_TOKEN>")
        sys.exit(1)

    print("=" * 60)
    print("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ AI Digest –≤ Notion")
    print("=" * 60)

    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º URL –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
    blog_db_id, news_db_id = get_database_urls_from_user()

    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø—É—Ç—å –∫ —à–∞–±–ª–æ–Ω—É
    template_path = get_template_path_from_user()

    print(f"\nüìã –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —à–∞–±–ª–æ–Ω: {template_path}")

    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä
    creator = DigestCreator(notion_token, blog_db_id, news_db_id, template_path=template_path)

    # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–º–µ—Ä –Ω–µ–¥–µ–ª–∏ –∏ –≥–æ–¥
    week_number, year = creator.get_current_week_info()

    print(f"\nüìÖ –¢–µ–∫—É—â–∞—è –Ω–µ–¥–µ–ª—è: {week_number}, –ì–æ–¥: {year}")
    print(f"üìù –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: AI Digest - Week {week_number} {year}")

    # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
    start_date, end_date = get_date_range_from_user()

    print(f"\nüìä –ü–µ—Ä–∏–æ–¥ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {start_date.strftime('%d.%m.%Y')} - {end_date.strftime('%d.%m.%Y')}")

    # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    confirm = input("\n–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å? [y/N]: ").strip().lower()
    if confirm not in ['y', 'yes', '–¥–∞', '–¥']:
        print("–û—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
        sys.exit(0)

    # –®–∞–≥ 1: –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
    print("\n" + "-" * 60)
    print("üìÑ –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ Database '–õ–∏—á–Ω—ã–π –±–ª–æ–≥'...")

    try:
        page_id = creator.create_digest_page(week_number, year)
        print(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞! ID: {page_id}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        sys.exit(1)

    # –®–∞–≥ 2: –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏
    print("\n" + "-" * 60)
    print("üì∞ –°–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ Database '–û–±–∑–æ—Ä —Ä—ã–Ω–∫–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è'...")

    try:
        news_items = creator.fetch_news_from_database(start_date, end_date)
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news_items)}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
        sys.exit(1)

    if not news_items:
        print("‚ö†Ô∏è –ù–æ–≤–æ—Å—Ç–µ–π –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        print("–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞ —Å –ø—É—Å—Ç–æ–π —Å–µ–∫—Ü–∏–µ–π Draft.")
        sys.exit(0)

    # –®–∞–≥ 3: –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–∞–º
    print("\n" + "-" * 60)
    print("üìä –ê–≥—Ä–µ–≥–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –¥–∞—Ç–∞–º...")

    aggregated = creator.aggregate_news_by_date(news_items)
    print(f"‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç: {len(aggregated)}")

    for date, items in sorted(aggregated.items(), key=lambda x: datetime.strptime(x[0], "%d.%m.%Y") if x[0] != "–ë–µ–∑ –¥–∞—Ç—ã" else datetime.min, reverse=True):
        print(f"   {date}: {len(items)} –Ω–æ–≤–æ—Å—Ç–µ–π")

    # –®–∞–≥ 4: –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å—Ç—Ä–∞–Ω–∏—Ü—É
    print("\n" + "-" * 60)
    print("üìù –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Å–µ–∫—Ü–∏—é Draft...")

    try:
        news_blocks = creator.format_news_as_markdown_blocks(aggregated)
        creator.append_blocks_to_page(page_id, news_blocks)
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –±–ª–æ–∫–æ–≤: {len(news_blocks)}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")
        sys.exit(1)

    # –ì–æ—Ç–æ–≤–æ
    print("\n" + "=" * 60)
    print("üéâ AI Digest —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω!")
    print(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞: AI Digest - Week {week_number} {year}")
    print(f"üîó ID: {page_id}")
    print("=" * 60)

    # –®–∞–≥ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π
    print("\n" + "-" * 60)
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–∞—Ç–µ–π...")

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è –ª–æ–≥-—Ñ–∞–π–ª–∞ —Å –¥–∞—Ç–æ–π –∏ –Ω–æ–º–µ—Ä–æ–º –Ω–µ–¥–µ–ª–∏
    log_filename = f"title_verification_week{week_number}_{year}.log"
    verifier = TitleVerifier(max_concurrent=5)
    mismatches = verifier.verify_titles(news_items, log_file=log_filename)

    print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {log_filename}")

    if mismatches:
        print(f"\n‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π: {len(mismatches)}")
        print("-" * 60)
        for i, mismatch in enumerate(mismatches, 1):
            print(f"\n{i}. URL: {mismatch['url']}")
            print(f"   –ù–∞–∑–≤–∞–Ω–∏–µ –≤ Digest: {mismatch['name']}")
            if mismatch.get('error'):
                print(f"   –û—à–∏–±–∫–∞: {mismatch['error']}")
            else:
                print(f"   –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ: {mismatch['actual_title']}")
    else:
        print("‚úÖ –í—Å–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–∞–º —Å—Ç–∞—Ç–µ–π")


if __name__ == "__main__":
    main()
